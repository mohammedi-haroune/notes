{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My Notes Here I try to mention everything I learned, ideas, books, code snippetes, configurations ..ect Important note Even though, there is content you may benefit from here, as the time of writing those notes were not intended to be understable for everyone ! Please comment down any questions, improvement proposal or anything you want to tell me about. I'm just writing notes about what I'm learning, about ideas that pitched me but the notes are not structured in a way to be read by everyone. Contribution If you find any mistake, typos or just wanting to improve those notes, feel free to open issues, pull requests are very welcomed. https://github.com/mohammedi-haroune/notes/","title":"Welcome to My Notes"},{"location":"#welcome-to-my-notes","text":"Here I try to mention everything I learned, ideas, books, code snippetes, configurations ..ect","title":"Welcome to My Notes"},{"location":"#important-note","text":"Even though, there is content you may benefit from here, as the time of writing those notes were not intended to be understable for everyone ! Please comment down any questions, improvement proposal or anything you want to tell me about. I'm just writing notes about what I'm learning, about ideas that pitched me but the notes are not structured in a way to be read by everyone.","title":"Important note"},{"location":"#contribution","text":"If you find any mistake, typos or just wanting to improve those notes, feel free to open issues, pull requests are very welcomed. https://github.com/mohammedi-haroune/notes/","title":"Contribution"},{"location":"PHd/share_resources/","text":"Share resources https://www.loadteam.com/ Q: What are these \"jobs\" that LoadTeam runs on my computer? A: LoadTeam uses your computer processor and memory to mine coins. LoadTeam does not currently use graphics cards. https://www.golem.network/ Needs KVM (Kernel Virtual Machines) Pays in their coin (GLM) Have SDKs and Code examples on how to run a Web server, a Blender task .. but the code is a little bit overwhelming for a regular user Leverges Docker images somehow open source: https://github.com/golemfactory/yagna https://www.hyperlink.org/ A lot of Marketing material, No information on what tasks supported, You have to sign up for early access to , as they say, \"Earn now\" https://www.honeygain.com Allow others to access internet through your computer, you'll technically, as far as I can tell, act as a proxy, some usecases: https://www.honeygain.com","title":"Share resources"},{"location":"PHd/share_resources/#share-resources","text":"","title":"Share resources"},{"location":"PHd/share_resources/#httpswwwloadteamcom","text":"Q: What are these \"jobs\" that LoadTeam runs on my computer? A: LoadTeam uses your computer processor and memory to mine coins. LoadTeam does not currently use graphics cards.","title":"https://www.loadteam.com/"},{"location":"PHd/share_resources/#httpswwwgolemnetwork","text":"Needs KVM (Kernel Virtual Machines) Pays in their coin (GLM) Have SDKs and Code examples on how to run a Web server, a Blender task .. but the code is a little bit overwhelming for a regular user Leverges Docker images somehow open source: https://github.com/golemfactory/yagna","title":"https://www.golem.network/"},{"location":"PHd/share_resources/#httpswwwhyperlinkorg","text":"A lot of Marketing material, No information on what tasks supported, You have to sign up for early access to , as they say, \"Earn now\"","title":"https://www.hyperlink.org/"},{"location":"PHd/share_resources/#httpswwwhoneygaincom","text":"Allow others to access internet through your computer, you'll technically, as far as I can tell, act as a proxy, some usecases: https://www.honeygain.com","title":"https://www.honeygain.com"},{"location":"TrackWebPage/Lawyers/","text":"Lawyer is a general word, it describes anyone who studied Law school, diffrent countries have diffrent terminology, see below barrister and solicitors , A barrister is a lawyer who specializes in higher court appearances. A solicitor is a lawyer who is trained to prepare cases and give advice on legal subjects and can represent people in lower courts. New Zealand. Barrister: a lawyer that frequents Court, or a Court lawyer Australia, the word \"lawyer\" can be used to refer to both barristers and solicitors United States, the term generally refers to attorneys Other law people In the US: - patent agents - paralegals","title":"Lawyers"},{"location":"TrackWebPage/Lawyers/#other-law-people","text":"In the US: - patent agents - paralegals","title":"Other law people"},{"location":"TrackWebPage/Messaging/","text":"Lawyers How are you dealing with law changes ? Hello, I'm reaching out to you hoping to know how are you keeping track of law changes over time ? I'll be very happy to talk about this (and your workflow in general) for a couple of minutes Regards, Send 26 messages on 14 Mars 2022 r/FREE Made a post in r/FREE, got 25 upvotes, 1 award and 100 guest accounts created (I posed the link directly to /guest/login)","title":"Messaging"},{"location":"TrackWebPage/Messaging/#lawyers","text":"How are you dealing with law changes ? Hello, I'm reaching out to you hoping to know how are you keeping track of law changes over time ? I'll be very happy to talk about this (and your workflow in general) for a couple of minutes Regards, Send 26 messages on 14 Mars 2022","title":"Lawyers"},{"location":"TrackWebPage/Messaging/#rfree","text":"Made a post in r/FREE, got 25 upvotes, 1 award and 100 guest accounts created (I posed the link directly to /guest/login)","title":"r/FREE"},{"location":"aws/APIGateway/","text":"API Gateway API Gateway is a solution for creating secure APIS in your cloud environment at any scale. Create APIs that act as a front door for applications to access data, business logic, or functionality from back-end services. API Gateway throttles api endpoints at 10,000 requests per second (can be increase via service request through AWS support) Stages allow you to have multiple published versions of your API eg. prod, staging, QA Each Stage has an Invoke URL which is the endpoint you use to interact With your API You can use a custom domain for your Invoke URL eg. api.exampro.co You need to publish your API via Deploy API. You choose which Stage you want to publish your API Resources are your URLs eg. /projects Resources can have Child resources eg. /projects/-id-/edit You defined multiple Methods on your Resources eg GET, POST, DELETE CORS issues are common With API Gateway, CORS can be enabled on all or individual endpoints Caching improves latency and reduces the amount of calls made to your endpoint Same Origin Policies help to prevent XSS attacks Same Origin Policies ignore tools like postman or curl CORS is always enforced by the client. You can require Authorization to your API via AWS Cognito or a custom Lambda.","title":"API Gateway"},{"location":"aws/APIGateway/#api-gateway","text":"API Gateway is a solution for creating secure APIS in your cloud environment at any scale. Create APIs that act as a front door for applications to access data, business logic, or functionality from back-end services. API Gateway throttles api endpoints at 10,000 requests per second (can be increase via service request through AWS support) Stages allow you to have multiple published versions of your API eg. prod, staging, QA Each Stage has an Invoke URL which is the endpoint you use to interact With your API You can use a custom domain for your Invoke URL eg. api.exampro.co You need to publish your API via Deploy API. You choose which Stage you want to publish your API Resources are your URLs eg. /projects Resources can have Child resources eg. /projects/-id-/edit You defined multiple Methods on your Resources eg GET, POST, DELETE CORS issues are common With API Gateway, CORS can be enabled on all or individual endpoints Caching improves latency and reduces the amount of calls made to your endpoint Same Origin Policies help to prevent XSS attacks Same Origin Policies ignore tools like postman or curl CORS is always enforced by the client. You can require Authorization to your API via AWS Cognito or a custom Lambda.","title":"API Gateway"},{"location":"aws/BeanStack/","text":"BeanStack Elastic Beanstalk handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring - When you want to run a web-application but you don't want to have think about the underlying infrastructure. - It costs nothing to use Elastic Beanstalk (only the resources it provisions eg. RDS, ELB, EC2 - Recommended for test or development apps. Not recommended for production use - You can choose from the following preconfigured platforms: Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker - BeanStack needs some configuration files to know how to run your app, for nodejs .ebextensions/nodecommand.config and .ebextensions/staticfiles.config - You can run dockerized environments on Elastic Beanstalk.","title":"BeanStack"},{"location":"aws/BeanStack/#beanstack","text":"Elastic Beanstalk handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring - When you want to run a web-application but you don't want to have think about the underlying infrastructure. - It costs nothing to use Elastic Beanstalk (only the resources it provisions eg. RDS, ELB, EC2 - Recommended for test or development apps. Not recommended for production use - You can choose from the following preconfigured platforms: Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker - BeanStack needs some configuration files to know how to run your app, for nodejs .ebextensions/nodecommand.config and .ebextensions/staticfiles.config - You can run dockerized environments on Elastic Beanstalk.","title":"BeanStack"},{"location":"aws/CloudFront/","text":"CloudFront Content Delivry Network (CDN). Creates cached copies of your website at various edge locations arround the world. Origin: S3 bucket, EC2 instnace, ELB or Route53 Edge locations: server nearby to the users in a AWS region Distribution: collection of Edge locations which defines how cached content should behave Two types: Web and RTMP (streaming media) Behaviours: Redirect to HTTPs, Restrict HTTP Methods, Restrict Viewer Access, Set TTLs Invalidations: invalidates cache on specific files Error pages: serve custom error pages eg. 404 Restrictions: Geo restriction to blacklist or whitelist specific countries Edge location aren't just read-only, you can write to them eg. PUT objects Refreshing cache costs money Origin Identity Access (OAI) is used to access private s3 buckets Access cached content can be protected via Signed Urls or Signed Cookies !!! I should play a little bit with CloudFront !!! ### Lambda@Edge Override the behaviour of request and responses: Viewer request: When CloudFront receives a request from a viewer Origin request: Before CloudFront forwards the request to the origin Origin resposne: When CloudFront receives a response from the origin Viewer resposne: Before CloudFront returns the response to the viewer Use cases: Authenticate requests using Icongito Run A/B Testing for marketing startegies","title":"CloudFront"},{"location":"aws/CloudFront/#cloudfront","text":"Content Delivry Network (CDN). Creates cached copies of your website at various edge locations arround the world. Origin: S3 bucket, EC2 instnace, ELB or Route53 Edge locations: server nearby to the users in a AWS region Distribution: collection of Edge locations which defines how cached content should behave Two types: Web and RTMP (streaming media) Behaviours: Redirect to HTTPs, Restrict HTTP Methods, Restrict Viewer Access, Set TTLs Invalidations: invalidates cache on specific files Error pages: serve custom error pages eg. 404 Restrictions: Geo restriction to blacklist or whitelist specific countries Edge location aren't just read-only, you can write to them eg. PUT objects Refreshing cache costs money Origin Identity Access (OAI) is used to access private s3 buckets Access cached content can be protected via Signed Urls or Signed Cookies","title":"CloudFront"},{"location":"aws/CloudFront/#i-should-play-a-little-bit-with-cloudfront","text":"### Lambda@Edge Override the behaviour of request and responses: Viewer request: When CloudFront receives a request from a viewer Origin request: Before CloudFront forwards the request to the origin Origin resposne: When CloudFront receives a response from the origin Viewer resposne: Before CloudFront returns the response to the viewer Use cases: Authenticate requests using Icongito Run A/B Testing for marketing startegies","title":"!!! I should play a little bit with CloudFront !!!"},{"location":"aws/CloudX-Formation-Watch-Logs/","text":"CloudFormation When being asked to automate the provisioning of resources think CloudFormation When Infrastructure as Code (laC) is mentioned think CloudFormation CloudFormation can be written in either JSON or YAML When CloudFormation encounters an error it will rollback with ROLLBACK_IN_PROGRESS CloudFormation templates larger than 51,200 bytes (0.05 MB) are too large to upload directly, and must be imported into CloudFormation via an S3 bucket. NestedStacks helps you break up your CloudFormation template into smaller reusable templates that can be composed into larger templates At least one resource under resources: must be defined for a CloudFormation template to be valid Format: MetaData extra information about your template Description a description of what the template is suppose to do Parameters is how you get user inputs into templates Transforms Applies marcos (like applying a mod which change the anatomy to be custom) Outputs are values you can use to import into other stacks Mappings maps keys to values, just like a lookup table Resources defines the resources you want to provision, at least one resource is required Conditions are whether resources are created or properties are assigned CloudWatch CloudWatch is a collection of monitoring services: Dashboards, Events, Alarms, Logs and Metrics CloudWatch Logs: log data from AWS services. eg. CPU Utilization CloudWatch Metrics: Represents a time-ordered set of data points, A variable to monitor eg. CPU Utilization over time CloudWatch Events: trigger an event based on a condition eg. ever hour take snapshot of server CloudWatch Alarms: triggers notifications based on metrics when a defined threshold is breached CloudWatch Dashboards: create visualizations based on metrics EC2 monitors at 5 min intervals and at Detailed Monitoring 1 minute intervals Most other service monitor at 1 minute intervals, With intervals of 1 , 3 , 5 minutes. Logs must belong to a Log Group CloudWatch Agent needs to be installed on EC2 host to track Memory Usage and Disk Size You can can stream custom log files eg. production.log Custom Metrics allow you to track High Resolution Metrics a sub minute intervals all the way down to 1 second. CloudLogs CloudTrail logs calls between AWS services - governance, compliance, operational auditing, and risk auditing are keywords relating to CloudTrail - When you need to know Who to blame think Cloud Trail - CloudTrail by default logs event data for the past 90s days via Event History - To track beyond 90 days you need to create Trail - To ensure logs have not been tampered With you need to turn on Log File Validation option - CloudTrail logs can be encrypted using KMS (Key Management Service) - CloudTrail can be set to log across all AWS accounts in an Organization and all regions in an account. - CloudTrail logs can be streamed to CloudWatch logs - Trails are outputted to an S3 bucket that you specify - CloudTrail logs two kinds of events: Management Events and Data Events - Management events log management operations eg. AttachRolePolicy - Data Events log data operations for resources (S3, Lambda) eg. GetObject, DeleteObject, and PutObject - Data Events are disabled by default when creating a Trail. - Trail logs in S3 can be analyzed using Athena","title":"CloudFormation"},{"location":"aws/CloudX-Formation-Watch-Logs/#cloudformation","text":"When being asked to automate the provisioning of resources think CloudFormation When Infrastructure as Code (laC) is mentioned think CloudFormation CloudFormation can be written in either JSON or YAML When CloudFormation encounters an error it will rollback with ROLLBACK_IN_PROGRESS CloudFormation templates larger than 51,200 bytes (0.05 MB) are too large to upload directly, and must be imported into CloudFormation via an S3 bucket. NestedStacks helps you break up your CloudFormation template into smaller reusable templates that can be composed into larger templates At least one resource under resources: must be defined for a CloudFormation template to be valid Format: MetaData extra information about your template Description a description of what the template is suppose to do Parameters is how you get user inputs into templates Transforms Applies marcos (like applying a mod which change the anatomy to be custom) Outputs are values you can use to import into other stacks Mappings maps keys to values, just like a lookup table Resources defines the resources you want to provision, at least one resource is required Conditions are whether resources are created or properties are assigned","title":"CloudFormation"},{"location":"aws/CloudX-Formation-Watch-Logs/#cloudwatch","text":"CloudWatch is a collection of monitoring services: Dashboards, Events, Alarms, Logs and Metrics CloudWatch Logs: log data from AWS services. eg. CPU Utilization CloudWatch Metrics: Represents a time-ordered set of data points, A variable to monitor eg. CPU Utilization over time CloudWatch Events: trigger an event based on a condition eg. ever hour take snapshot of server CloudWatch Alarms: triggers notifications based on metrics when a defined threshold is breached CloudWatch Dashboards: create visualizations based on metrics EC2 monitors at 5 min intervals and at Detailed Monitoring 1 minute intervals Most other service monitor at 1 minute intervals, With intervals of 1 , 3 , 5 minutes. Logs must belong to a Log Group CloudWatch Agent needs to be installed on EC2 host to track Memory Usage and Disk Size You can can stream custom log files eg. production.log Custom Metrics allow you to track High Resolution Metrics a sub minute intervals all the way down to 1 second.","title":"CloudWatch"},{"location":"aws/CloudX-Formation-Watch-Logs/#cloudlogs","text":"CloudTrail logs calls between AWS services - governance, compliance, operational auditing, and risk auditing are keywords relating to CloudTrail - When you need to know Who to blame think Cloud Trail - CloudTrail by default logs event data for the past 90s days via Event History - To track beyond 90 days you need to create Trail - To ensure logs have not been tampered With you need to turn on Log File Validation option - CloudTrail logs can be encrypted using KMS (Key Management Service) - CloudTrail can be set to log across all AWS accounts in an Organization and all regions in an account. - CloudTrail logs can be streamed to CloudWatch logs - Trails are outputted to an S3 bucket that you specify - CloudTrail logs two kinds of events: Management Events and Data Events - Management events log management operations eg. AttachRolePolicy - Data Events log data operations for resources (S3, Lambda) eg. GetObject, DeleteObject, and PutObject - Data Events are disabled by default when creating a Trail. - Trail logs in S3 can be analyzed using Athena","title":"CloudLogs"},{"location":"aws/Cognito/","text":"Cognito Decentralized managed authentication. Sign-up, sign-in integration for your apps. Social identity provider eg. Facebook, Google. User pools, allows users to authenticate using OAuth to IpD such as Facebook, Google, Amazone ... to connect to web-applications. Cognito User Pool is in itself a IpD, they use JWTs for persisting authentication Identity pools provides temporary AWS credentials to access services like S3 and DynamoDB Sync can sync user data and preferences across devices with one line of code (powerd by SNS) Web Identity Federation exchange identity and security information between Identity provider (IdP) and an application Identity Provider (IdP) a trusted provider for your user identity that lets you use authenticate to access other services eg. Facebook OIDC is a type of identity provider which user OAuth SAML is a type of identity provider which is used for Single Sign-on (SS0)","title":"Cognito"},{"location":"aws/Cognito/#cognito","text":"Decentralized managed authentication. Sign-up, sign-in integration for your apps. Social identity provider eg. Facebook, Google. User pools, allows users to authenticate using OAuth to IpD such as Facebook, Google, Amazone ... to connect to web-applications. Cognito User Pool is in itself a IpD, they use JWTs for persisting authentication Identity pools provides temporary AWS credentials to access services like S3 and DynamoDB Sync can sync user data and preferences across devices with one line of code (powerd by SNS) Web Identity Federation exchange identity and security information between Identity provider (IdP) and an application Identity Provider (IdP) a trusted provider for your user identity that lets you use authenticate to access other services eg. Facebook OIDC is a type of identity provider which user OAuth SAML is a type of identity provider which is used for Single Sign-on (SS0)","title":"Cognito"},{"location":"aws/DNS-Route53/","text":"DNS Convert domains names into routable IP addresses Domain Registra 3rd party copany who you register domains though Name Server is the server which contains the DNS records for a domain Start of Authority (SOA) contains information about the DNS zone and associated DNS records ???? A record convert a domain directly to IP record CNAME record convert domain name to another Time to Live (TTL) the time that DNS record will be cached for, the lower means changes propagates faster Route53 Route53 is a DNS provider, register and mange domains, create record sets. Think Godaddy or NameCheap Traffic Flow: visual editor for changing routing policies, can version policy records for easy rollback AWS Alias Record - AWS' smart DNS records: detects changed IPs for AWS resources and adjusts automatically Route53 Resolver: lets you regionally route DNS queries between your VPCs and your network Hybrid Environments Health checks can be created to monitor and automatically over endpoints, You can have health checks monitor other health checks Routing policies Simple: Default, multiple addresses result in a random endpoint selection Weighted: split based on assigned weights Latency-based: Based on region, direct to the lowest latency for users, not neccessarly the closest region Failover: direct trafic to a secondary site if the health of the primary is not good Geolocoation: based on geographic location of request origin Geo-proxmity: based on geographic location using Bias values (needs Route53 Traffic Flow) Multi-Value Answer: just like Simple but uses health check for selection","title":"DNS Route53"},{"location":"aws/DNS-Route53/#dns","text":"Convert domains names into routable IP addresses Domain Registra 3rd party copany who you register domains though Name Server is the server which contains the DNS records for a domain Start of Authority (SOA) contains information about the DNS zone and associated DNS records ???? A record convert a domain directly to IP record CNAME record convert domain name to another Time to Live (TTL) the time that DNS record will be cached for, the lower means changes propagates faster","title":"DNS"},{"location":"aws/DNS-Route53/#route53","text":"Route53 is a DNS provider, register and mange domains, create record sets. Think Godaddy or NameCheap Traffic Flow: visual editor for changing routing policies, can version policy records for easy rollback AWS Alias Record - AWS' smart DNS records: detects changed IPs for AWS resources and adjusts automatically Route53 Resolver: lets you regionally route DNS queries between your VPCs and your network Hybrid Environments Health checks can be created to monitor and automatically over endpoints, You can have health checks monitor other health checks","title":"Route53"},{"location":"aws/DNS-Route53/#routing-policies","text":"Simple: Default, multiple addresses result in a random endpoint selection Weighted: split based on assigned weights Latency-based: Based on region, direct to the lowest latency for users, not neccessarly the closest region Failover: direct trafic to a secondary site if the health of the primary is not good Geolocoation: based on geographic location of request origin Geo-proxmity: based on geographic location using Bias values (needs Route53 Traffic Flow) Multi-Value Answer: just like Simple but uses health check for selection","title":"Routing policies"},{"location":"aws/EC2-ELB/","text":"EC2 Pricing https://plazagonzalo.medium.com/ec2-exam-questions-aws-solutions-architect-8c0f0e643038 Solution: 4. A cluster placement group provides low latency and high throughput for instances deployed in a single AZ. Load Balancer Placement group doesn\u2019t exist. Solution: 3. A spread placement group is a group of instances placed on distinct underlying hardware, reducing the risk of simultaneous failures. On-dmenad By default, by the hour or by the minute, no up-front payment, no long-term commitment. Suitable for short-term , spikey or unpredictable Reserved Instances Best long-term savings with commitment. Best for steady-state , predictable usage or require reserved capacity . Reduced pricing is based on Term . Class Offering . Payment Option Terms : 1 Year or 3 Year contract Payment Options : All Upfront, Partial Upfront, No Upfront Standard Up to 75% reduced pricing. Cannot change attributes Convertible Up to 54% reduced pricing. Alows you to change attributes if greater or equal value Scheduled You reserve instances for specific time periods eg. once a week for a few hours. Savings vary RIs can shared between multiple accounts within an org Unused RIs can be sold in the Reserved Instance Marketplace Spot Provide a discount of 90% compared to On-Demand pricing If you terminate an instance you will stil be charged for any hour that it run ??? Dedicated Host Physical Isolation, dedicated hardware, most expensive Entreprises and Large Organizations may have security concerns or obligations AMI Amazon Machine Images provides the information required to launch an instance which holds: - Template for the root volume for the intance (EBS instance or Instance Store Snapshot) eg. OS, applications ... - Launch permissions that controls which AWS can use the AMI to launch instnaces - Block device mapping that specifies the volumes to attach to the instance when it's launched. - Region specific, we have to copy an AMI to other regions Auto scaling groups An ASG is a collection of EC2 instances grouped for scaling management Scaling Out is when you add servers. Scaling In when you remove servers. Scaling Up is when you inscrease the size of instance eg. updating the launch configuration with larger size Size of ASG is based on Min , Max and Desired capacity Target Scaling Policy scales based on when a target value for a metric is breached eg. Average CPU utilization exceed 75% Simple Scaling triggers scaling when an alarm is breached Scaling Policy with Steps is the new version of Simple Scaling and allows you to create steps based on intervals of alarm values Health checks can be run againts either an ELB or the EC2 instances ASG uses a Launch Configuration which holds the AMI, InstanceType, Role ... Launch configuration cannot be edited and must be cloned or a new one created. Auto scaling settings should be manually updated when new launch configurations are created ELB Distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses and Lambda functions Load Balancers can be physical hardware or virtual software, there are three types: - ALB (Application, HTTP/HTTPS): Web Apps - NLB (Network, TCP/UDP): high network thoughtput eg. Video games - CLB (Classic, Legacy): not recommended - ALB has Listeners, Rules and Target Groups to route traffic - NLB uses Listenres and target groups, there are no rules - CLB uses listeners and EC2 instances are directrly registered as targets to CLB An ELB must have at least two Availability Zones and cannot go cross-region, we must create one per region Exam question: Classic Load balancer respond with 504 error if the application is not responding Use X-Forwared-For to get original IP of incoming traffic We can attach Web Application Firewall (WAF) to ALB but not NLB or CLB We can attach Amazon Certification Manager SSL to any of the ELB for SSL termination ALB has advanced Request Routing rules where you can route based on subdomain header, path and other HTTP(S) information Sticky Sessions can be enabled for CLB or ALB and sessions are remembered via Cookies ECE Follow along In order to be able to use Session Manager to login to an EC2 instance, its IAM role should have AmazonEC2RoleforSSM policy or better the recommended one AmazonSSMManagedInstanceCore # Get the user-data, the script that was used after luanching the instace curl http://169.254.169.254/latest/user-data # Get the instance meta-data like ami-id, security groups ... curl http://169.254.169.254/latest/meta-data/ami-id Encrypt a running EBS Create a snapshot, check the \"Encryption\" checkbox and select a \"Master key\" Create an Image (AMI) from the EBS Snapshot Launch Configuration vs Launch Template Launch Configuration: AMI Machine Type Volumes IAM profile Security group User data Meta data Auto Scaling Group VPC Subnets Auto scaling group ensures there is enough number of EC2 machines (in the same AZ ?) but If the Whole AZ goes out, the Auto Scaling group won't help, we need to create high availability using load balancers so we can run instances in more that one AZ I couldn't change the security group inbound rule for HTTP to just accept \"Source\" from another security but I was able to create another security group with that same rule","title":"EC2 ELB"},{"location":"aws/EC2-ELB/#ec2-pricing","text":"https://plazagonzalo.medium.com/ec2-exam-questions-aws-solutions-architect-8c0f0e643038 Solution: 4. A cluster placement group provides low latency and high throughput for instances deployed in a single AZ. Load Balancer Placement group doesn\u2019t exist. Solution: 3. A spread placement group is a group of instances placed on distinct underlying hardware, reducing the risk of simultaneous failures.","title":"EC2 Pricing"},{"location":"aws/EC2-ELB/#on-dmenad","text":"By default, by the hour or by the minute, no up-front payment, no long-term commitment. Suitable for short-term , spikey or unpredictable","title":"On-dmenad"},{"location":"aws/EC2-ELB/#reserved-instances","text":"Best long-term savings with commitment. Best for steady-state , predictable usage or require reserved capacity . Reduced pricing is based on Term . Class Offering . Payment Option Terms : 1 Year or 3 Year contract Payment Options : All Upfront, Partial Upfront, No Upfront Standard Up to 75% reduced pricing. Cannot change attributes Convertible Up to 54% reduced pricing. Alows you to change attributes if greater or equal value Scheduled You reserve instances for specific time periods eg. once a week for a few hours. Savings vary RIs can shared between multiple accounts within an org Unused RIs can be sold in the Reserved Instance Marketplace","title":"Reserved Instances"},{"location":"aws/EC2-ELB/#spot","text":"Provide a discount of 90% compared to On-Demand pricing If you terminate an instance you will stil be charged for any hour that it run ???","title":"Spot"},{"location":"aws/EC2-ELB/#dedicated-host","text":"Physical Isolation, dedicated hardware, most expensive Entreprises and Large Organizations may have security concerns or obligations","title":"Dedicated Host"},{"location":"aws/EC2-ELB/#ami","text":"Amazon Machine Images provides the information required to launch an instance which holds: - Template for the root volume for the intance (EBS instance or Instance Store Snapshot) eg. OS, applications ... - Launch permissions that controls which AWS can use the AMI to launch instnaces - Block device mapping that specifies the volumes to attach to the instance when it's launched. - Region specific, we have to copy an AMI to other regions","title":"AMI"},{"location":"aws/EC2-ELB/#auto-scaling-groups","text":"An ASG is a collection of EC2 instances grouped for scaling management Scaling Out is when you add servers. Scaling In when you remove servers. Scaling Up is when you inscrease the size of instance eg. updating the launch configuration with larger size Size of ASG is based on Min , Max and Desired capacity Target Scaling Policy scales based on when a target value for a metric is breached eg. Average CPU utilization exceed 75% Simple Scaling triggers scaling when an alarm is breached Scaling Policy with Steps is the new version of Simple Scaling and allows you to create steps based on intervals of alarm values Health checks can be run againts either an ELB or the EC2 instances ASG uses a Launch Configuration which holds the AMI, InstanceType, Role ... Launch configuration cannot be edited and must be cloned or a new one created. Auto scaling settings should be manually updated when new launch configurations are created","title":"Auto scaling groups"},{"location":"aws/EC2-ELB/#elb","text":"Distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses and Lambda functions Load Balancers can be physical hardware or virtual software, there are three types: - ALB (Application, HTTP/HTTPS): Web Apps - NLB (Network, TCP/UDP): high network thoughtput eg. Video games - CLB (Classic, Legacy): not recommended - ALB has Listeners, Rules and Target Groups to route traffic - NLB uses Listenres and target groups, there are no rules - CLB uses listeners and EC2 instances are directrly registered as targets to CLB An ELB must have at least two Availability Zones and cannot go cross-region, we must create one per region Exam question: Classic Load balancer respond with 504 error if the application is not responding Use X-Forwared-For to get original IP of incoming traffic We can attach Web Application Firewall (WAF) to ALB but not NLB or CLB We can attach Amazon Certification Manager SSL to any of the ELB for SSL termination ALB has advanced Request Routing rules where you can route based on subdomain header, path and other HTTP(S) information Sticky Sessions can be enabled for CLB or ALB and sessions are remembered via Cookies","title":"ELB"},{"location":"aws/EC2-ELB/#ece-follow-along","text":"In order to be able to use Session Manager to login to an EC2 instance, its IAM role should have AmazonEC2RoleforSSM policy or better the recommended one AmazonSSMManagedInstanceCore # Get the user-data, the script that was used after luanching the instace curl http://169.254.169.254/latest/user-data # Get the instance meta-data like ami-id, security groups ... curl http://169.254.169.254/latest/meta-data/ami-id Encrypt a running EBS Create a snapshot, check the \"Encryption\" checkbox and select a \"Master key\" Create an Image (AMI) from the EBS Snapshot Launch Configuration vs Launch Template Launch Configuration: AMI Machine Type Volumes IAM profile Security group User data Meta data Auto Scaling Group VPC Subnets Auto scaling group ensures there is enough number of EC2 machines (in the same AZ ?) but If the Whole AZ goes out, the Auto Scaling group won't help, we need to create high availability using load balancers so we can run instances in more that one AZ I couldn't change the security group inbound rule for HTTP to just accept \"Source\" from another security but I was able to create another security group with that same rule","title":"ECE Follow along"},{"location":"aws/EFS-EBS/","text":"EFS Scalable, elastic cloud-native NFS file system. Attach a single file system to multiple EC2 instances. Don't worry about running out or managing disk space supports NFSv4 protocol You pay arround (starting from ?) 0.3$ GB / month Volumes can scale to petabyte size storage Volumes will shrink and grow to meet current data stored (elastic) Can support thousands of concurrent connections over NFS You data is stored across multiple AZs within a region Can mount multiple EC2 instance to a single EFS (as long as they are all in the same VPC) Creates Mount Points in all your VPC subnets so you can mount from anywhere whithin your VPC Provides Read After Write Consistency The mount point should have a security group that allows NFS access from the instances's security group EBS Virtual hard drive in the could. Create new volumes attach to EC2 instances, backup via snapshots and easy encryption. Volumes are automatically replicated whithin their AZ to protect from component failure IOPS stands for Inut/Output per second. It is the speed at which non-contiguous reads and writes can be performed on a storage medium. high I/O = lots of small fast reads and writes. Throughput is the data transfer rate to and from the storage meduim megabytes per second. Bandwidth is the measurement of the total possible speed of data movement along the network Think of Bandwidth as the Pipe and Throughput as the water EBS Types SSD General Purpouse SSD (gp2) Provisionned IOPS SSD (io1) ![[aws-ebs-types.png]] HDD Throughput Optimized HDD (st1) Cold HDD (sc1) ![[aws-ebs-types-hdd.png]] Magnetic EBS Magnetic (standard) ![[aws-ebs-types-magnetic.png]] Moving volumes From one AZ to another 1. Take snapshot of the volume 2. Create an AMI from the snapshot 3. launch new EC2 instance in desired AZ From one Region to another In addition to the steps above we have to copy the AMI to the other region Encrypt existing root volume just select the encryption option when creating the snapshot EBS vs Instance Store Volumes An EC2 instance can be backed (root device) by an EBS Volume or Instance Store Volume Instance Store Volume: Temporary (Ephemeral), located on disks that are physically attached to a host machine, created from a template stored in S3, Cannot stop instances, only reboot or temrminate, Data is lost in case of health fails or termination EBS Volume: Durable, created from EBS Snapshot, Can start/stop instances, Data persist if you reboot your system. Volumes exists on EBS, Snapshots exist on S3. Snapshots are incremental, only changes made since the last snapshot are moved to S3. If talking Snapshot of a root volume, the EC2 instnace should be stopped before Snapshotting. You can take Snapshots while the instance is running (if it's not the root volume ?) You can create AMIs from Snapshots or Volumes Volumes always exist in the same AZ as the EC2 instance By default root volumes are deleted on termination which can be disalbed (termination protection) Snapshots or restored encrypted volumes will also be encrypted You cannot share a snapshot if it has been encrypted. Unencrpted ones can be shared with other AWS accounts or made public","title":"EFS"},{"location":"aws/EFS-EBS/#efs","text":"Scalable, elastic cloud-native NFS file system. Attach a single file system to multiple EC2 instances. Don't worry about running out or managing disk space supports NFSv4 protocol You pay arround (starting from ?) 0.3$ GB / month Volumes can scale to petabyte size storage Volumes will shrink and grow to meet current data stored (elastic) Can support thousands of concurrent connections over NFS You data is stored across multiple AZs within a region Can mount multiple EC2 instance to a single EFS (as long as they are all in the same VPC) Creates Mount Points in all your VPC subnets so you can mount from anywhere whithin your VPC Provides Read After Write Consistency The mount point should have a security group that allows NFS access from the instances's security group","title":"EFS"},{"location":"aws/EFS-EBS/#ebs","text":"Virtual hard drive in the could. Create new volumes attach to EC2 instances, backup via snapshots and easy encryption. Volumes are automatically replicated whithin their AZ to protect from component failure IOPS stands for Inut/Output per second. It is the speed at which non-contiguous reads and writes can be performed on a storage medium. high I/O = lots of small fast reads and writes. Throughput is the data transfer rate to and from the storage meduim megabytes per second. Bandwidth is the measurement of the total possible speed of data movement along the network Think of Bandwidth as the Pipe and Throughput as the water","title":"EBS"},{"location":"aws/EFS-EBS/#ebs-types","text":"","title":"EBS Types"},{"location":"aws/EFS-EBS/#ssd","text":"General Purpouse SSD (gp2) Provisionned IOPS SSD (io1) ![[aws-ebs-types.png]]","title":"SSD"},{"location":"aws/EFS-EBS/#hdd","text":"Throughput Optimized HDD (st1) Cold HDD (sc1) ![[aws-ebs-types-hdd.png]]","title":"HDD"},{"location":"aws/EFS-EBS/#magnetic","text":"EBS Magnetic (standard) ![[aws-ebs-types-magnetic.png]]","title":"Magnetic"},{"location":"aws/EFS-EBS/#moving-volumes","text":"From one AZ to another 1. Take snapshot of the volume 2. Create an AMI from the snapshot 3. launch new EC2 instance in desired AZ From one Region to another In addition to the steps above we have to copy the AMI to the other region Encrypt existing root volume just select the encryption option when creating the snapshot","title":"Moving volumes"},{"location":"aws/EFS-EBS/#ebs-vs-instance-store-volumes","text":"An EC2 instance can be backed (root device) by an EBS Volume or Instance Store Volume Instance Store Volume: Temporary (Ephemeral), located on disks that are physically attached to a host machine, created from a template stored in S3, Cannot stop instances, only reboot or temrminate, Data is lost in case of health fails or termination EBS Volume: Durable, created from EBS Snapshot, Can start/stop instances, Data persist if you reboot your system. Volumes exists on EBS, Snapshots exist on S3. Snapshots are incremental, only changes made since the last snapshot are moved to S3. If talking Snapshot of a root volume, the EC2 instnace should be stopped before Snapshotting. You can take Snapshots while the instance is running (if it's not the root volume ?) You can create AMIs from Snapshots or Volumes Volumes always exist in the same AZ as the EC2 instance By default root volumes are deleted on termination which can be disalbed (termination protection) Snapshots or restored encrypted volumes will also be encrypted You cannot share a snapshot if it has been encrypted. Unencrpted ones can be shared with other AWS accounts or made public","title":"EBS vs Instance Store Volumes"},{"location":"aws/High%20Availability%20-%20Scaling/","text":"High Availability The ability for a system to remain available Think about what could cause a service to become unavailable When an AZ becomes unavailable eg. data-center flooded When a Region becomes unavailable eg. meteor strike When an web-application becomes unresponsive eg. too much traffic When an instance becomes unavailable eg. instance failure When a web application becomes unresponsive due to distance in geographic location The solution we need to implement in order to ensure High Availability: We should run our instances in Multi-AZ, an Elastic Load Balancer can route traffic to operational AZs. We should run instances in another region. We can route traffic to another Region via Route53 We should use Auto Scaling Groups to increase the amount of instances to meet the demand of traffic We should use Auto Scaling Groups to ensure a minimum amount of instances are running and have ELB route traffic to healthy instances We should use CloudFront to cache static content for faster delivery in nearby regions. We can also run our instances in nearby regions and route traffic using a geolocation policy in Route53 Scale Up vs Scale Out When utilization increases and we are reaching capacity we can: Scale up (Vertical Scaling) Increasing the size of instances - Simpler to manage. - Lower availability (if a single instance fails service becomes unavailable) Scale out (Horizontal Scaling) Adding more of the same - More complexity to manage. - Higher availability (if a single instance fail it doesn't matter) You will generally want to scale out and then up to balance complexity vs availability","title":"High Availability"},{"location":"aws/High%20Availability%20-%20Scaling/#high-availability","text":"The ability for a system to remain available","title":"High Availability"},{"location":"aws/High%20Availability%20-%20Scaling/#think-about-what-could-cause-a-service-to-become-unavailable","text":"When an AZ becomes unavailable eg. data-center flooded When a Region becomes unavailable eg. meteor strike When an web-application becomes unresponsive eg. too much traffic When an instance becomes unavailable eg. instance failure When a web application becomes unresponsive due to distance in geographic location","title":"Think about what could cause a service to become unavailable"},{"location":"aws/High%20Availability%20-%20Scaling/#the-solution-we-need-to-implement-in-order-to-ensure-high-availability","text":"We should run our instances in Multi-AZ, an Elastic Load Balancer can route traffic to operational AZs. We should run instances in another region. We can route traffic to another Region via Route53 We should use Auto Scaling Groups to increase the amount of instances to meet the demand of traffic We should use Auto Scaling Groups to ensure a minimum amount of instances are running and have ELB route traffic to healthy instances We should use CloudFront to cache static content for faster delivery in nearby regions. We can also run our instances in nearby regions and route traffic using a geolocation policy in Route53","title":"The solution we need to implement in order to ensure High Availability:"},{"location":"aws/High%20Availability%20-%20Scaling/#scale-up-vs-scale-out","text":"When utilization increases and we are reaching capacity we can:","title":"Scale Up vs Scale Out"},{"location":"aws/High%20Availability%20-%20Scaling/#scale-up-vertical-scaling","text":"Increasing the size of instances - Simpler to manage. - Lower availability (if a single instance fails service becomes unavailable)","title":"Scale up (Vertical Scaling)"},{"location":"aws/High%20Availability%20-%20Scaling/#scale-out-horizontal-scaling","text":"Adding more of the same - More complexity to manage. - Higher availability (if a single instance fail it doesn't matter) You will generally want to scale out and then up to balance complexity vs availability","title":"Scale out (Horizontal Scaling)"},{"location":"aws/IAM/","text":"IAM IAM allows management for users and resoruces Policies are attached to IAM identites (Users, Groups and Roles) - Common usecase: users belong to a group. Polies are attached to roles. Roles can be applied to groups to quickly add and remove permissions en-masse to users - Policies can be attached directly to users, theses are called \"inline policies\" - Roles can have many policies attached - Various AWS resources can have roles directly attached to them Types of policies Managed Policies: most common permission we can neeed (orange box) Custom Manager Policies Inline Policies Policies Version: policy language version, 2012-10-17 is the latest version Statement SId (optional): just a label Effect: Allow or Deny Action: list of actions Principal: account, user, role or federated user Resource Codition Password policy Set minimum requirement, rotate passwords so users have to udpate their passwords after X days Programmatic access keys Interact with AWS using the CLI, SDK or REST API Maximum is 2 keys per users, if we want more we have to remove old ones. MFA Admin can't require users to setup MFA but they can allow access to some resources only to users who have MFA.","title":"IAM"},{"location":"aws/IAM/#iam","text":"IAM allows management for users and resoruces Policies are attached to IAM identites (Users, Groups and Roles) - Common usecase: users belong to a group. Polies are attached to roles. Roles can be applied to groups to quickly add and remove permissions en-masse to users - Policies can be attached directly to users, theses are called \"inline policies\" - Roles can have many policies attached - Various AWS resources can have roles directly attached to them","title":"IAM"},{"location":"aws/IAM/#types-of-policies","text":"Managed Policies: most common permission we can neeed (orange box) Custom Manager Policies Inline Policies","title":"Types of policies"},{"location":"aws/IAM/#policies","text":"Version: policy language version, 2012-10-17 is the latest version Statement SId (optional): just a label Effect: Allow or Deny Action: list of actions Principal: account, user, role or federated user Resource Codition","title":"Policies"},{"location":"aws/IAM/#password-policy","text":"Set minimum requirement, rotate passwords so users have to udpate their passwords after X days","title":"Password policy"},{"location":"aws/IAM/#programmatic-access-keys","text":"Interact with AWS using the CLI, SDK or REST API Maximum is 2 keys per users, if we want more we have to remove old ones.","title":"Programmatic access keys"},{"location":"aws/IAM/#mfa","text":"Admin can't require users to setup MFA but they can allow access to some resources only to users who have MFA.","title":"MFA"},{"location":"aws/Kennisis/","text":"Kennisis Amazon Kinesis is the AWS solution for collecting, processing, and analyzing streaming data in the cloud. When you need \"real-time\" think Kinesis. - Kinesis Data Streams Per per running shard, data can persist within the stream, data is ordered and every consumer keep its own position. Consumers have to be manually added (coded), Data persists for 24 hours (default) to 168 hours. - Kinesis Firehose - Pay for only the data ingested, data immediately disappears once processed. - Consumer of choice is from a predefined set of services: S3, Redshift, Elasticsearch or Splunk - Kinesis Data Analytics - allows you to perform queries in real-time. Needs a Kinesis Data - Streams/Firehose as the input and output. - Kinesis Video Analytics securely ingests and stores video and audio encoded data to consumers such as SageMaker, Rekognition or other services to apply Machine learning and video processing. - KPL (Kinesis Producer Library) is a Java library to write data to a stream - You can write data to stream using AWS SDK, but KPL is more efficient","title":"Kennisis"},{"location":"aws/Kennisis/#kennisis","text":"Amazon Kinesis is the AWS solution for collecting, processing, and analyzing streaming data in the cloud. When you need \"real-time\" think Kinesis. - Kinesis Data Streams Per per running shard, data can persist within the stream, data is ordered and every consumer keep its own position. Consumers have to be manually added (coded), Data persists for 24 hours (default) to 168 hours. - Kinesis Firehose - Pay for only the data ingested, data immediately disappears once processed. - Consumer of choice is from a predefined set of services: S3, Redshift, Elasticsearch or Splunk - Kinesis Data Analytics - allows you to perform queries in real-time. Needs a Kinesis Data - Streams/Firehose as the input and output. - Kinesis Video Analytics securely ingests and stores video and audio encoded data to consumers such as SageMaker, Rekognition or other services to apply Machine learning and video processing. - KPL (Kinesis Producer Library) is a Java library to write data to a stream - You can write data to stream using AWS SDK, but KPL is more efficient","title":"Kennisis"},{"location":"aws/Lambda/","text":"Lambda Lambda's are serverless functions. You upload your code and it runs without you managing or provisioning any servers. - Lambda is serverless. You don't need to worry about underlying architecture - Lambda is a good fit for short running tasks where you don't need to customize the os environment. If you need long running tasks (> 15mins) and a custom OS environment than consider using Fargate - There are 7 runtime language environments officially supported by Lambda: Ruby, Python, Java, NodeJs, C#, Powershell and Go - You pay per invocation (The duration and the amount of memory used) rounded up to the nearest 100 milliseconds and you based on amount of requests. First IM requests per month are free - You can adjust the duration timeout for up to 15 mins and memory up to 3008 MB - You can trigger Lambdas from the SDK or multiple AWS services eg. S3, API Gateway, DynamoDB - Lambdas by default run in No VPC. To interact With some services you need to have your Lambda in the same VPC eg. RDS - Lambda can scale to 1000 of concurrent functions in seconds. (1000 is the default, you can increase With AWS Service Limit Increase) - Lambdas have Cold Starts. If a function has not been recently been execute there will be a delay","title":"Lambda"},{"location":"aws/Lambda/#lambda","text":"Lambda's are serverless functions. You upload your code and it runs without you managing or provisioning any servers. - Lambda is serverless. You don't need to worry about underlying architecture - Lambda is a good fit for short running tasks where you don't need to customize the os environment. If you need long running tasks (> 15mins) and a custom OS environment than consider using Fargate - There are 7 runtime language environments officially supported by Lambda: Ruby, Python, Java, NodeJs, C#, Powershell and Go - You pay per invocation (The duration and the amount of memory used) rounded up to the nearest 100 milliseconds and you based on amount of requests. First IM requests per month are free - You can adjust the duration timeout for up to 15 mins and memory up to 3008 MB - You can trigger Lambdas from the SDK or multiple AWS services eg. S3, API Gateway, DynamoDB - Lambdas by default run in No VPC. To interact With some services you need to have your Lambda in the same VPC eg. RDS - Lambda can scale to 1000 of concurrent functions in seconds. (1000 is the default, you can increase With AWS Service Limit Increase) - Lambdas have Cold Starts. If a function has not been recently been execute there will be a delay","title":"Lambda"},{"location":"aws/RDSandOthers/","text":"RDS AWS service for Relational Database Systems RDS instances are managed by AWS, you can't SSH into the instances Supports Aurora, MySQL, MariaDB, Postgres, Oracle, Microsoft SQL Server Multi-AZ makes an exact copy of your database in another AZ that is only standby, changes are automatically synchronized. Automatic fail over if one AZ goes out. Read replicas allow running multiple copies (up to 5) of the database cross region and can be combined with Multi-AZ. You can't write to read replicas and is intended to alleviate the workload of your primary database to improve performance. They use Async replication (??). Automated backups should be enabled. Read replica could be promoted to their own database but this breaks replication Automated backups, you choose a retention period from 1 to 35 days, There is no additional cost for backup storage, you define your backup window There are also Manual snapshots, if you delete your primary datase the snapshot will still exist and can be restored When you restore an instance it will create a new database. You just need to delete your old database and point trafiic to new restored database You can turn on encryption at-rest for RDS via KMS Migration from Postgres to Aurora depends on the version of the Database 12.5 and 9.6.20 versions of PostgresSQL are not supperted by aurora. What is supported is listed here . For aurora serverless you have to have backup enabled. Aurora Fully managed PostgreSQL (3x faster) or MySQL (5x faster) Automatic scaling and backups High availability with 2 copies in 3 AZs and Fault Tolerance 1/10 the cost over its competitor with similar performace and availability options Up to 15 Aurora replicas Can span multiple regions with Aurora Global Database Aurora serverless allow to start and stop and scale automatically with low-cost, ideal for new projects and/or infrequent database usage. RedShift DataWarehouse service with OLAP and Columnar store - Data can be loaded from S3, EMR, DynamoDB, or multiple data sources on remote hosts. - Redshift is Columnar Store database which can SQL-like queries and is an OLAP. - Redshift can handle petabytes worth of data. Redshift is for Data Warehousing - Redshift most common use case is Business Intelligence - Redshift can only run in a 1 availability zone (Single-AZ) - Reshift can run via a single node or multi-node (clusters) - A single node is 160 GB in size - A multi-node is comprised of a leader node and multiple compute nodes - You are bill per hour for each node (excluding leader node in multi-node) - You are not billed for the leader node - You can have up to 128 compute nodes - Redshift has two kinds of Node Type Dense Compute and Dense Storage - Redshift attempts to backup 3 copies of your data, the original, on compute node and on S3 - Similar data is stored on disk sequentially for faster reads - Redshift database can be encrypted via KMS or CloudHSM - Backup Retention is default to 1 day and can be increase to maximum of 35 days - Reshift can asynchronously back up your snapshot to Another Region delivered to S3 - Redshift uses Massively Parallel Processing (MPP) to distribute queries and data across all loads - ln the case of empty table, when importing Redshift will sample data to create a schema DynamoDB DynamoDB is a fully managed NoSQL key/value and document database. - Applications that contain large amounts of data but require predictable read and write performance while scaling is a good fit for DynamoDB - DynamoDB scales With whatever read and write capacity you specific per second. - DynamoDB can be set to have Eventually Consistent Reads (default) and Strongly Consistent Reads - Eventually consistent reads data is returned immediately but data can be inconsistent. Copies of data will be generally consistent in 1 second. - Strongly Consistent Reads will wait until data in consistent. Data will never be inconsistent but latency will be higher. Copies of data will be consistent With a guarantee ofl second. - DynamoDB stores 3 copies of data on SSD drives across 3 regions.","title":"RDS"},{"location":"aws/RDSandOthers/#rds","text":"AWS service for Relational Database Systems RDS instances are managed by AWS, you can't SSH into the instances Supports Aurora, MySQL, MariaDB, Postgres, Oracle, Microsoft SQL Server Multi-AZ makes an exact copy of your database in another AZ that is only standby, changes are automatically synchronized. Automatic fail over if one AZ goes out. Read replicas allow running multiple copies (up to 5) of the database cross region and can be combined with Multi-AZ. You can't write to read replicas and is intended to alleviate the workload of your primary database to improve performance. They use Async replication (??). Automated backups should be enabled. Read replica could be promoted to their own database but this breaks replication Automated backups, you choose a retention period from 1 to 35 days, There is no additional cost for backup storage, you define your backup window There are also Manual snapshots, if you delete your primary datase the snapshot will still exist and can be restored When you restore an instance it will create a new database. You just need to delete your old database and point trafiic to new restored database You can turn on encryption at-rest for RDS via KMS Migration from Postgres to Aurora depends on the version of the Database 12.5 and 9.6.20 versions of PostgresSQL are not supperted by aurora. What is supported is listed here . For aurora serverless you have to have backup enabled.","title":"RDS"},{"location":"aws/RDSandOthers/#aurora","text":"Fully managed PostgreSQL (3x faster) or MySQL (5x faster) Automatic scaling and backups High availability with 2 copies in 3 AZs and Fault Tolerance 1/10 the cost over its competitor with similar performace and availability options Up to 15 Aurora replicas Can span multiple regions with Aurora Global Database Aurora serverless allow to start and stop and scale automatically with low-cost, ideal for new projects and/or infrequent database usage.","title":"Aurora"},{"location":"aws/RDSandOthers/#redshift","text":"DataWarehouse service with OLAP and Columnar store - Data can be loaded from S3, EMR, DynamoDB, or multiple data sources on remote hosts. - Redshift is Columnar Store database which can SQL-like queries and is an OLAP. - Redshift can handle petabytes worth of data. Redshift is for Data Warehousing - Redshift most common use case is Business Intelligence - Redshift can only run in a 1 availability zone (Single-AZ) - Reshift can run via a single node or multi-node (clusters) - A single node is 160 GB in size - A multi-node is comprised of a leader node and multiple compute nodes - You are bill per hour for each node (excluding leader node in multi-node) - You are not billed for the leader node - You can have up to 128 compute nodes - Redshift has two kinds of Node Type Dense Compute and Dense Storage - Redshift attempts to backup 3 copies of your data, the original, on compute node and on S3 - Similar data is stored on disk sequentially for faster reads - Redshift database can be encrypted via KMS or CloudHSM - Backup Retention is default to 1 day and can be increase to maximum of 35 days - Reshift can asynchronously back up your snapshot to Another Region delivered to S3 - Redshift uses Massively Parallel Processing (MPP) to distribute queries and data across all loads - ln the case of empty table, when importing Redshift will sample data to create a schema","title":"RedShift"},{"location":"aws/RDSandOthers/#dynamodb","text":"DynamoDB is a fully managed NoSQL key/value and document database. - Applications that contain large amounts of data but require predictable read and write performance while scaling is a good fit for DynamoDB - DynamoDB scales With whatever read and write capacity you specific per second. - DynamoDB can be set to have Eventually Consistent Reads (default) and Strongly Consistent Reads - Eventually consistent reads data is returned immediately but data can be inconsistent. Copies of data will be generally consistent in 1 second. - Strongly Consistent Reads will wait until data in consistent. Data will never be inconsistent but latency will be higher. Copies of data will be consistent With a guarantee ofl second. - DynamoDB stores 3 copies of data on SSD drives across 3 regions.","title":"DynamoDB"},{"location":"aws/S3/","text":"S3 Questions Q: What is Durability ? What is Availability ? A: Availability measures how readily available a service is. Durability is used to measure the likelihood of data loss. More on this blog Q: When MFA, only the root can delete objects ? is this guy for real ? A: What does \"Min storage duration\" in Storage class ? > From Amazon Docs : > Note: The S3 Standard-IA and S3 One Zone-IA storage classes are suitable for objects larger than 128 KB that you plan to store for at least 30 days. If an object is less than 128 KB, Amazon S3 charges you for 128 KB. If you delete an object before the end of the 30-day minimum storage duration period, you are charged for 30 days. Q: What is \"Object Lock\" ? A: > Store objects using a write-once-read-many (WORM) model to help you prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. Learn more Q: What IAM role should we use to create replication and replication jobs ? A: Apparently This is a though one, read more here Q: After answering the previous question, set up a replication on AWS console to see if I get everything right Intro data storage architecture that manages data as objects , as opposed to other storage architectures like file systems and block storage. Stores unlimited amount of data without worry of underlying storage infrastructure. S3 Object S3 Object contain data which can go from 0 Bytes to 5 Terabytes and has: Key, Value, Version ID and Metadata S3 Buckets hold objects. Buckets can also have folders which in trun hold objects S3 is a universal namespace so bucket names mush be unique (think like having a domain name) Classes AZ: Availabilty Zone - Standard: Fast! 99.99% Availability, 11 9's Durability, 3 AZs - Standard Infrequently Accessed (IA): Still fast! Cheaper if you access files < once a month. Additional retreival fee. 50% less than standard (reduced availabilty) - One Zone IA: Still Fast! 1 AZ (99.5%). Cheaper than Standard IA by 20% less (reduced durability) data could get destroyed. Retreival fee is applied. - Glacier: long term cold storage. Retreival could take minutes to hours. Very cheap - Glacier Deep Archive: Lowest cost. Retreival is 12 hours. - Intelligent tiering: Uses ML to analyze your object usage and determine the appropriate stoage class. DAta is moved to the most cost-effective access tier, without any performance impact or added overhead. ![[s3-storage-class-comparison-new.png]] Security All buckets are private by default Logging per request in another bucket even in another account if desired ACL are a legacy feature, simpler but not recommended Bucket policies json document that define a complex rule access Encryption Trafic is encrypted via SSL/TLS by default Server side encryption (SSE): SSE-AES S3 handles the key, uses AES-256 algo SSE-KMS Envelope encryption, AWS KMS and you manage the keys SSE-C Customer provided key Client side encryption: you encrypt the files before uploading them to S3 Data Consistency For new objects (PUTS) : Read After Write consistency, you are able to read immediately after writing Overwite (PUTS) or Delete objects (DELETES) : Eventual Consistency, it may return an old copy if you read immeditely, it may take time for S3 to replicate versions to AZs, you need to generally wait a few seconds before reading. Cross-Region Replication When enabled, any object that is uplaoded will be automatically replicated to another region(s) Versionning must be turned on on both the source and the destination buckets. We can have CCR replicate to another AWS account Versionning Once enabled it cannot be disabled for already versionned files, it can only suspended Fully integrates with S3 lifecycle rules When working with versionning (does that mean when it's enabled?), multi factor authentication can provide extra protection against deletion of data Lifecycle Management Automate the process of moving files to diffrent Storage classes or deleting files altogether Can be used together with versionning and can be applied to bothe current and previous versions Example: [Standard] --> (After 7 days) --> [Glacier] --> (After 356 days) --> [Delete permanetly] Transfer acceleration Fast and secure transfer between s3 and users Users use a distinct URL for an Edge LOcation As data arrives to the Edge Location it is automatically routes to s3 over a specially optimized network path called Amazon Backbone Network Preseiged URL Temporarly access to private objects, only from CLI and SDK, have an expiration time MFA Delete CLI must be used to turn on MFA Versionning must be turned on for the bucket Only the bucket owner root can delete objects from the bucket !!! Is this guy for real !!! , said here: https://youtu.be/Ia-UEYYR44s?t=1898 and here: https://youtu.be/Ia-UEYYR44s?t=3669 Ensure objects are not deleted by accident Follow along: Messing arround with AWS By default we can't make objects public , we have to, first, uncheck the \"Block all public\" checkbox first in the permission tab of the bucket (not the object) Files uploaded before versioning is enabled will have \"null\" versions new versions don't inherit the public access When we delete an object, we actionaly delete the last version. When an object is encrypted, we still can access it, it just mean it's encrypted in the servers # List all buckets aws s3 ls # List all folders and objects in bucket aws s3 ls s3 : // example # Download aws s3 cp s3 : // example / folder / file . png # Upload aws s3 cp / Downloads / file . png s3 : // example / folder / file . png # Create preseigned URL aws s3 presign s3 : // example -- exprires - in 300 # seconds Minimum value for transition in lifecycle managment is 30 days We can change the storage class and the object ownership (with an account id) when creating Cross Region replication S3 succeful upload return 200 HTTP Getting hands dirty Policies are tied to buckets while ACL could be applied to indivual objects When we try to set Permissions for an object before Uploading it, we get this message: This bucket has the bucket owner enforced setting applied for Object Ownership. Use bucket policies to control access. Learn more User defined Metadata keys should be prefixed with x-amz-meta Bucket policies could be used to require encrypted uploads > If your bucket policy requires encrypted uploads, you must specify an encryption key or your upload will fail. When uploading objects we can configure a checksum function for additional data integrity validation There is another Storage class than the ones listed above: Glacier Instant Retreival and the formelly called Glacier is now called Glacier Flexible Retreival Server side encryption Enabling SSE for a bucket doesn't afface already uploaded objects There are only 2 options for server side encryption: SSE-S3 and SSE-KMS but there is this note: > To upload an object with a customer-provided encryption key (SSE-C), use the AWS CLI, AWS SDK, or Amazon S3 REST API. and according to this guide SSE-C properties are specified in request headers (HTTP ?): - x-amz-server-side\u200b-encryption\u200b-customer-algorithm - x-amz-server-side\u200b-encryption\u200b-customer-key - x-amz-server-side\u200b-encryption\u200b-customer-key-MD5 Edit server-side encryption for an object > - This action creates a new version of the object with updated settings and a new last-modified date. > - Objects copied with customer-provided encryption keys (SSE-C) will fail to be copied using the S3 console. To copy objects encrypted with SSE-C, use the AWS CLI, AWS SDK, or the Amazon S3 REST API. > - If this bucket uses the bucket owner enforced setting for S3 Object Ownership, object ACLs will not be copied. > Learn more Access points https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points.html Amazon S3 access points simplify data access for any AWS service or customer application that stores data in S3. Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations, such as GetObject and PutObject . Each access point has distinct permissions and network controls that S3 applies for any request that is made through that access point. Each access point enforces a customized access point policy that works in conjunction with the bucket policy that is attached to the underlying bucket. You can configure any access point to accept requests only from a virtual private cloud (VPC) to restrict Amazon S3 data access to a private network. You can also configure custom block public access settings for each access point.","title":"S3"},{"location":"aws/S3/#s3","text":"","title":"S3"},{"location":"aws/S3/#questions","text":"Q: What is Durability ? What is Availability ? A: Availability measures how readily available a service is. Durability is used to measure the likelihood of data loss. More on this blog Q: When MFA, only the root can delete objects ? is this guy for real ? A: What does \"Min storage duration\" in Storage class ? > From Amazon Docs : > Note: The S3 Standard-IA and S3 One Zone-IA storage classes are suitable for objects larger than 128 KB that you plan to store for at least 30 days. If an object is less than 128 KB, Amazon S3 charges you for 128 KB. If you delete an object before the end of the 30-day minimum storage duration period, you are charged for 30 days. Q: What is \"Object Lock\" ? A: > Store objects using a write-once-read-many (WORM) model to help you prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. Learn more Q: What IAM role should we use to create replication and replication jobs ? A: Apparently This is a though one, read more here Q: After answering the previous question, set up a replication on AWS console to see if I get everything right","title":"Questions"},{"location":"aws/S3/#intro","text":"data storage architecture that manages data as objects , as opposed to other storage architectures like file systems and block storage. Stores unlimited amount of data without worry of underlying storage infrastructure.","title":"Intro"},{"location":"aws/S3/#s3-object","text":"S3 Object contain data which can go from 0 Bytes to 5 Terabytes and has: Key, Value, Version ID and Metadata S3 Buckets hold objects. Buckets can also have folders which in trun hold objects S3 is a universal namespace so bucket names mush be unique (think like having a domain name)","title":"S3 Object"},{"location":"aws/S3/#classes","text":"AZ: Availabilty Zone - Standard: Fast! 99.99% Availability, 11 9's Durability, 3 AZs - Standard Infrequently Accessed (IA): Still fast! Cheaper if you access files < once a month. Additional retreival fee. 50% less than standard (reduced availabilty) - One Zone IA: Still Fast! 1 AZ (99.5%). Cheaper than Standard IA by 20% less (reduced durability) data could get destroyed. Retreival fee is applied. - Glacier: long term cold storage. Retreival could take minutes to hours. Very cheap - Glacier Deep Archive: Lowest cost. Retreival is 12 hours. - Intelligent tiering: Uses ML to analyze your object usage and determine the appropriate stoage class. DAta is moved to the most cost-effective access tier, without any performance impact or added overhead. ![[s3-storage-class-comparison-new.png]]","title":"Classes"},{"location":"aws/S3/#security","text":"All buckets are private by default Logging per request in another bucket even in another account if desired ACL are a legacy feature, simpler but not recommended Bucket policies json document that define a complex rule access","title":"Security"},{"location":"aws/S3/#encryption","text":"Trafic is encrypted via SSL/TLS by default Server side encryption (SSE): SSE-AES S3 handles the key, uses AES-256 algo SSE-KMS Envelope encryption, AWS KMS and you manage the keys SSE-C Customer provided key Client side encryption: you encrypt the files before uploading them to S3","title":"Encryption"},{"location":"aws/S3/#data-consistency","text":"For new objects (PUTS) : Read After Write consistency, you are able to read immediately after writing Overwite (PUTS) or Delete objects (DELETES) : Eventual Consistency, it may return an old copy if you read immeditely, it may take time for S3 to replicate versions to AZs, you need to generally wait a few seconds before reading.","title":"Data Consistency"},{"location":"aws/S3/#cross-region-replication","text":"When enabled, any object that is uplaoded will be automatically replicated to another region(s) Versionning must be turned on on both the source and the destination buckets. We can have CCR replicate to another AWS account","title":"Cross-Region Replication"},{"location":"aws/S3/#versionning","text":"Once enabled it cannot be disabled for already versionned files, it can only suspended Fully integrates with S3 lifecycle rules When working with versionning (does that mean when it's enabled?), multi factor authentication can provide extra protection against deletion of data","title":"Versionning"},{"location":"aws/S3/#lifecycle-management","text":"Automate the process of moving files to diffrent Storage classes or deleting files altogether Can be used together with versionning and can be applied to bothe current and previous versions Example: [Standard] --> (After 7 days) --> [Glacier] --> (After 356 days) --> [Delete permanetly]","title":"Lifecycle Management"},{"location":"aws/S3/#transfer-acceleration","text":"Fast and secure transfer between s3 and users Users use a distinct URL for an Edge LOcation As data arrives to the Edge Location it is automatically routes to s3 over a specially optimized network path called Amazon Backbone Network","title":"Transfer acceleration"},{"location":"aws/S3/#preseiged-url","text":"Temporarly access to private objects, only from CLI and SDK, have an expiration time","title":"Preseiged URL"},{"location":"aws/S3/#mfa-delete","text":"CLI must be used to turn on MFA Versionning must be turned on for the bucket Only the bucket owner root can delete objects from the bucket !!! Is this guy for real !!! , said here: https://youtu.be/Ia-UEYYR44s?t=1898 and here: https://youtu.be/Ia-UEYYR44s?t=3669 Ensure objects are not deleted by accident","title":"MFA Delete"},{"location":"aws/S3/#follow-along-messing-arround-with-aws","text":"By default we can't make objects public , we have to, first, uncheck the \"Block all public\" checkbox first in the permission tab of the bucket (not the object) Files uploaded before versioning is enabled will have \"null\" versions new versions don't inherit the public access When we delete an object, we actionaly delete the last version. When an object is encrypted, we still can access it, it just mean it's encrypted in the servers # List all buckets aws s3 ls # List all folders and objects in bucket aws s3 ls s3 : // example # Download aws s3 cp s3 : // example / folder / file . png # Upload aws s3 cp / Downloads / file . png s3 : // example / folder / file . png # Create preseigned URL aws s3 presign s3 : // example -- exprires - in 300 # seconds Minimum value for transition in lifecycle managment is 30 days We can change the storage class and the object ownership (with an account id) when creating Cross Region replication S3 succeful upload return 200 HTTP","title":"Follow along: Messing arround with AWS"},{"location":"aws/S3/#getting-hands-dirty","text":"Policies are tied to buckets while ACL could be applied to indivual objects When we try to set Permissions for an object before Uploading it, we get this message: This bucket has the bucket owner enforced setting applied for Object Ownership. Use bucket policies to control access. Learn more User defined Metadata keys should be prefixed with x-amz-meta Bucket policies could be used to require encrypted uploads > If your bucket policy requires encrypted uploads, you must specify an encryption key or your upload will fail. When uploading objects we can configure a checksum function for additional data integrity validation There is another Storage class than the ones listed above: Glacier Instant Retreival and the formelly called Glacier is now called Glacier Flexible Retreival","title":"Getting hands dirty"},{"location":"aws/S3/#server-side-encryption","text":"Enabling SSE for a bucket doesn't afface already uploaded objects There are only 2 options for server side encryption: SSE-S3 and SSE-KMS but there is this note: > To upload an object with a customer-provided encryption key (SSE-C), use the AWS CLI, AWS SDK, or Amazon S3 REST API. and according to this guide SSE-C properties are specified in request headers (HTTP ?): - x-amz-server-side\u200b-encryption\u200b-customer-algorithm - x-amz-server-side\u200b-encryption\u200b-customer-key - x-amz-server-side\u200b-encryption\u200b-customer-key-MD5 Edit server-side encryption for an object > - This action creates a new version of the object with updated settings and a new last-modified date. > - Objects copied with customer-provided encryption keys (SSE-C) will fail to be copied using the S3 console. To copy objects encrypted with SSE-C, use the AWS CLI, AWS SDK, or the Amazon S3 REST API. > - If this bucket uses the bucket owner enforced setting for S3 Object Ownership, object ACLs will not be copied. > Learn more","title":"Server side encryption"},{"location":"aws/S3/#access-points","text":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points.html Amazon S3 access points simplify data access for any AWS service or customer application that stores data in S3. Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations, such as GetObject and PutObject . Each access point has distinct permissions and network controls that S3 applies for any request that is made through that access point. Each access point enforces a customized access point policy that works in conjunction with the bucket policy that is attached to the underlying bucket. You can configure any access point to accept requests only from a virtual private cloud (VPC) to restrict Amazon S3 data access to a private network. You can also configure custom block public access settings for each access point.","title":"Access points"},{"location":"aws/SQS-SNS-ElastiCache/","text":"SQS SQS is a queuing service using messages With a queue. Think Sidekiq or RabbitMQ - SQS is used for Application Integration, it lets decoupled services and apps to talk to each other - To read SQS use need to pull the queue using the AWS SDK. SQS is not pushed-based - SQS supports both Standard and First-ln-First-Out (FIFO) queues - Standard allows nearly unlimited messages per second, does not guarantee order of delivery, always delivers at least once, you must protect again duplicate messages being processed - FIFO maintain the order of messages With a 300 messages per second limit - Short polling returns messages immediately, even if the message queue being polled is empty. - Long polling waits until message arrives in the queue, or the long poll timeout expires. ln majority of cases Long polling is preferred over short polling. its less expensive since it avoids empty reads - Visibility time-out is the period of time that messages are invisible in the SQS queue - Messages will be deleted from queue after a job has processed. (before visibility timeout expires) - If Visibility timeout expires than a job will become visible to the queue - The default Visibility time-out is 30 seconds. Timeout can be 0 seconds to a maximum of 12 hours. - SQS can retain messages from 60 seconds to 14 days and by default is 4 days - Message size between 1 byte to 256 kb, Extended Client Library for Java can increase to 2GB SNS Simple Notification Service (SNS) is a fully managed pub/sub messaging service - SNS is for Application Integration. It allows decoupled services and apps to communicate With each other - Topic a logical access point and communication channel. - A topic is able to deliver to multiple protocols - You can encrypt topics via KMS - Publishers use the AWS API via AWS CLI or SDK to push messages to a topic. Many AWS services integrate With SNS and act as publishers. - Subscriptions subscribe to topics. When a topic receives a message it automatically and immediately pushes messages to subscribers. - All messages published to SNS are stored redundantly across multiple Availability Zones (AZ). - The following protocols: - HTTPs and HTTPs create webhooks into your web-application - Email good for internal email notifications (only supports plain text) - Email-JSON sends you json via email - Amazon SQS place SNS message into SQS queue - AWS Lambda triggers a lambda function - SMS send a text message - Platform application endpoints Mobile Push eg. Apple, Google, Microsoft Baidu notification systems ElastiCache Manage cache in-memory Data store like Memcached and Redis - Memcached is a simple key / value store preferred for caching HTML fragments and is arguably faster than Redis - Redis has richer data types and operations. Great for leaderboard, geospatial data or keeping track of unread notifications. - Most frequently identical queries are stored in the cache - Resources only within the same VPC may connect to ElastiCache to ensure Iow latencies.","title":"SQS"},{"location":"aws/SQS-SNS-ElastiCache/#sqs","text":"SQS is a queuing service using messages With a queue. Think Sidekiq or RabbitMQ - SQS is used for Application Integration, it lets decoupled services and apps to talk to each other - To read SQS use need to pull the queue using the AWS SDK. SQS is not pushed-based - SQS supports both Standard and First-ln-First-Out (FIFO) queues - Standard allows nearly unlimited messages per second, does not guarantee order of delivery, always delivers at least once, you must protect again duplicate messages being processed - FIFO maintain the order of messages With a 300 messages per second limit - Short polling returns messages immediately, even if the message queue being polled is empty. - Long polling waits until message arrives in the queue, or the long poll timeout expires. ln majority of cases Long polling is preferred over short polling. its less expensive since it avoids empty reads - Visibility time-out is the period of time that messages are invisible in the SQS queue - Messages will be deleted from queue after a job has processed. (before visibility timeout expires) - If Visibility timeout expires than a job will become visible to the queue - The default Visibility time-out is 30 seconds. Timeout can be 0 seconds to a maximum of 12 hours. - SQS can retain messages from 60 seconds to 14 days and by default is 4 days - Message size between 1 byte to 256 kb, Extended Client Library for Java can increase to 2GB","title":"SQS"},{"location":"aws/SQS-SNS-ElastiCache/#sns","text":"Simple Notification Service (SNS) is a fully managed pub/sub messaging service - SNS is for Application Integration. It allows decoupled services and apps to communicate With each other - Topic a logical access point and communication channel. - A topic is able to deliver to multiple protocols - You can encrypt topics via KMS - Publishers use the AWS API via AWS CLI or SDK to push messages to a topic. Many AWS services integrate With SNS and act as publishers. - Subscriptions subscribe to topics. When a topic receives a message it automatically and immediately pushes messages to subscribers. - All messages published to SNS are stored redundantly across multiple Availability Zones (AZ). - The following protocols: - HTTPs and HTTPs create webhooks into your web-application - Email good for internal email notifications (only supports plain text) - Email-JSON sends you json via email - Amazon SQS place SNS message into SQS queue - AWS Lambda triggers a lambda function - SMS send a text message - Platform application endpoints Mobile Push eg. Apple, Google, Microsoft Baidu notification systems","title":"SNS"},{"location":"aws/SQS-SNS-ElastiCache/#elasticache","text":"Manage cache in-memory Data store like Memcached and Redis - Memcached is a simple key / value store preferred for caching HTML fragments and is arguably faster than Redis - Redis has richer data types and operations. Great for leaderboard, geospatial data or keeping track of unread notifications. - Most frequently identical queries are stored in the cache - Resources only within the same VPC may connect to ElastiCache to ensure Iow latencies.","title":"ElastiCache"},{"location":"aws/Snowball/","text":"Snowball Questions Q: For security purposes, data transfers must be completed whithin 90 days of the snowball being prepared. Intro Petabyte-sale data transfer service, note we have to use multiple ones to acheive that since the only available options are 50 TB and 80 TB Low cost 1/5th the cost of transfering 100TB over high speed internet which is suually 1000s of dollars Speed 100TB usually takes 100 days to transfer, snowball reduce that by less than a week E-Ink dispaly (lable for shipping information) Tamper and weather proof Data is encrypted end-to-end (265-bit encryption) uses Trusted Platform Module (TPM) which is a specialized ship that stores RSA keys specific to the host system for hardware authentication Can Import/Export from S3 it come with 50 TB (42 TB usable) and 80 TB (72 TB usable) objects For security purposes, data transfers must be completed whithin 90 days of the snowball being prepared. Snowball Edge More storage an on-site compute capabilites It has orange handles, maye that how we make visual disnction between the previous model LCD display (shipping information and other functionality) Can undertake local processing and edge-computing workloads Can be clustered in groups of 5 to 10 devices Three options for device configurations: Storage optimized (24 vCPUs) Compute optimized (54 vCPUs) GPU optimized (54 vCPUs) Two options: 100 TB (83 TB of usable space) 100 TB Clustered (45 TB per node) Snowmobile For exabytes-scale data 45-foot long ruggedized shipping container, pulled by a semi-trailer truck transfer up to 100PB per Snowmobile Security: GPS, Alarm, 24/7 video surveillance, an escort security vehicle while in transit (optional) AWS personnel wil help you with that","title":"Snowball"},{"location":"aws/Snowball/#snowball","text":"","title":"Snowball"},{"location":"aws/Snowball/#questions","text":"Q: For security purposes, data transfers must be completed whithin 90 days of the snowball being prepared.","title":"Questions"},{"location":"aws/Snowball/#intro","text":"Petabyte-sale data transfer service, note we have to use multiple ones to acheive that since the only available options are 50 TB and 80 TB Low cost 1/5th the cost of transfering 100TB over high speed internet which is suually 1000s of dollars Speed 100TB usually takes 100 days to transfer, snowball reduce that by less than a week E-Ink dispaly (lable for shipping information) Tamper and weather proof Data is encrypted end-to-end (265-bit encryption) uses Trusted Platform Module (TPM) which is a specialized ship that stores RSA keys specific to the host system for hardware authentication Can Import/Export from S3 it come with 50 TB (42 TB usable) and 80 TB (72 TB usable) objects For security purposes, data transfers must be completed whithin 90 days of the snowball being prepared.","title":"Intro"},{"location":"aws/Snowball/#snowball-edge","text":"More storage an on-site compute capabilites It has orange handles, maye that how we make visual disnction between the previous model LCD display (shipping information and other functionality) Can undertake local processing and edge-computing workloads Can be clustered in groups of 5 to 10 devices Three options for device configurations: Storage optimized (24 vCPUs) Compute optimized (54 vCPUs) GPU optimized (54 vCPUs) Two options: 100 TB (83 TB of usable space) 100 TB Clustered (45 TB per node)","title":"Snowball Edge"},{"location":"aws/Snowball/#snowmobile","text":"For exabytes-scale data 45-foot long ruggedized shipping container, pulled by a semi-trailer truck transfer up to 100PB per Snowmobile Security: GPS, Alarm, 24/7 video surveillance, an escort security vehicle while in transit (optional) AWS personnel wil help you with that","title":"Snowmobile"},{"location":"aws/Summary/","text":"Summary S3 Snowball VPC EC2 - ELB EFS - EBS IAM DNS - Route53 Lambda RDS and others SQS-SNS-ElastiCache Kennisis High Availability - Scaling Cognito CloudX --> Formation-Watch-Logs CloudFront BeanStack APIGateway","title":"Summary"},{"location":"aws/Summary/#summary","text":"S3 Snowball VPC EC2 - ELB EFS - EBS IAM DNS - Route53 Lambda RDS and others SQS-SNS-ElastiCache Kennisis High Availability - Scaling Cognito CloudX --> Formation-Watch-Logs CloudFront BeanStack APIGateway","title":"Summary"},{"location":"aws/VPC/","text":"Questions Q: What is Network Address Translation, it's used both in NAT gateways for private insntaces and it's one of the features of Internet Gateways for public instances ? A: Q: Does star configuration: 1 Central - 4 other VPCs a must ? or just an example ? A: Q: We have to create an entry in the route table with 0.0.0.0/0, but isn't that provided already ? A: If I was talking about Interget Gateways, Yes, we have to route table entries are not created automatically. Q: What does this means: \"Security groups are STATEFUL i.e if traffic is allowed inbound it is aloso allowed outbound\" ? Number of security groups per instance is determined by how many ENI it's connected to (not sure about it ?) What is an Availability Zone ? Physically ? What is the meaning of \"IPv4 CIDR\" ? What does the values mean like \"172.31.48.0/20\" ? Can we write a Security Group rule affecting outbound network What is the \"Tenancy\" that creates a dedicated host for us and expensive ? Q: What the heck is an AWS Transit Gateway ? Core components Provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. Think of it as your own personal data center. Gives you complete control over your virtual networking environment It has a lot of components and services like Internet Gateways, Routing tables, Public/Private subnets ... ![[vpc.png]] Key features It has 4 fields: Name, IPv4 CIDR block, IPv6 block, Tenacy Region specifc, every region come with one, you can create up to 5 VPC per region 200 subnets Cost nothing: VPCs, route tables, tabls, Nacls, internet, gateways, securtiy groups and subnets, VPC Peering Cost money: NAT Gateway, VPC Endpoint, VPN gateway, Customer Gateway DNS hostnames to allow accesss to EC2 machines via sub domains not enabled by default Default VPC It allows us to be able to immediately deploy instances, it creates: - size of /16 IPv4 CIDR block (172.31.0.0/16) - a size /20 default subnet in each availability zone - an internet gateway and connect it to your default VPC for machine to be able to access internet - Default security group and associate it with your default VPC - default network access contorl list (NACL) - default DHCP options of your account - When you create a VPC, it has automatically a main route table Default everywhere IP 0.0.0.0/0 it represents all possible IP addresses In our route table for IGW we are allowing internet access In our security groups inbound rules, we are allowing all trafic from the internet access our public resources VPC peering Allows to connect multiple VPCs over a direct network using private IP addresses - Intances on peerred VPCs behaves just like they are on the same network - Cross account and regions - Star configuration: 1 Central - 4 other VPCs - No transitive peering, perring must take place directly between VPCs - No overlapping CIDR Blocks (Network addresses) Route tables Determines where network trafic is directed - Each subnet in your VPC must be associated with a route table - A subnet can only be associated with one route table at a time but we can associate multiple subnets with the same route table Internet gateways (IGW) Allows access to the internet - provides a target in route table for internet-routable traffic - perform network address translation (NAT) for intances taht have been assigned public IPv4 addresses Basition / Jumpbox EC2 instances which are security harden, they help you gain access to your private EC2 instances via SSH or RCP Nat Gateways / Instaces are only intended for EC2 instances to gain outbound access to the inernet for things like security updates Nats cannot/should not be used as Bastions there is a service called system manager's session manager and it replaces the need for bastions so that you don't have to Direct Connect Establishes dedicated network connection from on-premises locations to AWS with a very fast network 50M-500M for lower bandwidth and 1GB-10GB for higher bandwidth - Helps reduce network costs and increase bandwidth throughput (great for high trafic networks) - Provides more consitent network experience that a typical internet-based connection (reliable and secure) VPC Endpoits Helps keep trafic between AWS services within the AWS network 2 kinds: Interface endpoints and Gateway endpoints Interface endpoint costs arround 7.5$/month. Gateway are free Interface endpoints uses an Elastic Network Interface (ENI) with Private (IP) (powered by AWS PrivateLink) Interface endpoints is a target for a specific route in your route table Interface endpoints support many AWS service Gateway endpoints only support DynamoDB and S3 VPC Flow logs Capture IP traffic information in-and-out of network interfaces whithin VPC Can be turned on at the VPC, Subnet or Network interface level Cannot be tagged like other resources Cannot be edited the configuration after created Cannot enable flow logs for VPCs which are peered with your VPC unless it is in the same account Can be exported to s3 or CloudWatch Logs VPC logs contains the source and destination IP addresses (not hostnames) Some instance trafic is not monitored: contacting AWS DNS servers Windows licence activation Traffic to and from the instance metadata address (169.254.169.254) DHCP traffic Any traffic to the reserved IP addresss of the default VPC router Question could be asked during the exam: Does VPC flow logs contain IP addresses or domain(hostname), IP is the answer NACLs NACL acts as a virtual firewall at the subnet level - VPCs automatically give a default NACL - each NACL contain a set of rules that can allow or deny traffic into and out of subnets - Subnets are associated with NACLs. Subnets can only belong to a single NACL. When associating with a new NACL the subnet will be removed from the previous one. - NACL will deny all traffic by default - Rule # determines the order of evaluation. From lowest to highest, the highest can be 32766 and its recommended to work in 10 or 100 increments to be able to create rules in between when needed - You can allow or deny traffic. You could block a single IP address which we cannot do with Security Groups Usecases: - Block a single IP address - Deny SSH (port 22) Security Groups A virtual firewall that controls the traffic to and from EC2 instances (instace level) - All inbound traffic is blocked by default. All outbound traffic is allowed by default. - A rule has a particular protocol and port access level - There are no deny rules. - Source could be IP range or specific IP addresse or another security group (but what does that mean ?) - Instance can belong to multiple security groups. Security groups can contain multiple EC2 instances - Security groups are STATEFUL i.e if traffic is allowed inbound it is aloso allowed outbound - Changes to security groups take effect immediately - Limits: - Up to 10k securtiy groups in a region (default is 2500), make a service limit increase request. - 60 inbound rules and 60 outbound rules per security group - 16 security groups per Elastic Network Interface (default is 5) - Number of security groups per instance is determined by how many ENI it's connected to (not sure about it ?) One-to-one relationship between VPC and IGW We can create a custom route table other than the default created with the IGW and set it to the main one for subsequent IGWs. One subnet for each AZ ?? What does that mean ? We can create multiple subnets in the same AZ. Maybe it means there should at least one subnet in each availability zone ? Subnet have a property for \"Auto assinging public IPv4 address to EC2 instances\" which we call \"Public subnets\" We have to create at least 3 subnets for availabilty. Network Address Translation (NAT) Remap on e address space to another Connect private network to the internet needs a NAT Gateway to remap the private IPs If we have two networks with conflicing IP ranges we can use NAT to make the addresses more aggreable NAT instances are EC2 instances that do the remaping, there are a lot of community AMIs to use. NAT gateways is a managed service that launches redundant instances whithin the select AZ They have to be in a public subnet when creating a NAT instances you must disable source and destination checks we must have a route out of the private subnet to the NAT instance NAT gateways starts at 5GBps and scale up to 45GBps Getting hands dirty Public subnets are those who are configured to automatically assing public IPv4 addresses to EC2 machines Bastion (Gua) provides Two factor authentication, session recording ... there is also AWS's Session manager which provide most of the features except session recording To allow an instance running in a private subnet to access the internet, we create a NAT gateway in a public subnet and add it to the route table of the private subnet We cannot change the subnet for an ec2 instance We have to create a CloudWatch log group to be able to collect VPC Flow logs there. We can't delete an Attached Internet Gateway, we havet to first detach it from the VPC. Egress-only Internet gateways allow outbound only access to private instances via IPv6, for IPv4 use NAT gateways. NAT Instances are EC2 instances tha provide Network Addresse Translation, NAT Gateways are hightly recommended because they are managed by AWS and they are high available in each AZ A VPC automatically comes with a default network ACL which allows all inbound/outbound traffic. A custom NACL denies all traffic, both inbound and outbound by default.","title":"VPC"},{"location":"aws/VPC/#questions","text":"Q: What is Network Address Translation, it's used both in NAT gateways for private insntaces and it's one of the features of Internet Gateways for public instances ? A: Q: Does star configuration: 1 Central - 4 other VPCs a must ? or just an example ? A: Q: We have to create an entry in the route table with 0.0.0.0/0, but isn't that provided already ? A: If I was talking about Interget Gateways, Yes, we have to route table entries are not created automatically. Q: What does this means: \"Security groups are STATEFUL i.e if traffic is allowed inbound it is aloso allowed outbound\" ? Number of security groups per instance is determined by how many ENI it's connected to (not sure about it ?) What is an Availability Zone ? Physically ? What is the meaning of \"IPv4 CIDR\" ? What does the values mean like \"172.31.48.0/20\" ? Can we write a Security Group rule affecting outbound network What is the \"Tenancy\" that creates a dedicated host for us and expensive ? Q: What the heck is an AWS Transit Gateway ?","title":"Questions"},{"location":"aws/VPC/#core-components","text":"Provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. Think of it as your own personal data center. Gives you complete control over your virtual networking environment It has a lot of components and services like Internet Gateways, Routing tables, Public/Private subnets ... ![[vpc.png]]","title":"Core components"},{"location":"aws/VPC/#key-features","text":"It has 4 fields: Name, IPv4 CIDR block, IPv6 block, Tenacy Region specifc, every region come with one, you can create up to 5 VPC per region 200 subnets Cost nothing: VPCs, route tables, tabls, Nacls, internet, gateways, securtiy groups and subnets, VPC Peering Cost money: NAT Gateway, VPC Endpoint, VPN gateway, Customer Gateway DNS hostnames to allow accesss to EC2 machines via sub domains not enabled by default","title":"Key features"},{"location":"aws/VPC/#default-vpc","text":"It allows us to be able to immediately deploy instances, it creates: - size of /16 IPv4 CIDR block (172.31.0.0/16) - a size /20 default subnet in each availability zone - an internet gateway and connect it to your default VPC for machine to be able to access internet - Default security group and associate it with your default VPC - default network access contorl list (NACL) - default DHCP options of your account - When you create a VPC, it has automatically a main route table","title":"Default VPC"},{"location":"aws/VPC/#default-everywhere-ip-00000","text":"it represents all possible IP addresses In our route table for IGW we are allowing internet access In our security groups inbound rules, we are allowing all trafic from the internet access our public resources","title":"Default everywhere IP 0.0.0.0/0"},{"location":"aws/VPC/#vpc-peering","text":"Allows to connect multiple VPCs over a direct network using private IP addresses - Intances on peerred VPCs behaves just like they are on the same network - Cross account and regions - Star configuration: 1 Central - 4 other VPCs - No transitive peering, perring must take place directly between VPCs - No overlapping CIDR Blocks (Network addresses)","title":"VPC peering"},{"location":"aws/VPC/#route-tables","text":"Determines where network trafic is directed - Each subnet in your VPC must be associated with a route table - A subnet can only be associated with one route table at a time but we can associate multiple subnets with the same route table","title":"Route tables"},{"location":"aws/VPC/#internet-gateways-igw","text":"Allows access to the internet - provides a target in route table for internet-routable traffic - perform network address translation (NAT) for intances taht have been assigned public IPv4 addresses","title":"Internet gateways (IGW)"},{"location":"aws/VPC/#basition-jumpbox","text":"EC2 instances which are security harden, they help you gain access to your private EC2 instances via SSH or RCP Nat Gateways / Instaces are only intended for EC2 instances to gain outbound access to the inernet for things like security updates Nats cannot/should not be used as Bastions there is a service called system manager's session manager and it replaces the need for bastions so that you don't have to","title":"Basition / Jumpbox"},{"location":"aws/VPC/#direct-connect","text":"Establishes dedicated network connection from on-premises locations to AWS with a very fast network 50M-500M for lower bandwidth and 1GB-10GB for higher bandwidth - Helps reduce network costs and increase bandwidth throughput (great for high trafic networks) - Provides more consitent network experience that a typical internet-based connection (reliable and secure)","title":"Direct Connect"},{"location":"aws/VPC/#vpc-endpoits","text":"Helps keep trafic between AWS services within the AWS network 2 kinds: Interface endpoints and Gateway endpoints Interface endpoint costs arround 7.5$/month. Gateway are free Interface endpoints uses an Elastic Network Interface (ENI) with Private (IP) (powered by AWS PrivateLink) Interface endpoints is a target for a specific route in your route table Interface endpoints support many AWS service Gateway endpoints only support DynamoDB and S3","title":"VPC Endpoits"},{"location":"aws/VPC/#vpc-flow-logs","text":"Capture IP traffic information in-and-out of network interfaces whithin VPC Can be turned on at the VPC, Subnet or Network interface level Cannot be tagged like other resources Cannot be edited the configuration after created Cannot enable flow logs for VPCs which are peered with your VPC unless it is in the same account Can be exported to s3 or CloudWatch Logs VPC logs contains the source and destination IP addresses (not hostnames) Some instance trafic is not monitored: contacting AWS DNS servers Windows licence activation Traffic to and from the instance metadata address (169.254.169.254) DHCP traffic Any traffic to the reserved IP addresss of the default VPC router Question could be asked during the exam: Does VPC flow logs contain IP addresses or domain(hostname), IP is the answer","title":"VPC Flow logs"},{"location":"aws/VPC/#nacls","text":"NACL acts as a virtual firewall at the subnet level - VPCs automatically give a default NACL - each NACL contain a set of rules that can allow or deny traffic into and out of subnets - Subnets are associated with NACLs. Subnets can only belong to a single NACL. When associating with a new NACL the subnet will be removed from the previous one. - NACL will deny all traffic by default - Rule # determines the order of evaluation. From lowest to highest, the highest can be 32766 and its recommended to work in 10 or 100 increments to be able to create rules in between when needed - You can allow or deny traffic. You could block a single IP address which we cannot do with Security Groups Usecases: - Block a single IP address - Deny SSH (port 22)","title":"NACLs"},{"location":"aws/VPC/#security-groups","text":"A virtual firewall that controls the traffic to and from EC2 instances (instace level) - All inbound traffic is blocked by default. All outbound traffic is allowed by default. - A rule has a particular protocol and port access level - There are no deny rules. - Source could be IP range or specific IP addresse or another security group (but what does that mean ?) - Instance can belong to multiple security groups. Security groups can contain multiple EC2 instances - Security groups are STATEFUL i.e if traffic is allowed inbound it is aloso allowed outbound - Changes to security groups take effect immediately - Limits: - Up to 10k securtiy groups in a region (default is 2500), make a service limit increase request. - 60 inbound rules and 60 outbound rules per security group - 16 security groups per Elastic Network Interface (default is 5) - Number of security groups per instance is determined by how many ENI it's connected to (not sure about it ?) One-to-one relationship between VPC and IGW We can create a custom route table other than the default created with the IGW and set it to the main one for subsequent IGWs. One subnet for each AZ ?? What does that mean ? We can create multiple subnets in the same AZ. Maybe it means there should at least one subnet in each availability zone ? Subnet have a property for \"Auto assinging public IPv4 address to EC2 instances\" which we call \"Public subnets\" We have to create at least 3 subnets for availabilty.","title":"Security Groups"},{"location":"aws/VPC/#network-address-translation-nat","text":"Remap on e address space to another Connect private network to the internet needs a NAT Gateway to remap the private IPs If we have two networks with conflicing IP ranges we can use NAT to make the addresses more aggreable NAT instances are EC2 instances that do the remaping, there are a lot of community AMIs to use. NAT gateways is a managed service that launches redundant instances whithin the select AZ They have to be in a public subnet when creating a NAT instances you must disable source and destination checks we must have a route out of the private subnet to the NAT instance NAT gateways starts at 5GBps and scale up to 45GBps","title":"Network Address Translation (NAT)"},{"location":"aws/VPC/#getting-hands-dirty","text":"Public subnets are those who are configured to automatically assing public IPv4 addresses to EC2 machines Bastion (Gua) provides Two factor authentication, session recording ... there is also AWS's Session manager which provide most of the features except session recording To allow an instance running in a private subnet to access the internet, we create a NAT gateway in a public subnet and add it to the route table of the private subnet We cannot change the subnet for an ec2 instance We have to create a CloudWatch log group to be able to collect VPC Flow logs there. We can't delete an Attached Internet Gateway, we havet to first detach it from the VPC. Egress-only Internet gateways allow outbound only access to private instances via IPv6, for IPv4 use NAT gateways. NAT Instances are EC2 instances tha provide Network Addresse Translation, NAT Gateways are hightly recommended because they are managed by AWS and they are high available in each AZ A VPC automatically comes with a default network ACL which allows all inbound/outbound traffic. A custom NACL denies all traffic, both inbound and outbound by default.","title":"Getting hands dirty"},{"location":"configurations/Mac/","text":"Enable hold to repeat key defaults write -g ApplePressAndHoldEnabled -bool false Disable notifications for specific folders in Thunderbird Tried out Mailbox Alert but wasn't what I was looking for, it looks like it has a separate alert system and don't integrate with system notifications I opted for FiltaQuilla , It has a Filter action named \"Do not notify\" which seems to do exactly what I want, disable a notification for a specific group of emails that I already had a filter for them to move to a separate folder, so I'll just add an action to the filters I already have.","title":"Mac"},{"location":"configurations/Mac/#enable-hold-to-repeat-key","text":"defaults write -g ApplePressAndHoldEnabled -bool false","title":"Enable hold to repeat key"},{"location":"configurations/Mac/#disable-notifications-for-specific-folders-in-thunderbird","text":"Tried out Mailbox Alert but wasn't what I was looking for, it looks like it has a separate alert system and don't integrate with system notifications I opted for FiltaQuilla , It has a Filter action named \"Do not notify\" which seems to do exactly what I want, disable a notification for a specific group of emails that I already had a filter for them to move to a separate folder, so I'll just add an action to the filters I already have.","title":"Disable notifications for specific folders in Thunderbird"},{"location":"configurations/SUMMARY/","text":"Summary Logback Log4j 2 Domejudge Archiva Docker Docker Swarm Google Cloud","title":"Summary"},{"location":"configurations/SUMMARY/#summary","text":"Logback Log4j 2 Domejudge Archiva Docker Docker Swarm Google Cloud","title":"Summary"},{"location":"configurations/Spark/","text":"Basics Don't confuse Local and Standalone modes. Local mode is a dev mode that runs everything in the same JVM, it's the default mode and configured with master local[*] or local[n] Standalone mode is Spark's own cluster manager just like (not that much) YARN and Mesos Standalone Start the master node ./start-master.sh Start the worker node (could be in the same machine) ./start-worker.sh spark://mac.local:7077 In your application set the master to spark://mac.local:7077 We can adjust how much executors per worker and how much cores an executor allocates with spark.executor.cores and spark.cores.max spark.executor.instances has no effect in the standalone mode Writing to the filesystem asks all the workers to write their partitions to the specified path, so we have to make sure that path exists for all workers S3 Set spark.jars.packages to com.amazonaws:aws-java-sdk:1.12.310,org.apache.hadoop:hadoop-aws:3.3.4 If the S3 bucket is public set fs.s3a.aws.credentials.provider to org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider Read the S3 object (not sure about the bucket) using its URL prefixed by s3a:// . Example s3a://cellpainting-gallery/datasets/dataset.csv Kubernetes brew install openjdk@11 brew install apache-spark brew install ubuntu/ microk8s enable registry Since microk8s is running in a VM, we need to use the VM IP address to talk to the registry from the host machine (Mac OS), but we need to use the registry service cluster IP (or DNS name ?) when submitting Spark applications # Get the VM IP kubectl get nodes -o wide # add \"insecure-registries\": [\"http://192.168.64.2:32000\"] to ~/.docker/daemon.json # or via the Docker app, then restart cd /opt/homebrew/Cellar/apache-spark/3.2.1/libexec/ docker-image-tool.sh -r 192 .168.64.2:32000 build docker-image-tool.sh -r 192 .168.64.2:32000 -p ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile build docker-image-tool.sh -r 192 .168.64.2:32000 push","title":"Basics"},{"location":"configurations/Spark/#basics","text":"Don't confuse Local and Standalone modes. Local mode is a dev mode that runs everything in the same JVM, it's the default mode and configured with master local[*] or local[n] Standalone mode is Spark's own cluster manager just like (not that much) YARN and Mesos","title":"Basics"},{"location":"configurations/Spark/#standalone","text":"Start the master node ./start-master.sh Start the worker node (could be in the same machine) ./start-worker.sh spark://mac.local:7077 In your application set the master to spark://mac.local:7077 We can adjust how much executors per worker and how much cores an executor allocates with spark.executor.cores and spark.cores.max spark.executor.instances has no effect in the standalone mode Writing to the filesystem asks all the workers to write their partitions to the specified path, so we have to make sure that path exists for all workers","title":"Standalone"},{"location":"configurations/Spark/#s3","text":"Set spark.jars.packages to com.amazonaws:aws-java-sdk:1.12.310,org.apache.hadoop:hadoop-aws:3.3.4 If the S3 bucket is public set fs.s3a.aws.credentials.provider to org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider Read the S3 object (not sure about the bucket) using its URL prefixed by s3a:// . Example s3a://cellpainting-gallery/datasets/dataset.csv","title":"S3"},{"location":"configurations/Spark/#kubernetes","text":"brew install openjdk@11 brew install apache-spark brew install ubuntu/ microk8s enable registry Since microk8s is running in a VM, we need to use the VM IP address to talk to the registry from the host machine (Mac OS), but we need to use the registry service cluster IP (or DNS name ?) when submitting Spark applications # Get the VM IP kubectl get nodes -o wide # add \"insecure-registries\": [\"http://192.168.64.2:32000\"] to ~/.docker/daemon.json # or via the Docker app, then restart cd /opt/homebrew/Cellar/apache-spark/3.2.1/libexec/ docker-image-tool.sh -r 192 .168.64.2:32000 build docker-image-tool.sh -r 192 .168.64.2:32000 -p ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile build docker-image-tool.sh -r 192 .168.64.2:32000 push","title":"Kubernetes"},{"location":"configurations/alembic/","text":"Alembic init: it will create a directory named \"alembic\" contains all the needed configuration/code files for alembic alembic init alembic configure SQLAlchemy URL Change it in alembic.ini Or Change the way it's read in alembic/env.py if you want to read it from environment variables or any other custom business logic you need.","title":"Alembic"},{"location":"configurations/alembic/#alembic","text":"init: it will create a directory named \"alembic\" contains all the needed configuration/code files for alembic alembic init alembic configure SQLAlchemy URL Change it in alembic.ini Or Change the way it's read in alembic/env.py if you want to read it from environment variables or any other custom business logic you need.","title":"Alembic"},{"location":"configurations/ambari/","text":"Apache Ambari ambari-agent should be already installed on the hosts IF NOT installed then Command start time 2019 - 09 - 02 03 : 02 : 36 env : \u2018 / var / lib / ambari - agent / tmp / os_check_type1567389754 . py \u2019 : No such file or directory Connection to 192.168 . 33.11 closed . SSH command execution finished host = 192.168 . 33.11 , exitcode = 127 Command end time 2019 - 09 - 02 03 : 02 : 37 ERROR : Bootstrap of host 192.168 . 33.11 fails because previous action finished with non - zero exit code ( 127 ) ERROR MESSAGE : Connection to 192.168 . 33.11 closed . STDOUT : env : \u2018 / var / lib / ambari - agent / tmp / os_check_type1567389754 . py \u2019 : No such file or directory Connection to 192.168 . 33.11 closed . SSh key should not have passphrase if not then: Permission denied (public key) Install agent/server sudo wget -O /etc/apt/sources.list.d/ambari.list http://public-repo-1.hortonworks.com/ambari/ubuntu16/2.x/updates/2.7.3.0/ambari.list sudo apt-key adv --recv-keys --keyserver keyserver.ubuntu.com B9733A7A07513CAD\\n sudo apt update sudo apt install ambari-agent sudo apt install ambari-server Vagrant confiugration: multiple machines and copy ssh key This will allow access to the machine as the root user config . vm . provider \"virtualbox\" do | vb | # Display the VirtualBox GUI when booting the machine vb . gui = false # Customize the amount of memory on the VM: vb . memory = \"1024\" end config . vm . define \"namenode\" do | namenode | config . vm . network \"private_network\" , ip : \"192.168.33.10\" end config . vm . define \"datanode1\" do | datanode1 | config . vm . network \"private_network\" , ip : \"192.168.33.11\" end config . vm . define \"datanode2\" do | datanode2 | config . vm . network \"private_network\" , ip : \"192.168.33.12\" end ssh_pub_key = File . readlines ( \" #{ Dir . home } /.ssh/id_rsa_ambari.pub\" ) . first . strip config . vm . provision 'shell' , inline : 'mkdir -p /root/.ssh' config . vm . provision 'shell' , inline : \"echo #{ ssh_pub_key } >> /root/.ssh/authorized_keys\" config . vm . provision 'shell' , inline : \"echo #{ ssh_pub_key } >> /home/vagrant/.ssh/authorized_keys\" , privileged : false","title":"Apache Ambari"},{"location":"configurations/ambari/#apache-ambari","text":"ambari-agent should be already installed on the hosts IF NOT installed then Command start time 2019 - 09 - 02 03 : 02 : 36 env : \u2018 / var / lib / ambari - agent / tmp / os_check_type1567389754 . py \u2019 : No such file or directory Connection to 192.168 . 33.11 closed . SSH command execution finished host = 192.168 . 33.11 , exitcode = 127 Command end time 2019 - 09 - 02 03 : 02 : 37 ERROR : Bootstrap of host 192.168 . 33.11 fails because previous action finished with non - zero exit code ( 127 ) ERROR MESSAGE : Connection to 192.168 . 33.11 closed . STDOUT : env : \u2018 / var / lib / ambari - agent / tmp / os_check_type1567389754 . py \u2019 : No such file or directory Connection to 192.168 . 33.11 closed . SSh key should not have passphrase if not then: Permission denied (public key)","title":"Apache Ambari"},{"location":"configurations/ambari/#install-agentserver","text":"sudo wget -O /etc/apt/sources.list.d/ambari.list http://public-repo-1.hortonworks.com/ambari/ubuntu16/2.x/updates/2.7.3.0/ambari.list sudo apt-key adv --recv-keys --keyserver keyserver.ubuntu.com B9733A7A07513CAD\\n sudo apt update sudo apt install ambari-agent sudo apt install ambari-server","title":"Install agent/server"},{"location":"configurations/ambari/#vagrant-confiugration-multiple-machines-and-copy-ssh-key","text":"This will allow access to the machine as the root user config . vm . provider \"virtualbox\" do | vb | # Display the VirtualBox GUI when booting the machine vb . gui = false # Customize the amount of memory on the VM: vb . memory = \"1024\" end config . vm . define \"namenode\" do | namenode | config . vm . network \"private_network\" , ip : \"192.168.33.10\" end config . vm . define \"datanode1\" do | datanode1 | config . vm . network \"private_network\" , ip : \"192.168.33.11\" end config . vm . define \"datanode2\" do | datanode2 | config . vm . network \"private_network\" , ip : \"192.168.33.12\" end ssh_pub_key = File . readlines ( \" #{ Dir . home } /.ssh/id_rsa_ambari.pub\" ) . first . strip config . vm . provision 'shell' , inline : 'mkdir -p /root/.ssh' config . vm . provision 'shell' , inline : \"echo #{ ssh_pub_key } >> /root/.ssh/authorized_keys\" config . vm . provision 'shell' , inline : \"echo #{ ssh_pub_key } >> /home/vagrant/.ssh/authorized_keys\" , privileged : false","title":"Vagrant confiugration: multiple machines and copy ssh key"},{"location":"configurations/ansible/","text":"Ansible Getting started video Getting Started with Ansible | Ansible.com Architecture Modules Modules control system resources, packages, files or nearly anything else. Example apt module : directive1 = value1 directive2 = value2 Variables Variables make it easy to configure and alternate some configurations. Example: a pakcage version, username .. etc . They could be used in playbooks, files, inventories and command line. Inventories List of target machines which could be: static servers, ranges, dynamic servers on AWS, GCP ...etc [webservers] www1.example.com www2.example.com [dbservers] db0.example.com db1.example.com Playbooks Plain text YAML files that describe the desired state of something. Playbooks are Ansible\u2019s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process. If Ansible modules are the tools in your workshop, playbooks are your instruction manuals, and your inventory of hosts are your raw material. Playbooks contains plays Plays contain tasks Tasks contain modules Tasks runs sequentially (mimicking you typing command lines) Handlers are triggered by tasks, and run once at the end of plays. --- - hosts : webservers remote_user : root tasks : - name : ensure apache is at the latest version yum : name : httpd state : latest - name : write the apache config file template : src : /srv/httpd.j2 dest : /etc/httpd.conf - hosts : databases remote_user : root tasks : - name : ensure postgresql is at the latest version yum : name : postgresql state : latest - name : ensure that postgresql is started service : name : postgresql state : started Roles Roles are special kind of Playbooks that are fully self-contained with tasks, variables, configuration templates and other supporting files.","title":"Ansible"},{"location":"configurations/ansible/#ansible","text":"","title":"Ansible"},{"location":"configurations/ansible/#getting-started-video","text":"Getting Started with Ansible | Ansible.com","title":"Getting started video"},{"location":"configurations/ansible/#architecture","text":"","title":"Architecture"},{"location":"configurations/ansible/#modules","text":"Modules control system resources, packages, files or nearly anything else. Example apt module : directive1 = value1 directive2 = value2","title":"Modules"},{"location":"configurations/ansible/#variables","text":"Variables make it easy to configure and alternate some configurations. Example: a pakcage version, username .. etc . They could be used in playbooks, files, inventories and command line.","title":"Variables"},{"location":"configurations/ansible/#inventories","text":"List of target machines which could be: static servers, ranges, dynamic servers on AWS, GCP ...etc [webservers] www1.example.com www2.example.com [dbservers] db0.example.com db1.example.com","title":"Inventories"},{"location":"configurations/ansible/#playbooks","text":"Plain text YAML files that describe the desired state of something. Playbooks are Ansible\u2019s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process. If Ansible modules are the tools in your workshop, playbooks are your instruction manuals, and your inventory of hosts are your raw material. Playbooks contains plays Plays contain tasks Tasks contain modules Tasks runs sequentially (mimicking you typing command lines) Handlers are triggered by tasks, and run once at the end of plays. --- - hosts : webservers remote_user : root tasks : - name : ensure apache is at the latest version yum : name : httpd state : latest - name : write the apache config file template : src : /srv/httpd.j2 dest : /etc/httpd.conf - hosts : databases remote_user : root tasks : - name : ensure postgresql is at the latest version yum : name : postgresql state : latest - name : ensure that postgresql is started service : name : postgresql state : started","title":"Playbooks"},{"location":"configurations/ansible/#roles","text":"Roles are special kind of Playbooks that are fully self-contained with tasks, variables, configuration templates and other supporting files.","title":"Roles"},{"location":"configurations/archiva/","text":"Archiva Create a user in Archiva to use for deployment (or use guest if you wish to deploy without a username and password) The deployment user needs the Role 'Repository Manager' for each repository that you want to deploy to To configure sbt's publish add the folowing to build.sbt or the commonSettings for multi project build: publishTo := Some ( \"snapshots\" at \"http://localhost:8080/repository/snapshots/\" ), credentials += Credentials ( \"Repository Archiva Managed snapshots Repository\" , \"localhost\" , \"gitlab-ci\" , \"gitlab-ci1\" ) run it using docker docker run - v ~/ existing - archiva - base : / var / archiva - p 8080 : 8080 - d ninjaben / archiva - docker Users should be: confirmed, not blocked and not required to change password","title":"Archiva"},{"location":"configurations/archiva/#archiva","text":"Create a user in Archiva to use for deployment (or use guest if you wish to deploy without a username and password) The deployment user needs the Role 'Repository Manager' for each repository that you want to deploy to To configure sbt's publish add the folowing to build.sbt or the commonSettings for multi project build: publishTo := Some ( \"snapshots\" at \"http://localhost:8080/repository/snapshots/\" ), credentials += Credentials ( \"Repository Archiva Managed snapshots Repository\" , \"localhost\" , \"gitlab-ci\" , \"gitlab-ci1\" )","title":"Archiva"},{"location":"configurations/archiva/#run-it-using-docker","text":"docker run - v ~/ existing - archiva - base : / var / archiva - p 8080 : 8080 - d ninjaben / archiva - docker Users should be: confirmed, not blocked and not required to change password","title":"run it using docker"},{"location":"configurations/attach_gcp_disk/","text":"Adding persistence disk to existing VMs You should: Have an existing VMS An external disk created and attached to that VMs The commands bellow will format the disk, mount it and make it mounted automatically on boot. use the lsblk command to list the disks that are attached to your instance and find the disk that you want to format and mount. sudo lsblk Format the disk. You can use any file format that you need, but the most simple method is to format the entire disk with a single ext4 file system and no partition table sudo mkfs.ext4 -m 0 -F -E lazy_itable_init = 0 ,lazy_journal_init = 0 ,discard /dev/ [ DEVICE_ID ] Create a directory sudo mkdir -p /mnt/disks/ [ MNT_DIR ] Use the mount tool to mount the disk to the instance with the discard option enabled sudo mount -o discard,defaults /dev/ [ DEVICE_ID ] /mnt/disks/ [ MNT_DIR ] Configure read and write permissions on the device sudo chmod a+w /mnt/disks/ [ MNT_DIR ] Use the blkid command to find the UUID for the zonal persistent disk sudo blkid /dev/ [ DEVICE_ID ] Open the /etc/fstab file in a text editor and create an entry that includes the UUID. For example: UUID =[ UUID_VALUE ] /mnt/disks/ [ MNT_DIR ] ext4 discard,defaults,nofail 0 2 Note nofail tells the system to not fail if it couldn't mount the disk on boot.","title":"Adding persistence disk to existing VMs"},{"location":"configurations/attach_gcp_disk/#adding-persistence-disk-to-existing-vms","text":"You should: Have an existing VMS An external disk created and attached to that VMs The commands bellow will format the disk, mount it and make it mounted automatically on boot. use the lsblk command to list the disks that are attached to your instance and find the disk that you want to format and mount. sudo lsblk Format the disk. You can use any file format that you need, but the most simple method is to format the entire disk with a single ext4 file system and no partition table sudo mkfs.ext4 -m 0 -F -E lazy_itable_init = 0 ,lazy_journal_init = 0 ,discard /dev/ [ DEVICE_ID ] Create a directory sudo mkdir -p /mnt/disks/ [ MNT_DIR ] Use the mount tool to mount the disk to the instance with the discard option enabled sudo mount -o discard,defaults /dev/ [ DEVICE_ID ] /mnt/disks/ [ MNT_DIR ] Configure read and write permissions on the device sudo chmod a+w /mnt/disks/ [ MNT_DIR ] Use the blkid command to find the UUID for the zonal persistent disk sudo blkid /dev/ [ DEVICE_ID ] Open the /etc/fstab file in a text editor and create an entry that includes the UUID. For example: UUID =[ UUID_VALUE ] /mnt/disks/ [ MNT_DIR ] ext4 discard,defaults,nofail 0 2 Note nofail tells the system to not fail if it couldn't mount the disk on boot.","title":"Adding persistence disk to existing VMs"},{"location":"configurations/docker/","text":"Docker Install docker curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh To run docker commands without sudo sudo usermod -aG docker $LOGNAME or newgrp docker Note You should logout and log back in in order for this to take effect, otherwise run commands with sudo Access gcr (google cloud registery, where our docker images live :p) If you are inside a gcp vm you don't have to log in to your google acount an application credentials is provided and will be utilized by default gcloud auth configure-docker Install gcloud for Ubuntu export CLOUD_SDK_REPO = \"cloud-sdk- $( lsb_release -c -s ) \" echo \"deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo apt-get update && sudo apt-get install google-cloud-sdk Initialize configurations like login account and default project gcloud init Portainer Portainer is a dashboard for docker docker run -d -p 9000 :9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer Start containers automatically Use: --restart-policy : no : Do not automatically restart the container. (the default) on-failure : Restart the container if it exits due to an error, which manifests as a non-zero exit code. always : Always restart the container if it stops. If it is manually stopped, it is restarted only when Docker daemon restarts or the container itself is manually restarted. (See the second bullet listed in restart policy details) unless-stopped : Similar to always, except that when the container is stopped (manually or otherwise), it is not restarted even after Docker daemon restarts. Connect containers to both default and bridge network I couldn't find any proper way to do that but there a workarround I found here connect-bridge : image : docker:stable volumes : - /var/run/docker.sock:/var/run/docker.sock:ro command : /bin/sh -c \"docker ps -f label=networks=bridge -q | xargs -I'{}' docker network connect bridge {}\"","title":"Docker"},{"location":"configurations/docker/#docker","text":"","title":"Docker"},{"location":"configurations/docker/#install-docker","text":"curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh To run docker commands without sudo sudo usermod -aG docker $LOGNAME or newgrp docker Note You should logout and log back in in order for this to take effect, otherwise run commands with sudo","title":"Install docker"},{"location":"configurations/docker/#access-gcr-google-cloud-registery-where-our-docker-images-live-p","text":"If you are inside a gcp vm you don't have to log in to your google acount an application credentials is provided and will be utilized by default gcloud auth configure-docker","title":"Access gcr (google cloud registery, where our docker images live :p)"},{"location":"configurations/docker/#install-gcloud-for-ubuntu","text":"export CLOUD_SDK_REPO = \"cloud-sdk- $( lsb_release -c -s ) \" echo \"deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo apt-get update && sudo apt-get install google-cloud-sdk","title":"Install gcloud for Ubuntu"},{"location":"configurations/docker/#initialize-configurations","text":"like login account and default project gcloud init","title":"Initialize configurations"},{"location":"configurations/docker/#portainer","text":"Portainer is a dashboard for docker docker run -d -p 9000 :9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer","title":"Portainer"},{"location":"configurations/docker/#start-containers-automatically","text":"Use: --restart-policy : no : Do not automatically restart the container. (the default) on-failure : Restart the container if it exits due to an error, which manifests as a non-zero exit code. always : Always restart the container if it stops. If it is manually stopped, it is restarted only when Docker daemon restarts or the container itself is manually restarted. (See the second bullet listed in restart policy details) unless-stopped : Similar to always, except that when the container is stopped (manually or otherwise), it is not restarted even after Docker daemon restarts.","title":"Start containers automatically"},{"location":"configurations/docker/#connect-containers-to-both-default-and-bridge-network","text":"I couldn't find any proper way to do that but there a workarround I found here connect-bridge : image : docker:stable volumes : - /var/run/docker.sock:/var/run/docker.sock:ro command : /bin/sh -c \"docker ps -f label=networks=bridge -q | xargs -I'{}' docker network connect bridge {}\"","title":"Connect containers to both default and bridge network"},{"location":"configurations/domejudge/","text":"DomJudge Starting mariabdb container ... docker run -d --name dj-mariadb -v data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD = hED522QC@pwDR $n + -e MYSQL_USER = domjudge -e MYSQL_PASSWORD = _D-F%X2KfKNSqCQB -e MYSQL_DATABASE = domjudge -p 13306 :3306 mariadb --max-connections = 1000 Starting domserver container .... docker run -d -v /sys/fs/cgroup:/sys/fs/cgroup:ro --link dj-mariadb:mariadb -e MYSQL_HOST = mariadb -e MYSQL_USER = domjudge -e MYSQL_DATABASE = domjudge -e MYSQL_PASSWORD = _D-F%X2KfKNSqCQB -e MYSQL_ROOT_PASSWORD = hED522QC@pwDR $n + -p 80 :80 --name domserver domjudge/domserver:latest Required for the judgehosts to work Make sure you change the password of the judgehost account in the webinterface to something and write down the value into the JUDGEDAEMON_PASSWORD environment variable, otherwise the judgehosts will fail to start. Starting judgehost-0 container ... docker run -d --privileged -v /sys/fs/cgroup:/sys/fs/cgroup:ro --link domserver:domserver -e DOMSERVER_BASEURL = http://domserver/ -e JUDGEDAEMON_PASSWORD = Bng2TV2Nv%mPdfW! --name judgehost-0 --link domserver:domserver --hostname judgedaemon-0 -e DAEMON_ID = 0 domjudge/judgehost:latest Note Even if the domejudge is down the web interface can't detect that and the status Active: yes is still there with a green flag ( Status:OK ), after some time it is replaced with the yellow flag ( Status:Warning ) then the red one ( Status:Critical) but the status Active:yes is still there. if the judghost is booted up again the green flag ( Status:OK ) will showed up agin If we run another judgehost (with another DAEMON_ID ), it will show up in the judgehosts web interface JUDGEDAEMON_USERNAME (defaults to judgehost ) environment variable should be the password for the JUDGEDAEMON_PASSWORD (defaults to password ) user. This should be ready before starting the domjudge If the db container took down the web interface will go out of service When the db container is rebooted up :Boom: the web interface is reactive again and the data is also persitested.","title":"DomJudge"},{"location":"configurations/domejudge/#domjudge","text":"","title":"DomJudge"},{"location":"configurations/domejudge/#starting-mariabdb-container","text":"docker run -d --name dj-mariadb -v data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD = hED522QC@pwDR $n + -e MYSQL_USER = domjudge -e MYSQL_PASSWORD = _D-F%X2KfKNSqCQB -e MYSQL_DATABASE = domjudge -p 13306 :3306 mariadb --max-connections = 1000","title":"Starting mariabdb container ..."},{"location":"configurations/domejudge/#starting-domserver-container","text":"docker run -d -v /sys/fs/cgroup:/sys/fs/cgroup:ro --link dj-mariadb:mariadb -e MYSQL_HOST = mariadb -e MYSQL_USER = domjudge -e MYSQL_DATABASE = domjudge -e MYSQL_PASSWORD = _D-F%X2KfKNSqCQB -e MYSQL_ROOT_PASSWORD = hED522QC@pwDR $n + -p 80 :80 --name domserver domjudge/domserver:latest Required for the judgehosts to work Make sure you change the password of the judgehost account in the webinterface to something and write down the value into the JUDGEDAEMON_PASSWORD environment variable, otherwise the judgehosts will fail to start.","title":"Starting domserver container ...."},{"location":"configurations/domejudge/#starting-judgehost-0-container","text":"docker run -d --privileged -v /sys/fs/cgroup:/sys/fs/cgroup:ro --link domserver:domserver -e DOMSERVER_BASEURL = http://domserver/ -e JUDGEDAEMON_PASSWORD = Bng2TV2Nv%mPdfW! --name judgehost-0 --link domserver:domserver --hostname judgedaemon-0 -e DAEMON_ID = 0 domjudge/judgehost:latest Note Even if the domejudge is down the web interface can't detect that and the status Active: yes is still there with a green flag ( Status:OK ), after some time it is replaced with the yellow flag ( Status:Warning ) then the red one ( Status:Critical) but the status Active:yes is still there. if the judghost is booted up again the green flag ( Status:OK ) will showed up agin If we run another judgehost (with another DAEMON_ID ), it will show up in the judgehosts web interface JUDGEDAEMON_USERNAME (defaults to judgehost ) environment variable should be the password for the JUDGEDAEMON_PASSWORD (defaults to password ) user. This should be ready before starting the domjudge If the db container took down the web interface will go out of service When the db container is rebooted up :Boom: the web interface is reactive again and the data is also persitested.","title":"Starting judgehost-0 container ..."},{"location":"configurations/gcloud/","text":"Cheat sheet create gcloud compute instances start swarm-manager set-machine-type Before set-machine-type gcloud config set compute/zone us-central1-a Instances should be stopped gcloud compute instances set-machine-type swarm-manager --machine-type g1-small stop gcloud compute instances stop swarm-manager Completion makes life easier, just use it eval $(gcloud completion zsh)","title":"Cheat sheet"},{"location":"configurations/gcloud/#cheat-sheet","text":"","title":"Cheat sheet"},{"location":"configurations/gcloud/#create","text":"gcloud compute instances start swarm-manager","title":"create"},{"location":"configurations/gcloud/#set-machine-type","text":"Before set-machine-type gcloud config set compute/zone us-central1-a Instances should be stopped gcloud compute instances set-machine-type swarm-manager --machine-type g1-small","title":"set-machine-type"},{"location":"configurations/gcloud/#stop","text":"gcloud compute instances stop swarm-manager","title":"stop"},{"location":"configurations/gcloud/#completion-makes-life-easier-just-use-it","text":"eval $(gcloud completion zsh)","title":"Completion makes life easier, just use it"},{"location":"configurations/hosting/","text":"VPS vs Dedicated server Hosting VPS Hosting offers is a VM running in a shared server with others, each VM has its own resource limits: CPU, RAM ... which usually can be changed on-demand Dedicated servers offers a physical server with a predefined set of resources dedicated exclusively to one user which has much more horse power and thus much more expensive VPS are cheap and offer on-demand resources, you can start small and allocate resources whenever you need them. Dedicated servers offer more resource, control and security since you're the only one using a physical server, no one else is running anything but you. Note: If you are just starting, you have a regular application and you don't know what you need, just use a VPS you won't usually see any difference at this stage. Cloud providers comparison for VPS offers Contabo Prices are dead cheap, it's is too good to be true you can get a 6vCPU, 16 GB of RAM and 100 GB NVMe VPS for $12 which only gets you a 1vCPU, 2 GB of RAM and 50 GB NVMe in Digital Ocean According to this benchmark , this this one they have the lowest performance Despite that, I'm still curious and seduced by the price and I'm wiling to give it a shot and see the results myself Digital Ocean Moderate prices, simple to use, predictable and easy to understand pricing https://www.digitalocean.com/pricing $20: 2vCPU, 4 GB RAM, 80 GB SSD Google Cloud High prices, a hell of a platform with tons and tons of services, complex and fine-grained pricing model: it charges for CPU, RAM, Disk, Network separately and has many many machine types and customizations Prices for an E2 machine type, there are other expensive types like N1 and N2 but Google claim to offer similar performance at lower cost with E2 machines $25: 2vCPU, 4 GB RAM $13: 80 GB SSD or $3.2 for HDD disk or $8 for a balanced SSD disk if we go for the balanced SSD, GCP is $13/month (or 50%) more expensive than DigitalOcean AWS Pretty much the same as GCP with more services and more complex pricing models, also the learning curve is steeper an requires much more knowledge and time to get started. Not used it so much, I can't estimate the prices right now AWS Lightsail Virtual private servers for a low, predictable price Looks like a DigitalOcean offered by AWS, similar prices and offers https://aws.amazon.com/lightsail/pricing/?opdp1=pricing Exact same price ($20) as Digital Ocean","title":"VPS vs Dedicated server Hosting"},{"location":"configurations/hosting/#vps-vs-dedicated-server-hosting","text":"VPS Hosting offers is a VM running in a shared server with others, each VM has its own resource limits: CPU, RAM ... which usually can be changed on-demand Dedicated servers offers a physical server with a predefined set of resources dedicated exclusively to one user which has much more horse power and thus much more expensive VPS are cheap and offer on-demand resources, you can start small and allocate resources whenever you need them. Dedicated servers offer more resource, control and security since you're the only one using a physical server, no one else is running anything but you. Note: If you are just starting, you have a regular application and you don't know what you need, just use a VPS you won't usually see any difference at this stage.","title":"VPS vs Dedicated server Hosting"},{"location":"configurations/hosting/#cloud-providers-comparison-for-vps-offers","text":"","title":"Cloud providers comparison for VPS offers"},{"location":"configurations/hosting/#contabo","text":"Prices are dead cheap, it's is too good to be true you can get a 6vCPU, 16 GB of RAM and 100 GB NVMe VPS for $12 which only gets you a 1vCPU, 2 GB of RAM and 50 GB NVMe in Digital Ocean According to this benchmark , this this one they have the lowest performance Despite that, I'm still curious and seduced by the price and I'm wiling to give it a shot and see the results myself","title":"Contabo"},{"location":"configurations/hosting/#digital-ocean","text":"Moderate prices, simple to use, predictable and easy to understand pricing https://www.digitalocean.com/pricing $20: 2vCPU, 4 GB RAM, 80 GB SSD","title":"Digital Ocean"},{"location":"configurations/hosting/#google-cloud","text":"High prices, a hell of a platform with tons and tons of services, complex and fine-grained pricing model: it charges for CPU, RAM, Disk, Network separately and has many many machine types and customizations Prices for an E2 machine type, there are other expensive types like N1 and N2 but Google claim to offer similar performance at lower cost with E2 machines $25: 2vCPU, 4 GB RAM $13: 80 GB SSD or $3.2 for HDD disk or $8 for a balanced SSD disk if we go for the balanced SSD, GCP is $13/month (or 50%) more expensive than DigitalOcean","title":"Google Cloud"},{"location":"configurations/hosting/#aws","text":"Pretty much the same as GCP with more services and more complex pricing models, also the learning curve is steeper an requires much more knowledge and time to get started. Not used it so much, I can't estimate the prices right now","title":"AWS"},{"location":"configurations/hosting/#aws-lightsail","text":"Virtual private servers for a low, predictable price Looks like a DigitalOcean offered by AWS, similar prices and offers https://aws.amazon.com/lightsail/pricing/?opdp1=pricing Exact same price ($20) as Digital Ocean","title":"AWS Lightsail"},{"location":"configurations/kafka/","text":"Kafka Consumer Consumption Status kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group test-migration | awk '{lag_sum += $5; total_sum += $4; done_sum += $3} END {print \"Completed count: \" done_sum \", Lag: \" lag_sum \", Total number of records: \" total_sum}' Get number of messages in a topic kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic mytopic --time -1 --offsets 1 | awk -F \":\" '{sum += $3} END {print sum}' View the details of a consumer group kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group <group name> List the consumer groups known to Kafka kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list Kafka Consumers: Reading Data from Kafka Kafka Consumer Concepts Consumers and Consumer Groups Kafka consumers are typically part of a consumer group . When multiple consumers are subscribed to a topic and belong to the same consumer group, each consumer in the group will receive messages from a different subset of the partitions in the topic. If we add more consumers to a single group with a single topic than we have partitions, some of the consumers will be idle and get no messages at all. To summarize, you create a new consumer group for each application that needs all the messages from one or more topics. You add consumers to an existing consumer group to scale the reading and processing of messages from the topics, so each additional consumer in a group will only get a subset of the messages. Consumer Groups and Partition Rebalance Rebalance is the process of changing the ownership of a topic from consumer to another, it happens when a consumer cashes, when a new consumer start consuming or when the topic is modified (eg. the number of partitions is modified), during rebalance the entire consumer groups can't consumer the messages, it causes a short window of unavailability .","title":"Kafka"},{"location":"configurations/kafka/#kafka","text":"","title":"Kafka"},{"location":"configurations/kafka/#consumer-consumption-status","text":"kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group test-migration | awk '{lag_sum += $5; total_sum += $4; done_sum += $3} END {print \"Completed count: \" done_sum \", Lag: \" lag_sum \", Total number of records: \" total_sum}'","title":"Consumer Consumption Status"},{"location":"configurations/kafka/#get-number-of-messages-in-a-topic","text":"kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic mytopic --time -1 --offsets 1 | awk -F \":\" '{sum += $3} END {print sum}'","title":"Get number of messages in a topic"},{"location":"configurations/kafka/#view-the-details-of-a-consumer-group","text":"kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group <group name>","title":"View the details of a consumer group"},{"location":"configurations/kafka/#list-the-consumer-groups-known-to-kafka","text":"kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list","title":"List the consumer groups known to Kafka"},{"location":"configurations/kafka/#kafka-consumers-reading-data-from-kafka","text":"","title":"Kafka Consumers: Reading Data from Kafka"},{"location":"configurations/kafka/#kafka-consumer-concepts","text":"","title":"Kafka Consumer Concepts"},{"location":"configurations/kafka/#consumers-and-consumer-groups","text":"Kafka consumers are typically part of a consumer group . When multiple consumers are subscribed to a topic and belong to the same consumer group, each consumer in the group will receive messages from a different subset of the partitions in the topic. If we add more consumers to a single group with a single topic than we have partitions, some of the consumers will be idle and get no messages at all. To summarize, you create a new consumer group for each application that needs all the messages from one or more topics. You add consumers to an existing consumer group to scale the reading and processing of messages from the topics, so each additional consumer in a group will only get a subset of the messages.","title":"Consumers and Consumer Groups"},{"location":"configurations/kafka/#consumer-groups-and-partition-rebalance","text":"Rebalance is the process of changing the ownership of a topic from consumer to another, it happens when a consumer cashes, when a new consumer start consuming or when the topic is modified (eg. the number of partitions is modified), during rebalance the entire consumer groups can't consumer the messages, it causes a short window of unavailability .","title":"Consumer Groups and Partition Rebalance"},{"location":"configurations/log4j2/","text":"Apache Log4j 2 Log4j 2 tends to be the most recent logging framework for java. Apache Log4j 2 is an upgrade to Log4j that provides significant improvements over its predecessor, Log4j 1.x, and provides many of the improvements available in Logback while fixing some inherent problems in Logback\u2019s architecture. Configuraiton <?xml version=\"1.0\" encoding=\"UTF-8\"?> ; <Configuration> <Properties> <Property name= \"name1\" > value </property> <Property name= \"name2\" value= \"value2\" /> </Properties> <filter ... /> <Appenders> <appender ... > <filter ... /> </appender> ... </Appenders> <Loggers> <Logger name= \"name1\" > <filter ... /> </Logger> ... <Root level= \"level\" > <AppenderRef ref= \"name\" /> </Root> </Loggers> </Configuration> Rolling Over Default Rollover Strategy The default rollover strategy accepts both a date/time pattern and an integer from the filePattern attribute specified on the RollingFileAppender itself. If the date/time pattern is present it will be replaced with the current date and time values. If the pattern contains an integer it will be incremented on each rollover. If the pattern contains both a date/time and integer in the pattern the integer will be incremented until the result of the date/time pattern changes. If the file pattern ends with \".gz\", \".zip\", \".bz2\", \".deflate\", \".pack200\", or \".xz\" the resulting archive will be compressed using the compression scheme that matches the suffix Note if the fileIndex attribute is set to \"nomax\" then the min and max values will be ignored and file numbering will increment by 1 and each rollover will have an incrementally higher value with no maximum number of files.","title":"Apache Log4j 2"},{"location":"configurations/log4j2/#apache-log4j-2","text":"Log4j 2 tends to be the most recent logging framework for java. Apache Log4j 2 is an upgrade to Log4j that provides significant improvements over its predecessor, Log4j 1.x, and provides many of the improvements available in Logback while fixing some inherent problems in Logback\u2019s architecture.","title":"Apache Log4j 2"},{"location":"configurations/log4j2/#configuraiton","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?> ; <Configuration> <Properties> <Property name= \"name1\" > value </property> <Property name= \"name2\" value= \"value2\" /> </Properties> <filter ... /> <Appenders> <appender ... > <filter ... /> </appender> ... </Appenders> <Loggers> <Logger name= \"name1\" > <filter ... /> </Logger> ... <Root level= \"level\" > <AppenderRef ref= \"name\" /> </Root> </Loggers> </Configuration>","title":"Configuraiton"},{"location":"configurations/log4j2/#rolling-over","text":"","title":"Rolling Over"},{"location":"configurations/log4j2/#default-rollover-strategy","text":"The default rollover strategy accepts both a date/time pattern and an integer from the filePattern attribute specified on the RollingFileAppender itself. If the date/time pattern is present it will be replaced with the current date and time values. If the pattern contains an integer it will be incremented on each rollover. If the pattern contains both a date/time and integer in the pattern the integer will be incremented until the result of the date/time pattern changes. If the file pattern ends with \".gz\", \".zip\", \".bz2\", \".deflate\", \".pack200\", or \".xz\" the resulting archive will be compressed using the compression scheme that matches the suffix Note if the fileIndex attribute is set to \"nomax\" then the min and max values will be ignored and file numbering will increment by 1 and each rollover will have an incrementally higher value with no maximum number of files.","title":"Default Rollover Strategy"},{"location":"configurations/logback/","text":"LogBack How it works ? Let us begin by discussing the initialization steps that logback follows to try to configure itself: Logback tries to find a file called logback.groovy in the classpath. If no such file is found, logback tries to find a file called logback-test.xml in the classpath. If no such file is found, it checks for the file logback.xml in the classpath. If neither file is found, logback configures itself automatically using the BasicConfigurator which will cause logging output to be directed to the console. This minimal configuration consists of a ConsoleAppender attached to the root logger. The output is formatted using a PatternLayoutEncoder set to the pattern %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} \u2013 %msg%n . Moreover, by default the root logger is assigned the DEBUG level. Loggers Logger is the client code that executes the log events Every single logger is attached to a LoggerContext which is responsible for manufacturing loggers as well as arranging them in a tree like hierarchy. Level order: TRACE < DEBUG < INFO < WARN < ERROR . Retrieving Loggers: LoggerFactory.getLogger Loggers are named entities. Their names are case-sensitive and they follow the hierarchical naming rule (like java classes) Appenders In logback speak, an output destination is called an appender. Appenders must implement the ch.qos.logback.core.Appender . More than one appender can be attached to a logger. Appenders are inherited additively from the logger hierarchy this is called Appender Adivitiy and can be disabled by setting AditivityFlag to false . Layouts The layout is responsible for formatting the logging request. Example: %-4relative [%thread] %-5level %logger{32} - %msg%n will output something akin to: 176 [ main ] DEBUG manual . architecture . HelloWorld2 - Hello world . Parametrized logging logger . debug ( \"Entry number: \" + i + \" is \" + String . valueOf ( entry [ i ] ); This is will incur the cost of evaluting and constructing the string message even if the debug logging is not enabled A better alternative is: Object entry = new SomeObject (); logger . debug ( \"The entry is {}.\" , entry ); A peek under the hood Let us now analyze the steps logback takes when the user invokes the info() method of a logger named com.wombat Get the filter chain decision: If it exists, the TurboFilter chain is invoked. Turbo filters can set a context-wide threshold, or filter out certain events based on information such as Marker , Level , Logger , message, or the Throwable that are associated with each logging request. If the reply of the filter chain is FilterReply.DENY , then the logging request is dropped. If it is FilterReply.NEUTRAL , then we continue with the next step, i.e. step 2. In case the reply is FilterReply.ACCEPT , we skip the next and directly jump to step 3. Apply the basic selection rule Create a LoggingEvent object Invoking appenders Formatting the output Sending out the LoggingEvent Basic Configuration <configuration> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <!-- encoders are assigned the type ch.qos.logback.classic.encoder.PatternLayoutEncoder by default --> <encoder> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </encoder> </appender> <root level= \"debug\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration> you can instruct the configuration file to dump status data, even in the absence of errors. To achieve this, you need to set the debug attribute of the configuration element Specifying the location of the default configuration file as a system property java -Dlogback.configurationFile = /path/to/config.xml chapters.configuration.MyApp1 Configuration file syntax Configuring loggers, or the <logger> element Attributes : name , level , additivity , inherited (force it to inherit from ancestor) The logger element my contain zero or more appender-ref Configuring the root logger, or the <root> element The root logger accept only the level attribute and my contain zero or more appender-ref Note ! Let us note that the basic-selection rule depends on the effective level of the logger being invoked, not the level of the logger where appenders are attached. Logback will first determine whether a logging statement is enabled or not, and if enabled, it will invoke the appenders found in the logger hierarchy, regardless of their level. The following configuration file is a case in point <configuration> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </encoder> </appender> <logger name= \"chapters.configuration\" level= \"INFO\" /> <!-- turn OFF all logging (children can override) --> <root level= \"OFF\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration> the level of the root logger has no apparent effect because the loggers in chapters.configuration.* are enabled for the INFO level Configuring Appender Example <configuration> <appender name= \"FILE\" class= \"ch.qos.logback.core.FileAppender\" > <file> myApp.log </file> <encoder> <pattern> %date %level [%thread] %logger{10} [%file:%line] %msg%n </pattern> </encoder> </appender> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %msg%n </pattern> </encoder> </appender> <root level= \"debug\" > <appender-ref ref= \"FILE\" /> <appender-ref ref= \"STDOUT\" /> </root> </configuration> Be careful By default, appenders are cumulative : a logger will log to the appenders attached to itself (if any) as well as all the appenders attached to its ancestors. Thus, attaching the same appender to multiple loggers will cause logging output to be duplicated . How to disable ? <configuration> <appender name= \"FILE\" class= \"ch.qos.logback.core.FileAppender\" > <file> foo.log </file> <encoder> <pattern> %date %level [%thread] %logger{10} [%file : %line] %msg%n </pattern> </encoder> </appender> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %msg%n </pattern> </encoder> </appender> <logger name= \"chapters.configuration.Foo\" additivity= \"false\" > <appender-ref ref= \"FILE\" /> </logger> <root level= \"debug\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration> Sniffing Appender <configuration> <appender name= \"SIFT\" class= \"ch.qos.logback.classic.sift.SiftingAppender\" > <!-- in the absence of the class attribute, it is assumed that the desired discriminator type is ch.qos.logback.classic.sift.MDCBasedDiscriminator --> <discriminator> <key> userid </key> <defaultValue> unknown </defaultValue> </discriminator> <sift> <appender name= \"FILE-${userid}\" class= \"ch.qos.logback.core.FileAppender\" > <file> ${userid}.log </file> <append> false </append> <layout class= \"ch.qos.logback.classic.PatternLayout\" > <pattern> %d [%thread] %level %mdc %logger{35} - %msg%n </pattern> </layout> </appender> </sift> </appender> <root level= \"DEBUG\" > <appender-ref ref= \"SIFT\" /> </root> </configuration> Default Mapping Classes Mapped Diagnostic Context To use MDC just call the MDC.put method and refer to the variable with %key in the pattern of layouts or ${key} in the file name pattern of Appenders using SiftingAppender Conditions <configuration debug= \"true\" > <if condition= 'property(\"HOSTNAME\").contains(\"torino\")' > <then> <appender name= \"CON\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %d %-5level %logger{35} - %msg %n </pattern> </encoder> </appender> <root> <appender-ref ref= \"CON\" /> </root> </then> </if> <appender name= \"FILE\" class= \"ch.qos.logback.core.FileAppender\" > <file> ${randomOutputDir}/conditional.log </file> <encoder> <pattern> %d %-5level %logger{35} - %msg %n </pattern> </encoder> </appender> <root level= \"ERROR\" > <appender-ref ref= \"FILE\" /> </root> </configuration> Size and time based rolling policy <configuration> <appender name = \"ROLLING\" class = \"ch.qos.logback.core.rolling.RollingFileAppender\" > <file>mylog.txt</file> <rollingPolicy class = \"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\" > <!-- rollover daily --> <fileNamePattern>mylog-%d { yyyy-MM-dd } .%i.txt</fileNamePattern> <!-- each file should be at most 100MB, keep 60 days worth of history, but at most 20GB --> <maxFileSize>100MB</maxFileSize> <!-- if maxHistory is not set: 'maxHistory' is not set, ignoring 'totalSizeCap' option with value [ 10 KB ] --> <maxHistory>60</maxHistory> <totalSizeCap>20GB</totalSizeCap> </rollingPolicy> <encoder> <pattern>%msg%n</pattern> </encoder> </appender> <root level = \"DEBUG\" > <appender-ref ref = \"ROLLING\" /> </root> </configuration> Note totalSizeCap is ignored if maxHistory is not set Compression To enable compression just name your file *.gz or *.zip and logback will do that for you Example file property set to /wombat/foo.txt : During November 23rd, 2009, logging output will go to the file /wombat/foo.txt . At midnight that file will be compressed and renamed as /wombat/foo.2009-11-23.gz . A new /wombat/foo.txt file will be created where logging output will go for the rest of November 24rd. At midnight November 24th, /wombat/foo.txt will be compressed and renamed as /wombat/foo.2009-11-24.gz and so on. Note File compresion is not support in prudent mode","title":"LogBack"},{"location":"configurations/logback/#logback","text":"","title":"LogBack"},{"location":"configurations/logback/#how-it-works","text":"Let us begin by discussing the initialization steps that logback follows to try to configure itself: Logback tries to find a file called logback.groovy in the classpath. If no such file is found, logback tries to find a file called logback-test.xml in the classpath. If no such file is found, it checks for the file logback.xml in the classpath. If neither file is found, logback configures itself automatically using the BasicConfigurator which will cause logging output to be directed to the console. This minimal configuration consists of a ConsoleAppender attached to the root logger. The output is formatted using a PatternLayoutEncoder set to the pattern %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} \u2013 %msg%n . Moreover, by default the root logger is assigned the DEBUG level.","title":"How it works ?"},{"location":"configurations/logback/#loggers","text":"Logger is the client code that executes the log events Every single logger is attached to a LoggerContext which is responsible for manufacturing loggers as well as arranging them in a tree like hierarchy. Level order: TRACE < DEBUG < INFO < WARN < ERROR . Retrieving Loggers: LoggerFactory.getLogger Loggers are named entities. Their names are case-sensitive and they follow the hierarchical naming rule (like java classes)","title":"Loggers"},{"location":"configurations/logback/#appenders","text":"In logback speak, an output destination is called an appender. Appenders must implement the ch.qos.logback.core.Appender . More than one appender can be attached to a logger. Appenders are inherited additively from the logger hierarchy this is called Appender Adivitiy and can be disabled by setting AditivityFlag to false .","title":"Appenders"},{"location":"configurations/logback/#layouts","text":"The layout is responsible for formatting the logging request. Example: %-4relative [%thread] %-5level %logger{32} - %msg%n will output something akin to: 176 [ main ] DEBUG manual . architecture . HelloWorld2 - Hello world .","title":"Layouts"},{"location":"configurations/logback/#parametrized-logging","text":"logger . debug ( \"Entry number: \" + i + \" is \" + String . valueOf ( entry [ i ] ); This is will incur the cost of evaluting and constructing the string message even if the debug logging is not enabled A better alternative is: Object entry = new SomeObject (); logger . debug ( \"The entry is {}.\" , entry );","title":"Parametrized logging"},{"location":"configurations/logback/#a-peek-under-the-hood","text":"Let us now analyze the steps logback takes when the user invokes the info() method of a logger named com.wombat Get the filter chain decision: If it exists, the TurboFilter chain is invoked. Turbo filters can set a context-wide threshold, or filter out certain events based on information such as Marker , Level , Logger , message, or the Throwable that are associated with each logging request. If the reply of the filter chain is FilterReply.DENY , then the logging request is dropped. If it is FilterReply.NEUTRAL , then we continue with the next step, i.e. step 2. In case the reply is FilterReply.ACCEPT , we skip the next and directly jump to step 3. Apply the basic selection rule Create a LoggingEvent object Invoking appenders Formatting the output Sending out the LoggingEvent","title":"A peek under the hood"},{"location":"configurations/logback/#basic-configuration","text":"<configuration> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <!-- encoders are assigned the type ch.qos.logback.classic.encoder.PatternLayoutEncoder by default --> <encoder> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </encoder> </appender> <root level= \"debug\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration> you can instruct the configuration file to dump status data, even in the absence of errors. To achieve this, you need to set the debug attribute of the configuration element","title":"Basic Configuration"},{"location":"configurations/logback/#specifying-the-location-of-the-default-configuration-file-as-a-system-property","text":"java -Dlogback.configurationFile = /path/to/config.xml chapters.configuration.MyApp1","title":"Specifying the location of the default configuration file as a system property"},{"location":"configurations/logback/#configuration-file-syntax","text":"","title":"Configuration file syntax"},{"location":"configurations/logback/#configuring-loggers-or-theloggerelement","text":"Attributes : name , level , additivity , inherited (force it to inherit from ancestor) The logger element my contain zero or more appender-ref","title":"Configuring loggers, or the&lt;logger&gt;element"},{"location":"configurations/logback/#configuring-the-root-logger-or-therootelement","text":"The root logger accept only the level attribute and my contain zero or more appender-ref","title":"Configuring the root logger, or the&lt;root&gt;element"},{"location":"configurations/logback/#note","text":"Let us note that the basic-selection rule depends on the effective level of the logger being invoked, not the level of the logger where appenders are attached. Logback will first determine whether a logging statement is enabled or not, and if enabled, it will invoke the appenders found in the logger hierarchy, regardless of their level. The following configuration file is a case in point <configuration> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </encoder> </appender> <logger name= \"chapters.configuration\" level= \"INFO\" /> <!-- turn OFF all logging (children can override) --> <root level= \"OFF\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration> the level of the root logger has no apparent effect because the loggers in chapters.configuration.* are enabled for the INFO level","title":"Note !"},{"location":"configurations/logback/#configuring-appender","text":"","title":"Configuring Appender"},{"location":"configurations/logback/#example","text":"<configuration> <appender name= \"FILE\" class= \"ch.qos.logback.core.FileAppender\" > <file> myApp.log </file> <encoder> <pattern> %date %level [%thread] %logger{10} [%file:%line] %msg%n </pattern> </encoder> </appender> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %msg%n </pattern> </encoder> </appender> <root level= \"debug\" > <appender-ref ref= \"FILE\" /> <appender-ref ref= \"STDOUT\" /> </root> </configuration>","title":"Example"},{"location":"configurations/logback/#be-careful","text":"By default, appenders are cumulative : a logger will log to the appenders attached to itself (if any) as well as all the appenders attached to its ancestors. Thus, attaching the same appender to multiple loggers will cause logging output to be duplicated .","title":"Be careful"},{"location":"configurations/logback/#how-to-disable","text":"<configuration> <appender name= \"FILE\" class= \"ch.qos.logback.core.FileAppender\" > <file> foo.log </file> <encoder> <pattern> %date %level [%thread] %logger{10} [%file : %line] %msg%n </pattern> </encoder> </appender> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %msg%n </pattern> </encoder> </appender> <logger name= \"chapters.configuration.Foo\" additivity= \"false\" > <appender-ref ref= \"FILE\" /> </logger> <root level= \"debug\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration>","title":"How to disable ?"},{"location":"configurations/logback/#sniffing-appender","text":"<configuration> <appender name= \"SIFT\" class= \"ch.qos.logback.classic.sift.SiftingAppender\" > <!-- in the absence of the class attribute, it is assumed that the desired discriminator type is ch.qos.logback.classic.sift.MDCBasedDiscriminator --> <discriminator> <key> userid </key> <defaultValue> unknown </defaultValue> </discriminator> <sift> <appender name= \"FILE-${userid}\" class= \"ch.qos.logback.core.FileAppender\" > <file> ${userid}.log </file> <append> false </append> <layout class= \"ch.qos.logback.classic.PatternLayout\" > <pattern> %d [%thread] %level %mdc %logger{35} - %msg%n </pattern> </layout> </appender> </sift> </appender> <root level= \"DEBUG\" > <appender-ref ref= \"SIFT\" /> </root> </configuration>","title":"Sniffing Appender"},{"location":"configurations/logback/#default-mapping-classes","text":"","title":"Default Mapping Classes"},{"location":"configurations/logback/#mapped-diagnostic-context","text":"To use MDC just call the MDC.put method and refer to the variable with %key in the pattern of layouts or ${key} in the file name pattern of Appenders using SiftingAppender","title":"Mapped Diagnostic Context"},{"location":"configurations/logback/#conditions","text":"<configuration debug= \"true\" > <if condition= 'property(\"HOSTNAME\").contains(\"torino\")' > <then> <appender name= \"CON\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %d %-5level %logger{35} - %msg %n </pattern> </encoder> </appender> <root> <appender-ref ref= \"CON\" /> </root> </then> </if> <appender name= \"FILE\" class= \"ch.qos.logback.core.FileAppender\" > <file> ${randomOutputDir}/conditional.log </file> <encoder> <pattern> %d %-5level %logger{35} - %msg %n </pattern> </encoder> </appender> <root level= \"ERROR\" > <appender-ref ref= \"FILE\" /> </root> </configuration>","title":"Conditions"},{"location":"configurations/logback/#size-and-time-based-rolling-policy","text":"<configuration> <appender name = \"ROLLING\" class = \"ch.qos.logback.core.rolling.RollingFileAppender\" > <file>mylog.txt</file> <rollingPolicy class = \"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\" > <!-- rollover daily --> <fileNamePattern>mylog-%d { yyyy-MM-dd } .%i.txt</fileNamePattern> <!-- each file should be at most 100MB, keep 60 days worth of history, but at most 20GB --> <maxFileSize>100MB</maxFileSize> <!-- if maxHistory is not set: 'maxHistory' is not set, ignoring 'totalSizeCap' option with value [ 10 KB ] --> <maxHistory>60</maxHistory> <totalSizeCap>20GB</totalSizeCap> </rollingPolicy> <encoder> <pattern>%msg%n</pattern> </encoder> </appender> <root level = \"DEBUG\" > <appender-ref ref = \"ROLLING\" /> </root> </configuration> Note totalSizeCap is ignored if maxHistory is not set","title":"Size and time based rolling policy"},{"location":"configurations/logback/#compression","text":"To enable compression just name your file *.gz or *.zip and logback will do that for you Example file property set to /wombat/foo.txt : During November 23rd, 2009, logging output will go to the file /wombat/foo.txt . At midnight that file will be compressed and renamed as /wombat/foo.2009-11-23.gz . A new /wombat/foo.txt file will be created where logging output will go for the rest of November 24rd. At midnight November 24th, /wombat/foo.txt will be compressed and renamed as /wombat/foo.2009-11-24.gz and so on. Note File compresion is not support in prudent mode","title":"Compression"},{"location":"configurations/prometheus/","text":"Metrics types Count: incresed only values Guage: single values that could go up or down Histogram: A histogram samples observations (usually things like request durations or response sizes) and counts them in configurable buckets. It also provides a sum of all observed values. A histogram with a base metric name of <basename> exposes multiple time series during a scrape: - cumulative counters for the observation buckets, exposed as <basename>_bucket{le=\"<upper inclusive bound>\"} - the total sum of all observed values, exposed as <basename>_sum - the count of events that have been observed, exposed as <basename>_count (identical to <basename>_bucket{le=\"+Inf\"} above) Summary Similar to a histogram, a summary samples observations (usually things like request durations and response sizes). While it also provides a total count of observations and a sum of all observed values, it calculates configurable quantiles over a sliding time window. A summary with a base metric name of <basename> exposes multiple time series during a scrape: streaming \u03c6-quantiles (0 \u2264 \u03c6 \u2264 1) of observed events, exposed as <basename>{quantile=\"<\u03c6>\"} the total sum of all observed values, exposed as <basename>_sum the count of events that have been observed, exposed as <basename>_count See histograms and summaries for detailed explanations of \u03c6-quantiles, summary usage, and differences to histograms. Histogram Operations Average request duration To calculate the average request duration during the last 5 minutes from a histogram or summary called http_request_duration_seconds, use the following expression: rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m]) Percentage of requests served within 300ms The following expression calculates it by job for the requests served in the last 5 minutes sum(rate(http_request_duration_seconds_bucket{le=\"0.3\"}[5m])) by (job) / sum(rate(http_request_duration_seconds_count[5m])) by (job) Apdex Score (see below) The target request duration is 300ms. The tolerable request duration is 1.2s. The following expression yields the Apdex score for each job over the last 5 minutes: (sum(rate(http_request_duration_seconds_bucket{le=\"0.3\"}[5m])) by (job) + sum(rate(http_request_duration_seconds_bucket{le=\"1.2\"}[5m])) by (job)) / 2 / sum(rate(http_request_duration_seconds_count[5m])) by (job) Apdex wiki: https://en.wikipedia.org/wiki/Apdex Apdex tries to calculate the user satisfaction as a range from 0 (no user satisfied) to 1 (all users satisfied) using the formula: Apdex_t = \\frac{SatisfiedCount + \\farc{ToleratingCount}{2}}{TotalSamples} Example: if there are 100 samples with a target time of 3 seconds, where 60 are below 3 seconds, 30 are between 3 and 12 seconds, and the remaining 10 are above 12 seconds, the Apdex score is: Apdex_3 = (60 + (30/2)) / 100 = 0.75 Google's SLI sum(rate(http_requests_total{host=\"api\", status!~\"5..\"}[7d])) / sum(rate(http_requests_total{host=\"api\"}[7d])) Functions rate Rate calculate the per-second increase rate of a rate(v range-vector) calculates the per-second average rate of increase of the time series in the range vector (could be generated by [time] syntax), it handles counter resets correctly and it does extrapolation to handle missing scraps and imperfect alignment. Example: Let's say we have the following 2 minutes range vector that we got using: <counter_name>[2m] 1095 @ 1586350105.912 1095 @ 1586350120.912 1095 @ 1586350135.912 1096 @ 1586350150.912 1096 @ 1586350165.912 1096 @ 1586350180.912 1096 @ 1586350195.912 1096 @ 1586350210.912 Calling rate(<counter_name>[2m]) should return per-second the rate by which the <counter_name> increased which is in this case \\frac{1096 - 1095}{2 * 60} = \\frac{1}{120} = 0.008333333 right ? Nah !! remember rate does extrapolation in case of imperfect alignment 1586350210 (last scrap) - 1586350105 (first scrap) = 105 seconds but we are calculating rate over two minutes which is 120 seconds hence extraplation: 105 seconds -------------------- 1 120 seconds -------------------- x that gives us x = \\frac{120}{105} = 1.142857143 Notes: - Don't use with guage, it will consider values going down as counter resets - combining rate() with an aggregation operator (e.g. sum() ) or a function aggregating over time (any function ending in _over_time ), always take a rate() first, then aggregate. Otherwise rate() cannot detect counter resets when your target restarts. histogram_quantile Example: buckets = [{0.1 0} {0.25 5} {0.5 7} {1 10} {+Inf 10}] q = 0.95 histogram_quantile(q, buckets) = 0.9166666666666667 Why using the 95% quantile ? - TL;DR: It's more realistic to ignore the top 5% exceptional values - Resources: - https://en.wikipedia.org/wiki/Burstable_billing - https://thwack.solarwinds.com/t5/NPM-Discussions/Purpose-of-the-95-percentile-in-the-graph/td-p/275438","title":"Metrics types"},{"location":"configurations/prometheus/#metrics-types","text":"Count: incresed only values Guage: single values that could go up or down Histogram: A histogram samples observations (usually things like request durations or response sizes) and counts them in configurable buckets. It also provides a sum of all observed values. A histogram with a base metric name of <basename> exposes multiple time series during a scrape: - cumulative counters for the observation buckets, exposed as <basename>_bucket{le=\"<upper inclusive bound>\"} - the total sum of all observed values, exposed as <basename>_sum - the count of events that have been observed, exposed as <basename>_count (identical to <basename>_bucket{le=\"+Inf\"} above) Summary Similar to a histogram, a summary samples observations (usually things like request durations and response sizes). While it also provides a total count of observations and a sum of all observed values, it calculates configurable quantiles over a sliding time window. A summary with a base metric name of <basename> exposes multiple time series during a scrape: streaming \u03c6-quantiles (0 \u2264 \u03c6 \u2264 1) of observed events, exposed as <basename>{quantile=\"<\u03c6>\"} the total sum of all observed values, exposed as <basename>_sum the count of events that have been observed, exposed as <basename>_count See histograms and summaries for detailed explanations of \u03c6-quantiles, summary usage, and differences to histograms.","title":"Metrics types"},{"location":"configurations/prometheus/#histogram-operations","text":"Average request duration To calculate the average request duration during the last 5 minutes from a histogram or summary called http_request_duration_seconds, use the following expression: rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m]) Percentage of requests served within 300ms The following expression calculates it by job for the requests served in the last 5 minutes sum(rate(http_request_duration_seconds_bucket{le=\"0.3\"}[5m])) by (job) / sum(rate(http_request_duration_seconds_count[5m])) by (job) Apdex Score (see below) The target request duration is 300ms. The tolerable request duration is 1.2s. The following expression yields the Apdex score for each job over the last 5 minutes: (sum(rate(http_request_duration_seconds_bucket{le=\"0.3\"}[5m])) by (job) + sum(rate(http_request_duration_seconds_bucket{le=\"1.2\"}[5m])) by (job)) / 2 / sum(rate(http_request_duration_seconds_count[5m])) by (job)","title":"Histogram Operations"},{"location":"configurations/prometheus/#apdex","text":"wiki: https://en.wikipedia.org/wiki/Apdex Apdex tries to calculate the user satisfaction as a range from 0 (no user satisfied) to 1 (all users satisfied) using the formula: Apdex_t = \\frac{SatisfiedCount + \\farc{ToleratingCount}{2}}{TotalSamples} Example: if there are 100 samples with a target time of 3 seconds, where 60 are below 3 seconds, 30 are between 3 and 12 seconds, and the remaining 10 are above 12 seconds, the Apdex score is: Apdex_3 = (60 + (30/2)) / 100 = 0.75","title":"Apdex"},{"location":"configurations/prometheus/#googles-sli","text":"sum(rate(http_requests_total{host=\"api\", status!~\"5..\"}[7d])) / sum(rate(http_requests_total{host=\"api\"}[7d]))","title":"Google's SLI"},{"location":"configurations/prometheus/#functions","text":"","title":"Functions"},{"location":"configurations/prometheus/#rate","text":"Rate calculate the per-second increase rate of a rate(v range-vector) calculates the per-second average rate of increase of the time series in the range vector (could be generated by [time] syntax), it handles counter resets correctly and it does extrapolation to handle missing scraps and imperfect alignment. Example: Let's say we have the following 2 minutes range vector that we got using: <counter_name>[2m] 1095 @ 1586350105.912 1095 @ 1586350120.912 1095 @ 1586350135.912 1096 @ 1586350150.912 1096 @ 1586350165.912 1096 @ 1586350180.912 1096 @ 1586350195.912 1096 @ 1586350210.912 Calling rate(<counter_name>[2m]) should return per-second the rate by which the <counter_name> increased which is in this case \\frac{1096 - 1095}{2 * 60} = \\frac{1}{120} = 0.008333333 right ? Nah !! remember rate does extrapolation in case of imperfect alignment 1586350210 (last scrap) - 1586350105 (first scrap) = 105 seconds but we are calculating rate over two minutes which is 120 seconds hence extraplation: 105 seconds -------------------- 1 120 seconds -------------------- x that gives us x = \\frac{120}{105} = 1.142857143 Notes: - Don't use with guage, it will consider values going down as counter resets - combining rate() with an aggregation operator (e.g. sum() ) or a function aggregating over time (any function ending in _over_time ), always take a rate() first, then aggregate. Otherwise rate() cannot detect counter resets when your target restarts.","title":"rate"},{"location":"configurations/prometheus/#histogram_quantile","text":"Example: buckets = [{0.1 0} {0.25 5} {0.5 7} {1 10} {+Inf 10}] q = 0.95 histogram_quantile(q, buckets) = 0.9166666666666667 Why using the 95% quantile ? - TL;DR: It's more realistic to ignore the top 5% exceptional values - Resources: - https://en.wikipedia.org/wiki/Burstable_billing - https://thwack.solarwinds.com/t5/NPM-Discussions/Purpose-of-the-95-percentile-in-the-graph/td-p/275438","title":"histogram_quantile"},{"location":"configurations/security/","text":"GPG Overview: https://www.aplawrence.com/Basics/gpg.html Gnu Handbook: Example: Tom want to send an encrypted message to Marge Marge should Generate a key gpg --gen-key - List keys gpg --list-keys - Export her public key and send it to Tom gpg --armor --export $EMAIL > marge_public_key Tom should: Import Marge's public key gpg --import marge_public_key - Optionally , sign the key if he 's sure that it comes from Marge and it wasn' t intercepted / changed gpg --edit-key $EMAIL gpg > sign - Use it to encrypt a file echo \"A very very confidential message!\" > secrets gpg --out secrets_to_marge --encrypt secrets Tom sends secrets_to_marge somehow (email, hand to hand ...), then marge do gpg --output secrets_from_tom --decrypt secrets_to_marge Pass: password management Install pass sudo apt install pass Create a gpg key gpg --key-gen Init pass pass init mohammedi . haroun @gmail . com Init git and push pass git init pass git remote add origin git @github . com : mohammedi - haroune / password - store . git pass git push --set-upstream origin master Install Password Store App Generate ssh key from the App Add it to Github Settings page Pull passwords Install OpenKeyChain App Generate secret key for your gpg key Copy it to your phone somehow and add it to OpenKeyChain (passphrase required to read it) gpg --armor --export-secret-key mohammedi.haroun@gmail.com Go to the Password Store App, it'll ask for permission to read from OpenKeyChain to be able to decrypt the password got from the git repository, give it the permission and Boom !!! Install chrome extension Install frontend from Chrome webstore Install the native host app from source # Download the code from the releases page: https://github.com/browserpass/browserpass-native/releases # Install go sudo snap install go # Build and test sudo make # Install sudo make install #Configure browserpass for Google Chrome browser, for the current user only make hosts-chrome-user # Or System-wide # sudo make hosts-chrome !!! Note: Installing with apt didn't worked for me because there where no hosts and policies in /usr/lib/browserpass/ Install pass import : https://github.com/roddhjav/pass-import wget -qO - https://pkg.pujol.io/debian/gpgkey | sudo apt-key add - echo 'deb [arch=amd64] https://pkg.pujol.io/debian/repo all main' | sudo tee /etc/apt/sources.list.d/pkg.pujol.io.list sudo apt-get update sudo apt-get install pass-extension-import","title":"GPG"},{"location":"configurations/security/#gpg","text":"Overview: https://www.aplawrence.com/Basics/gpg.html Gnu Handbook:","title":"GPG"},{"location":"configurations/security/#example-tom-want-to-send-an-encrypted-message-to-marge","text":"Marge should Generate a key gpg --gen-key - List keys gpg --list-keys - Export her public key and send it to Tom gpg --armor --export $EMAIL > marge_public_key Tom should: Import Marge's public key gpg --import marge_public_key - Optionally , sign the key if he 's sure that it comes from Marge and it wasn' t intercepted / changed gpg --edit-key $EMAIL gpg > sign - Use it to encrypt a file echo \"A very very confidential message!\" > secrets gpg --out secrets_to_marge --encrypt secrets Tom sends secrets_to_marge somehow (email, hand to hand ...), then marge do gpg --output secrets_from_tom --decrypt secrets_to_marge","title":"Example: Tom want to send an encrypted message to Marge"},{"location":"configurations/security/#pass-password-management","text":"Install pass sudo apt install pass Create a gpg key gpg --key-gen Init pass pass init mohammedi . haroun @gmail . com Init git and push pass git init pass git remote add origin git @github . com : mohammedi - haroune / password - store . git pass git push --set-upstream origin master Install Password Store App Generate ssh key from the App Add it to Github Settings page Pull passwords Install OpenKeyChain App Generate secret key for your gpg key Copy it to your phone somehow and add it to OpenKeyChain (passphrase required to read it) gpg --armor --export-secret-key mohammedi.haroun@gmail.com Go to the Password Store App, it'll ask for permission to read from OpenKeyChain to be able to decrypt the password got from the git repository, give it the permission and Boom !!! Install chrome extension Install frontend from Chrome webstore Install the native host app from source # Download the code from the releases page: https://github.com/browserpass/browserpass-native/releases # Install go sudo snap install go # Build and test sudo make # Install sudo make install #Configure browserpass for Google Chrome browser, for the current user only make hosts-chrome-user # Or System-wide # sudo make hosts-chrome !!! Note: Installing with apt didn't worked for me because there where no hosts and policies in /usr/lib/browserpass/ Install pass import : https://github.com/roddhjav/pass-import wget -qO - https://pkg.pujol.io/debian/gpgkey | sudo apt-key add - echo 'deb [arch=amd64] https://pkg.pujol.io/debian/repo all main' | sudo tee /etc/apt/sources.list.d/pkg.pujol.io.list sudo apt-get update sudo apt-get install pass-extension-import","title":"Pass: password management"},{"location":"configurations/swarm/","text":"Docker Swarm What is a swarm? A swarm consists of multiple Docker hosts which run in swarm mode and act as managers (to manage membership and delegation) and workers (which run swarm services). A key difference between standalone containers and swarm services is that only swarm managers can manage a swarm Nodes A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server. To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes. Services and tasks A service is the definition of the tasks to execute on the manager or worker nodes. A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Load balancing The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. docker-machine export GOOGLE_DISK_SIZE = 10 export GOOGLE_DISK_TYPE = pd-standard export GOOGLE_MACHINE_IMAGE = https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/ubuntu-1804-bionic-v20190320 export GOOGLE_MACHINE_TYPE = g1-small export GOOGLE_NETWORK = default export GOOGLE_PREEMPTIBLE = true export GOOGLE_PROJECT = amanee-ec1eb export GOOGLE_ZONE = us-central1-a export GOOGLE_USERNAME = docker export GOOGLE_USE_EXISTING = true # try to find existing one, otherwise fail # export GOOGLE_USE_INTERNAL_IP= # export GOOGLE_ADDRESS= # export GOOGLE_SUBNETWORK= # export GOOGLE_TAGS= restarting a google cloud with ephemeral ip address cuases docker-machine to fail, running docker-machine regenerate-certs swarm-worker1 fixes the issue but it restarts the docker daemon which might stop running containers. restarting also causes swarm to be broken, we need to leave and re join the nodes. get the token docker swarm join-token worker Response: docker swarm join --token SWMTKN-1-29na4ee9gsnkt0zn6artrtx2bgb2ajeev96k1bhxdxo7xl5gpi-8o8uf1thlf5c31qrrosuppn7d 10 .128.0.8:2377 inspect service docker service inspect --pretty <SERVICE-ID> To see which nodes are running the service: docker service ps <SERVICE-ID> Run docker ps on the node where the task is running to see details about the container for the task. instances doens't have access to the gcr, this is rather a weired behaviour. Docker swarm Init docker swarm init Deploy docker stack deploy -c docker-compose.yml <stack-name> Remove docker stack rm <stack-name> Scale docker service scale <service-name> = <number> Configure a docker-machine shell to the swarm manager So far, you\u2019ve been wrapping Docker commands in docker-machine ssh to talk to the VMs. Another option is to run docker-machine env <machine> to get and run a command that configures your current shell to talk to the Docker daemon on the VM. This method works better for the next step because it allows you to use your local docker-compose.yml file to deploy the app \u201cremotely\u201d without having to copy it","title":"Docker Swarm"},{"location":"configurations/swarm/#docker-swarm","text":"","title":"Docker Swarm"},{"location":"configurations/swarm/#what-is-a-swarm","text":"A swarm consists of multiple Docker hosts which run in swarm mode and act as managers (to manage membership and delegation) and workers (which run swarm services). A key difference between standalone containers and swarm services is that only swarm managers can manage a swarm","title":"What is a swarm?"},{"location":"configurations/swarm/#nodes","text":"A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server. To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.","title":"Nodes"},{"location":"configurations/swarm/#services-and-tasks","text":"A service is the definition of the tasks to execute on the manager or worker nodes. A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm.","title":"Services and tasks"},{"location":"configurations/swarm/#load-balancing","text":"The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry.","title":"Load balancing"},{"location":"configurations/swarm/#docker-machine","text":"export GOOGLE_DISK_SIZE = 10 export GOOGLE_DISK_TYPE = pd-standard export GOOGLE_MACHINE_IMAGE = https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/ubuntu-1804-bionic-v20190320 export GOOGLE_MACHINE_TYPE = g1-small export GOOGLE_NETWORK = default export GOOGLE_PREEMPTIBLE = true export GOOGLE_PROJECT = amanee-ec1eb export GOOGLE_ZONE = us-central1-a export GOOGLE_USERNAME = docker export GOOGLE_USE_EXISTING = true # try to find existing one, otherwise fail # export GOOGLE_USE_INTERNAL_IP= # export GOOGLE_ADDRESS= # export GOOGLE_SUBNETWORK= # export GOOGLE_TAGS= restarting a google cloud with ephemeral ip address cuases docker-machine to fail, running docker-machine regenerate-certs swarm-worker1 fixes the issue but it restarts the docker daemon which might stop running containers. restarting also causes swarm to be broken, we need to leave and re join the nodes.","title":"docker-machine"},{"location":"configurations/swarm/#get-the-token","text":"docker swarm join-token worker Response: docker swarm join --token SWMTKN-1-29na4ee9gsnkt0zn6artrtx2bgb2ajeev96k1bhxdxo7xl5gpi-8o8uf1thlf5c31qrrosuppn7d 10 .128.0.8:2377","title":"get the token"},{"location":"configurations/swarm/#inspect-service","text":"docker service inspect --pretty <SERVICE-ID> To see which nodes are running the service: docker service ps <SERVICE-ID> Run docker ps on the node where the task is running to see details about the container for the task. instances doens't have access to the gcr, this is rather a weired behaviour.","title":"inspect service"},{"location":"configurations/swarm/#docker-swarm_1","text":"","title":"Docker swarm"},{"location":"configurations/swarm/#init","text":"docker swarm init","title":"Init"},{"location":"configurations/swarm/#deploy","text":"docker stack deploy -c docker-compose.yml <stack-name>","title":"Deploy"},{"location":"configurations/swarm/#remove","text":"docker stack rm <stack-name>","title":"Remove"},{"location":"configurations/swarm/#scale","text":"docker service scale <service-name> = <number>","title":"Scale"},{"location":"configurations/swarm/#configure-a-docker-machine-shell-to-the-swarm-manager","text":"So far, you\u2019ve been wrapping Docker commands in docker-machine ssh to talk to the VMs. Another option is to run docker-machine env <machine> to get and run a command that configures your current shell to talk to the Docker daemon on the VM. This method works better for the next step because it allows you to use your local docker-compose.yml file to deploy the app \u201cremotely\u201d without having to copy it","title":"Configure a docker-machine shell to the swarm manager"},{"location":"databases/Elasticsearch/","text":"Getting Started With Elasticsearch Basic Concepts Cluster A cluster is identified by a unique name which by default is elasticsearch . This name is important because a node can only be part of a cluster if the node is set up to join the cluster by its name. Node A node can be configured to join a specific cluster by the cluster name. By default, each node is set up to join a cluster named elasticsearch if there are no other Elasticsearch nodes currently running on your network, starting a single node will by default form a new single-node cluster named elasticsearch Index An index is a collection of documents that have somewhat similar characteristics Type A type used to be a logical category/partition of your index to allow you to store different types of documents in the same index, the whole concept of types will be removed in a later version. See Removal of mapping types Document A document is a basic unit of information that can be indexed, it's expressed in JSON Shards and Replicas An index can be subdivised to multiple pieces called shards Elasticsearch allows you to make one or more copies of your index\u2019s shards into what are called replica By default, each index in Elasticsearch is allocated 5 primary shards and 1 replica which means that if you have at least two nodes in your cluster, your index will have 5 primary shards and another 5 replica shards (1 complete replica) for a total of 10 shards per index. Exploring the Cluster Cluster health GET /_cat/health?v Whenever we ask for the cluster health, we either get green, yellow, or red. Green - everything is good (cluster is fully functional) Yellow - all data is available but some replicas are not yet allocated (cluster is fully functional) Red - some data is not available for whatever reason (cluster is partially functional) Elasticsearch uses unicast network discovery by default to find other nodes on the same machine, it is possible that you could accidentally start up more than one node on your computer and have them all join a single cluster. In this scenario, you may see more than 1 node in the above response. GET /_cat/nodes?v List All Indices GET /_cat/indices?v Create an Index If you have only one node, the indices created would have yellow health state which means some replicas are not already allocated. Index and Query a Document Create The Document PUT /cus t omer/_doc/ 1 ?pre tt y { \"name\" : \"John Doe\" } Response: { \"_index\" : \"customer\" , \"_type\" : \"_doc\" , \"_id\" : \"1\" , \"_version\" : 1 , \"result\" : \"created\" , \"_shards\" : { \"total\" : 2 , \"successful\" : 1 , \"failed\" : 0 }, \"_seq_no\" : 0 , \"_primary_term\" : 1 } Elasticsearch will automatically create the customer index if it didn\u2019t already exist beforehand. Query the Document GET /customer/_doc/1?pretty Respose: { \"_index\" : \"customer\" , \"_type\" : \"_doc\" , \"_id\" : \"1\" , \"_version\" : 1 , \"_seq_no\" : 25 , \"_primary_term\" : 1 , \"found\" : true , \"_source\" : { \"name\" : \"John Doe\" } } Delete an Index DELETE /customer?pretty Modifiying Your Data Reindexig documents using the same id, updates the previously indexed document. Updating Documents Elasticsearch does not actually do in-place updates under the hood. Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot. POST /customer/_doc/1/_update?pretty { \"doc\": { \"name\": \"Jane Doe\" } } Updates can also be performed by using simple scripts. This example uses a script to increment the age by 5: POST /customer/_doc/1/_update?pretty { \"script\" : \"ctx._source.age += 5\" } Deleting Documents DELETE /customer/_doc/2?pretty See the _delete_by_query API to delete all documents matching a specific query Batch Processing Elasticsearch also provides the ability to perform any of the above operations in batches using the _bulk API As a quick example, the following call indexes two documents (ID 1 - John Doe and ID 2 - Jane Doe) in one bulk operation: POST /customer/_doc/_bulk?pretty {\"index\":{\"_id\":\"1\"}} {\"name\": \"John Doe\" } {\"index\":{\"_id\":\"2\"}} {\"name\": \"Jane Doe\" } This example updates the first document (ID of 1) and then deletes the second document (ID of 2) in one bulk operation: POST /customer/_doc/_bulk?pretty {\"update\":{\"_id\":\"1\"}} {\"doc\": { \"name\": \"John Doe becomes Jane Doe\" } } {\"delete\":{\"_id\":\"2\"}} The Bulk API does not fail due to failures in one of the actions. If a single action fails for whatever reason, it will continue to process the remainder of the actions after it. When the bulk API returns, it will provide a status for each action (in the same order it was sent in) so that you can check if a specific action failed or not","title":"Getting Started With Elasticsearch"},{"location":"databases/Elasticsearch/#getting-started-with-elasticsearch","text":"","title":"Getting Started With Elasticsearch"},{"location":"databases/Elasticsearch/#basic-concepts","text":"","title":"Basic Concepts"},{"location":"databases/Elasticsearch/#cluster","text":"A cluster is identified by a unique name which by default is elasticsearch . This name is important because a node can only be part of a cluster if the node is set up to join the cluster by its name.","title":"Cluster"},{"location":"databases/Elasticsearch/#node","text":"A node can be configured to join a specific cluster by the cluster name. By default, each node is set up to join a cluster named elasticsearch if there are no other Elasticsearch nodes currently running on your network, starting a single node will by default form a new single-node cluster named elasticsearch","title":"Node"},{"location":"databases/Elasticsearch/#index","text":"An index is a collection of documents that have somewhat similar characteristics","title":"Index"},{"location":"databases/Elasticsearch/#type","text":"A type used to be a logical category/partition of your index to allow you to store different types of documents in the same index, the whole concept of types will be removed in a later version. See Removal of mapping types","title":"Type"},{"location":"databases/Elasticsearch/#document","text":"A document is a basic unit of information that can be indexed, it's expressed in JSON","title":"Document"},{"location":"databases/Elasticsearch/#shards-and-replicas","text":"An index can be subdivised to multiple pieces called shards Elasticsearch allows you to make one or more copies of your index\u2019s shards into what are called replica By default, each index in Elasticsearch is allocated 5 primary shards and 1 replica which means that if you have at least two nodes in your cluster, your index will have 5 primary shards and another 5 replica shards (1 complete replica) for a total of 10 shards per index.","title":"Shards and Replicas"},{"location":"databases/Elasticsearch/#exploring-the-cluster","text":"","title":"Exploring the Cluster"},{"location":"databases/Elasticsearch/#cluster-health","text":"GET /_cat/health?v Whenever we ask for the cluster health, we either get green, yellow, or red. Green - everything is good (cluster is fully functional) Yellow - all data is available but some replicas are not yet allocated (cluster is fully functional) Red - some data is not available for whatever reason (cluster is partially functional) Elasticsearch uses unicast network discovery by default to find other nodes on the same machine, it is possible that you could accidentally start up more than one node on your computer and have them all join a single cluster. In this scenario, you may see more than 1 node in the above response. GET /_cat/nodes?v","title":"Cluster health"},{"location":"databases/Elasticsearch/#list-all-indices","text":"GET /_cat/indices?v","title":"List All Indices"},{"location":"databases/Elasticsearch/#create-an-index","text":"If you have only one node, the indices created would have yellow health state which means some replicas are not already allocated.","title":"Create an Index"},{"location":"databases/Elasticsearch/#index-and-query-a-document","text":"","title":"Index and Query a Document"},{"location":"databases/Elasticsearch/#create-the-document","text":"PUT /cus t omer/_doc/ 1 ?pre tt y { \"name\" : \"John Doe\" } Response: { \"_index\" : \"customer\" , \"_type\" : \"_doc\" , \"_id\" : \"1\" , \"_version\" : 1 , \"result\" : \"created\" , \"_shards\" : { \"total\" : 2 , \"successful\" : 1 , \"failed\" : 0 }, \"_seq_no\" : 0 , \"_primary_term\" : 1 } Elasticsearch will automatically create the customer index if it didn\u2019t already exist beforehand.","title":"Create The Document"},{"location":"databases/Elasticsearch/#query-the-document","text":"GET /customer/_doc/1?pretty Respose: { \"_index\" : \"customer\" , \"_type\" : \"_doc\" , \"_id\" : \"1\" , \"_version\" : 1 , \"_seq_no\" : 25 , \"_primary_term\" : 1 , \"found\" : true , \"_source\" : { \"name\" : \"John Doe\" } }","title":"Query the Document"},{"location":"databases/Elasticsearch/#delete-an-index","text":"DELETE /customer?pretty","title":"Delete an Index"},{"location":"databases/Elasticsearch/#modifiying-your-data","text":"Reindexig documents using the same id, updates the previously indexed document.","title":"Modifiying Your Data"},{"location":"databases/Elasticsearch/#updating-documents","text":"Elasticsearch does not actually do in-place updates under the hood. Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot. POST /customer/_doc/1/_update?pretty { \"doc\": { \"name\": \"Jane Doe\" } } Updates can also be performed by using simple scripts. This example uses a script to increment the age by 5: POST /customer/_doc/1/_update?pretty { \"script\" : \"ctx._source.age += 5\" }","title":"Updating Documents"},{"location":"databases/Elasticsearch/#deleting-documents","text":"DELETE /customer/_doc/2?pretty See the _delete_by_query API to delete all documents matching a specific query","title":"Deleting Documents"},{"location":"databases/Elasticsearch/#batch-processing","text":"Elasticsearch also provides the ability to perform any of the above operations in batches using the _bulk API As a quick example, the following call indexes two documents (ID 1 - John Doe and ID 2 - Jane Doe) in one bulk operation: POST /customer/_doc/_bulk?pretty {\"index\":{\"_id\":\"1\"}} {\"name\": \"John Doe\" } {\"index\":{\"_id\":\"2\"}} {\"name\": \"Jane Doe\" } This example updates the first document (ID of 1) and then deletes the second document (ID of 2) in one bulk operation: POST /customer/_doc/_bulk?pretty {\"update\":{\"_id\":\"1\"}} {\"doc\": { \"name\": \"John Doe becomes Jane Doe\" } } {\"delete\":{\"_id\":\"2\"}} The Bulk API does not fail due to failures in one of the actions. If a single action fails for whatever reason, it will continue to process the remainder of the actions after it. When the bulk API returns, it will provide a status for each action (in the same order it was sent in) so that you can check if a specific action failed or not","title":"Batch Processing"},{"location":"databases/GraphQL/","text":"GraphQL Source: https://graphql.org/ A query language and a runtime to fulfill those queries Ask for exactly what you want Makes it easier to evolve APIs over time. Because clients asks for exactly what they want, we can add fields without affecting them Get many resources in a single request compared to hitting multiple URLs in RESTful APIs Describe what you want with a type system instead of endpoints in RESTful APIs Powerful dev tools, know what data you can request, highlight potential issues before sending requests, this is most probably because APIs are defined as types GraphQL APIs leverage existing data without enforcing any specific storage engine Example: type Query { hero : Character } type Character { name : String friends : [ Character ] homeWorld : Planet species : Species } type Planet { name : String climate : String } type Species { name : String lifespan : Int origin : Planet } { hero { name friends { name homeWorld { name climate } species { name lifespan origin { name } } } } } Best practice Source: https://graphql.org/learn/best-practices/ Serve everything on a single HTTP route JSON with Gzip No need for versionning Everything is nullable by default Server-side Batching & Caching using a tool like Facebook's DataLoader . Pagination, read more about this in the article on Pagination . Caching Source: https://graphql.org/learn/caching/ Since GraphQL is exposed at the same HTTP URL, clients can't use HTTP caching to easily avoid refetching resources, and for identifying when two resources are the same. A standard solution for this is to identify resources with a Globally Unique ID across all types but this could be problematic if we're migrating existing APIs to GraphQL, this could be solved by exposing the previous APIs ID in a separate field, if not the client can also derive the globally unique identifier. Oftentimes, this would be as simple as combining the type of the object (queried with __typename ) with some type-unique identifier. Thinking in graphs Source: https://graphql.org/learn/thinking-in-graphs/ [ ] Get back to the page and document it Queries and Mutations Source: https://graphql.org/learn/queries/ Fields GraphQL is all about asking for specific fields on objects { hero { name # Queries can have comments! friends { name } } } { \"data\" : { \"hero\" : { \"name\" : \"R2-D2\" , \"friends\" : [ { \"name\" : \"Luke Skywalker\" }, { \"name\" : \"Han Solo\" }, { \"name\" : \"Leia Organa\" } ] } } } Arguments Arguments can be of many different types, servers can also define custom types. Read more about the GraphQL type system here. { human(id: \"1000\") { name height(unit: FOOT) } } { \"data\" : { \"human\" : { \"name\" : \"Luke Skywalker\" , \"height\" : 5.6430448 } } } Aliases { empireHero: hero(episode: EMPIRE) { name } jediHero: hero(episode: JEDI) { name } } \"data\" : { \"empireHero\" : { \"name\" : \"Luke Skywalker\" }, \"jediHero\" : { \"name\" : \"R2-D2\" } } } Fragments { leftComparison: hero(episode: EMPIRE) { ...comparisonFields } rightComparison: hero(episode: JEDI) { ...comparisonFields } } fragment comparisonFields on Character { name appearsIn friends { name } } { \"data\" : { \"leftComparison\" : { \"name\" : \"Luke Skywalker\" , \"appearsIn\" : [ \"NEWHOPE\" , \"EMPIRE\" , \"JEDI\" ], \"friends\" : [ { \"name\" : \"Han Solo\" }, { \"name\" : \"Leia Organa\" }, { \"name\" : \"C-3PO\" }, { \"name\" : \"R2-D2\" } ] }, \"rightComparison\" : { \"name\" : \"R2-D2\" , \"appearsIn\" : [ \"NEWHOPE\" , \"EMPIRE\" , \"JEDI\" ], \"friends\" : [ { \"name\" : \"Luke Skywalker\" }, { \"name\" : \"Han Solo\" }, { \"name\" : \"Leia Organa\" } ] } } } Using variables inside fragments query HeroComparison($first: Int = 3) { leftComparison: hero(episode: EMPIRE) { ...comparisonFields } rightComparison: hero(episode: JEDI) { ...comparisonFields } } fragment comparisonFields on Character { name friendsConnection(first: $first) { totalCount edges { node { name } } } } { \"data\" : { \"leftComparison\" : { \"name\" : \"Luke Skywalker\" , \"friendsConnection\" : { \"totalCount\" : 4 , \"edges\" : [ { \"node\" : { \"name\" : \"Han Solo\" } }, { \"node\" : { \"name\" : \"Leia Organa\" } }, { \"node\" : { \"name\" : \"C-3PO\" } } ] } }, \"rightComparison\" : { \"name\" : \"R2-D2\" , \"friendsConnection\" : { \"totalCount\" : 3 , \"edges\" : [ { \"node\" : { \"name\" : \"Luke Skywalker\" } }, { \"node\" : { \"name\" : \"Han Solo\" } }, { \"node\" : { \"name\" : \"Leia Organa\" } } ] } } } } Operation name operation_type operation_name ( variable_definition ) { operation_content } The operation type is either query , mutation , or subscription and describes what type of operation you're intending to do. The operation type is required unless you're using the query shorthand syntax, in which case you can't supply a name or variable definitions for your operation. Variables Variables avoids clients to dynamically manipulate the query string at runtime and serialize it into a GraphQL format. When we start working with variables, we need to do three things: 1. Replace the static value in the query with $variableName 2. Declare $variableName: variableType = [defaultValue] as one of the variables accepted by the query 3. Pass variableName: value in the separate, transport-specific (usually JSON) variables dictionary query HeroNameAndFriends($episode: Episode) { hero(episode: $episode) { name friends { name } } } # variables { \"episode\" : \"JEDI\" } { \"data\" : { \"hero\" : { \"name\" : \"R2-D2\" , \"friends\" : [ { \"name\" : \"Luke Skywalker\" }, { \"name\" : \"Han Solo\" }, { \"name\" : \"Leia Organa\" } ] } } } Directives query Hero ( $ episode : Episode , $ withFriends : Boolean ! ) { hero ( episode : $ episode ) { name friends @include ( if : $ withFriends ) { name } } } # variables { \"episode\" : \"JEDI\" , \"withFriends\" : true } { \"data\" : { \"hero\" : { \"name\" : \"R2-D2\" , \"friends\" : [ { \"name\" : \"Luke Skywalker\" }, { \"name\" : \"Han Solo\" }, { \"name\" : \"Leia Organa\" } ] } } } The core GraphQL specification includes exactly two directives, which must be supported by any spec-compliant GraphQL server implementation: - @include(if: Boolean) Only include this field in the result if the argument is true . - @skip(if: Boolean) Skip this field if the argument is true . Directives can be useful to get out of situations where you otherwise would need to do string manipulation to add and remove fields in your query. Servers are free to add experimental features for custom directives Mutations an operation type that mean to modify server-side data, query operations shouldn't be used to do that, think GET vs POST in REST APIs. we can mutate and query the new value of the field with one request. mutation CreateReviewForEpisode($ep: Episode!, $review: ReviewInput!) { createReview(episode: $ep, review: $review) { stars commentary } } # variables { \"ep\" : \"JEDI\" , \"review\" : { \"stars\" : 5 , \"commentary\" : \"This is a great movie!\" } } { \"data\" : { \"createReview\" : { \"stars\" : 5 , \"commentary\" : \"This is a great movie!\" } } } Multiple fields in mutations While query fields are executed in parallel, mutation fields run in series, one after the other. This means that if we send two incrementCredits mutations in one request, the first is guaranteed to finish before the second begins, ensuring that we don't end up with a race condition with ourselves. Inline Fragments Inline Fragments are useful when dealing with union types. Learn about them in the schema guide. query HeroForEpisode($ep: Episode!) { hero(episode: $ep) { name ... on Droid { primaryFunction } ... on Human { height } } } # variables { \"ep\" : \"JEDI\" } { \"data\" : { \"hero\" : { \"name\" : \"R2-D2\" , \"primaryFunction\" : \"Astromech\" } } } In this query, the hero field returns the type Character , which might be either a Human or a Droid depending on the episode argument. In the direct selection, you can only ask for fields that exist on the Character interface, such as name . Named fragments can also be used in the same way, since a named fragment always has a type attached. Meta fields { search(text: \"an\") { __typename ... on Human { name } ... on Droid { name } ... on Starship { name } } } { \"data\" : { \"search\" : [ { \"__typename\" : \"Human\" , \"name\" : \"Han Solo\" }, { \"__typename\" : \"Human\" , \"name\" : \"Leia Organa\" }, { \"__typename\" : \"Starship\" , \"name\" : \"TIE Advanced x1\" } ] } } In the above query, search returns a union type that can be one of three options. It would be impossible to tell apart the different types from the client without the __typename field. GraphQL services provide a few meta fields, the rest of which are used to expose the Introspection system. Execution Source: https://graphql.org/learn/execution/ You can think of each field in a GraphQL query as a function or method of the previous type which returns the next type. In fact, this is exactly how GraphQL works. Each field on each type is backed by a function called the resolver which is provided by the GraphQL server developer. When a field is executed, the corresponding resolver is called to produce the next value. If a field produces a scalar value like a string or number, then the execution completes. Let's take this example type Query { human ( id : ID ! ) : Human } type Human { name : String appearsIn : [ Episode ] starships : [ Starship ] } enum Episode { NEWHOPE EMPIRE JEDI } type Starship { name : String } { human(id: 1002) { name appearsIn starships { name } } } { \"data\" : { \"human\" : { \"name\" : \"Han Solo\" , \"appearsIn\" : [ \"NEWHOPE\" , \"EMPIRE\" , \"JEDI\" ], \"starships\" : [ { \"name\" : \"Millenium Falcon\" }, { \"name\" : \"Imperial shuttle\" } ] } } } Root fields & resolvers At the top level of every GraphQL server is a type that represents all of the possible entry points into the GraphQL API, it's often called the Root type or the Query type. A resolver function accesses the database and returns the the field value, it receives four arguments: - obj The previous object, which for a field on the root Query type is often not used. - args The arguments provided to the field in the GraphQL query. - context A value which is provided to every resolver and holds important contextual information like the currently logged in user, or access to a database. - info A value which holds field-specific information relevant to the current query as well as the schema details, also refer to type GraphQLResolveInfo for more details . Resolvers can be asynchrnous, GraphQL will wait for Promises, Futures, and Tasks to complete before continuing and will do so with optimal concurrency. Query : { human ( obj , args , context , info ) { return context . db . loadHumanByID ( args . id ). then ( userData => new Human ( userData ) ) } } Scalar coercion Human : { appearsIn ( obj ) { return obj . appearsIn // returns [ 4, 5, 6 ] } } Notice that our type system claims appearsIn will return Enum values with known values, the type system knows what to expect and will convert the values returned by a resolver function into something that upholds the API contract. In this case, there may be an Enum defined on our server which uses numbers like 4 , 5 , and 6 internally, but represents them as Enum values in the GraphQL type system. Introspection It's often useful to ask a GraphQL schema for information about what queries it supports. GraphQL allows us to do so using introspection root fields like __schema , __type and __typename Examples: 1. List all available types { __schema { types { name } } } { \"data\" : { \"__schema\" : { \"types\" : [ { \"name\" : \"Query\" }, { \"name\" : \"String\" }, { \"name\" : \"ID\" }, { \"name\" : \"Mutation\" }, { \"name\" : \"Episode\" }, { \"name\" : \"Character\" }, { \"name\" : \"Int\" }, { \"name\" : \"LengthUnit\" }, { \"name\" : \"Human\" }, { \"name\" : \"Float\" }, { \"name\" : \"Droid\" }, { \"name\" : \"FriendsConnection\" }, { \"name\" : \"FriendsEdge\" }, { \"name\" : \"PageInfo\" }, { \"name\" : \"Boolean\" }, { \"name\" : \"Review\" }, { \"name\" : \"ReviewInput\" }, { \"name\" : \"Starship\" }, { \"name\" : \"SearchResult\" }, { \"name\" : \"__Schema\" }, { \"name\" : \"__Type\" }, { \"name\" : \"__TypeKind\" }, { \"name\" : \"__Field\" }, { \"name\" : \"__InputValue\" }, { \"name\" : \"__EnumValue\" }, { \"name\" : \"__Directive\" }, { \"name\" : \"__DirectiveLocation\" } ] } } } Check the fields of a given type { __type(name: \"Droid\") { name fields { name type { name kind ofType { name kind } } } } } { \"data\" : { \"__type\" : { \"name\" : \"Droid\" , \"fields\" : [ { \"name\" : \"id\" , \"type\" : { \"name\" : null , \"kind\" : \"NON_NULL\" , \"ofType\" : { \"name\" : \"ID\" , \"kind\" : \"SCALAR\" } } }, { \"name\" : \"name\" , \"type\" : { \"name\" : null , \"kind\" : \"NON_NULL\" , \"ofType\" : { \"name\" : \"String\" , \"kind\" : \"SCALAR\" } } }, { \"name\" : \"friends\" , \"type\" : { \"name\" : null , \"kind\" : \"LIST\" , \"ofType\" : { \"name\" : \"Character\" , \"kind\" : \"INTERFACE\" } } }, { \"name\" : \"friendsConnection\" , \"type\" : { \"name\" : null , \"kind\" : \"NON_NULL\" , \"ofType\" : { \"name\" : \"FriendsConnection\" , \"kind\" : \"OBJECT\" } } }, { \"name\" : \"appearsIn\" , \"type\" : { \"name\" : null , \"kind\" : \"NON_NULL\" , \"ofType\" : { \"name\" : null , \"kind\" : \"LIST\" } } }, { \"name\" : \"primaryFunction\" , \"type\" : { \"name\" : \"String\" , \"kind\" : \"SCALAR\" , \"ofType\" : null } } ] } } }","title":"GraphQL"},{"location":"databases/GraphQL/#graphql","text":"Source: https://graphql.org/ A query language and a runtime to fulfill those queries Ask for exactly what you want Makes it easier to evolve APIs over time. Because clients asks for exactly what they want, we can add fields without affecting them Get many resources in a single request compared to hitting multiple URLs in RESTful APIs Describe what you want with a type system instead of endpoints in RESTful APIs Powerful dev tools, know what data you can request, highlight potential issues before sending requests, this is most probably because APIs are defined as types GraphQL APIs leverage existing data without enforcing any specific storage engine Example: type Query { hero : Character } type Character { name : String friends : [ Character ] homeWorld : Planet species : Species } type Planet { name : String climate : String } type Species { name : String lifespan : Int origin : Planet } { hero { name friends { name homeWorld { name climate } species { name lifespan origin { name } } } } }","title":"GraphQL"},{"location":"databases/GraphQL/#best-practice","text":"Source: https://graphql.org/learn/best-practices/ Serve everything on a single HTTP route JSON with Gzip No need for versionning Everything is nullable by default Server-side Batching & Caching using a tool like Facebook's DataLoader . Pagination, read more about this in the article on Pagination .","title":"Best practice"},{"location":"databases/GraphQL/#caching","text":"Source: https://graphql.org/learn/caching/ Since GraphQL is exposed at the same HTTP URL, clients can't use HTTP caching to easily avoid refetching resources, and for identifying when two resources are the same. A standard solution for this is to identify resources with a Globally Unique ID across all types but this could be problematic if we're migrating existing APIs to GraphQL, this could be solved by exposing the previous APIs ID in a separate field, if not the client can also derive the globally unique identifier. Oftentimes, this would be as simple as combining the type of the object (queried with __typename ) with some type-unique identifier.","title":"Caching"},{"location":"databases/GraphQL/#thinking-in-graphs","text":"Source: https://graphql.org/learn/thinking-in-graphs/ [ ] Get back to the page and document it","title":"Thinking in graphs"},{"location":"databases/GraphQL/#queries-and-mutations","text":"Source: https://graphql.org/learn/queries/","title":"Queries and Mutations"},{"location":"databases/GraphQL/#fields","text":"GraphQL is all about asking for specific fields on objects { hero { name # Queries can have comments! friends { name } } } { \"data\" : { \"hero\" : { \"name\" : \"R2-D2\" , \"friends\" : [ { \"name\" : \"Luke Skywalker\" }, { \"name\" : \"Han Solo\" }, { \"name\" : \"Leia Organa\" } ] } } }","title":"Fields"},{"location":"databases/GraphQL/#arguments","text":"Arguments can be of many different types, servers can also define custom types. Read more about the GraphQL type system here. { human(id: \"1000\") { name height(unit: FOOT) } } { \"data\" : { \"human\" : { \"name\" : \"Luke Skywalker\" , \"height\" : 5.6430448 } } }","title":"Arguments"},{"location":"databases/GraphQL/#aliases","text":"{ empireHero: hero(episode: EMPIRE) { name } jediHero: hero(episode: JEDI) { name } } \"data\" : { \"empireHero\" : { \"name\" : \"Luke Skywalker\" }, \"jediHero\" : { \"name\" : \"R2-D2\" } } }","title":"Aliases"},{"location":"databases/GraphQL/#fragments","text":"{ leftComparison: hero(episode: EMPIRE) { ...comparisonFields } rightComparison: hero(episode: JEDI) { ...comparisonFields } } fragment comparisonFields on Character { name appearsIn friends { name } } { \"data\" : { \"leftComparison\" : { \"name\" : \"Luke Skywalker\" , \"appearsIn\" : [ \"NEWHOPE\" , \"EMPIRE\" , \"JEDI\" ], \"friends\" : [ { \"name\" : \"Han Solo\" }, { \"name\" : \"Leia Organa\" }, { \"name\" : \"C-3PO\" }, { \"name\" : \"R2-D2\" } ] }, \"rightComparison\" : { \"name\" : \"R2-D2\" , \"appearsIn\" : [ \"NEWHOPE\" , \"EMPIRE\" , \"JEDI\" ], \"friends\" : [ { \"name\" : \"Luke Skywalker\" }, { \"name\" : \"Han Solo\" }, { \"name\" : \"Leia Organa\" } ] } } }","title":"Fragments"},{"location":"databases/GraphQL/#using-variables-inside-fragments","text":"query HeroComparison($first: Int = 3) { leftComparison: hero(episode: EMPIRE) { ...comparisonFields } rightComparison: hero(episode: JEDI) { ...comparisonFields } } fragment comparisonFields on Character { name friendsConnection(first: $first) { totalCount edges { node { name } } } } { \"data\" : { \"leftComparison\" : { \"name\" : \"Luke Skywalker\" , \"friendsConnection\" : { \"totalCount\" : 4 , \"edges\" : [ { \"node\" : { \"name\" : \"Han Solo\" } }, { \"node\" : { \"name\" : \"Leia Organa\" } }, { \"node\" : { \"name\" : \"C-3PO\" } } ] } }, \"rightComparison\" : { \"name\" : \"R2-D2\" , \"friendsConnection\" : { \"totalCount\" : 3 , \"edges\" : [ { \"node\" : { \"name\" : \"Luke Skywalker\" } }, { \"node\" : { \"name\" : \"Han Solo\" } }, { \"node\" : { \"name\" : \"Leia Organa\" } } ] } } } }","title":"Using variables inside fragments"},{"location":"databases/GraphQL/#operation-name","text":"operation_type operation_name ( variable_definition ) { operation_content } The operation type is either query , mutation , or subscription and describes what type of operation you're intending to do. The operation type is required unless you're using the query shorthand syntax, in which case you can't supply a name or variable definitions for your operation.","title":"Operation name"},{"location":"databases/GraphQL/#variables","text":"Variables avoids clients to dynamically manipulate the query string at runtime and serialize it into a GraphQL format. When we start working with variables, we need to do three things: 1. Replace the static value in the query with $variableName 2. Declare $variableName: variableType = [defaultValue] as one of the variables accepted by the query 3. Pass variableName: value in the separate, transport-specific (usually JSON) variables dictionary query HeroNameAndFriends($episode: Episode) { hero(episode: $episode) { name friends { name } } } # variables { \"episode\" : \"JEDI\" } { \"data\" : { \"hero\" : { \"name\" : \"R2-D2\" , \"friends\" : [ { \"name\" : \"Luke Skywalker\" }, { \"name\" : \"Han Solo\" }, { \"name\" : \"Leia Organa\" } ] } } }","title":"Variables"},{"location":"databases/GraphQL/#directives","text":"query Hero ( $ episode : Episode , $ withFriends : Boolean ! ) { hero ( episode : $ episode ) { name friends @include ( if : $ withFriends ) { name } } } # variables { \"episode\" : \"JEDI\" , \"withFriends\" : true } { \"data\" : { \"hero\" : { \"name\" : \"R2-D2\" , \"friends\" : [ { \"name\" : \"Luke Skywalker\" }, { \"name\" : \"Han Solo\" }, { \"name\" : \"Leia Organa\" } ] } } } The core GraphQL specification includes exactly two directives, which must be supported by any spec-compliant GraphQL server implementation: - @include(if: Boolean) Only include this field in the result if the argument is true . - @skip(if: Boolean) Skip this field if the argument is true . Directives can be useful to get out of situations where you otherwise would need to do string manipulation to add and remove fields in your query. Servers are free to add experimental features for custom directives","title":"Directives"},{"location":"databases/GraphQL/#mutations","text":"an operation type that mean to modify server-side data, query operations shouldn't be used to do that, think GET vs POST in REST APIs. we can mutate and query the new value of the field with one request. mutation CreateReviewForEpisode($ep: Episode!, $review: ReviewInput!) { createReview(episode: $ep, review: $review) { stars commentary } } # variables { \"ep\" : \"JEDI\" , \"review\" : { \"stars\" : 5 , \"commentary\" : \"This is a great movie!\" } } { \"data\" : { \"createReview\" : { \"stars\" : 5 , \"commentary\" : \"This is a great movie!\" } } }","title":"Mutations"},{"location":"databases/GraphQL/#multiple-fields-in-mutations","text":"While query fields are executed in parallel, mutation fields run in series, one after the other. This means that if we send two incrementCredits mutations in one request, the first is guaranteed to finish before the second begins, ensuring that we don't end up with a race condition with ourselves.","title":"Multiple fields in mutations"},{"location":"databases/GraphQL/#inline-fragments","text":"Inline Fragments are useful when dealing with union types. Learn about them in the schema guide. query HeroForEpisode($ep: Episode!) { hero(episode: $ep) { name ... on Droid { primaryFunction } ... on Human { height } } } # variables { \"ep\" : \"JEDI\" } { \"data\" : { \"hero\" : { \"name\" : \"R2-D2\" , \"primaryFunction\" : \"Astromech\" } } } In this query, the hero field returns the type Character , which might be either a Human or a Droid depending on the episode argument. In the direct selection, you can only ask for fields that exist on the Character interface, such as name . Named fragments can also be used in the same way, since a named fragment always has a type attached.","title":"Inline Fragments"},{"location":"databases/GraphQL/#meta-fields","text":"{ search(text: \"an\") { __typename ... on Human { name } ... on Droid { name } ... on Starship { name } } } { \"data\" : { \"search\" : [ { \"__typename\" : \"Human\" , \"name\" : \"Han Solo\" }, { \"__typename\" : \"Human\" , \"name\" : \"Leia Organa\" }, { \"__typename\" : \"Starship\" , \"name\" : \"TIE Advanced x1\" } ] } } In the above query, search returns a union type that can be one of three options. It would be impossible to tell apart the different types from the client without the __typename field. GraphQL services provide a few meta fields, the rest of which are used to expose the Introspection system.","title":"Meta fields"},{"location":"databases/GraphQL/#execution","text":"Source: https://graphql.org/learn/execution/ You can think of each field in a GraphQL query as a function or method of the previous type which returns the next type. In fact, this is exactly how GraphQL works. Each field on each type is backed by a function called the resolver which is provided by the GraphQL server developer. When a field is executed, the corresponding resolver is called to produce the next value. If a field produces a scalar value like a string or number, then the execution completes. Let's take this example type Query { human ( id : ID ! ) : Human } type Human { name : String appearsIn : [ Episode ] starships : [ Starship ] } enum Episode { NEWHOPE EMPIRE JEDI } type Starship { name : String } { human(id: 1002) { name appearsIn starships { name } } } { \"data\" : { \"human\" : { \"name\" : \"Han Solo\" , \"appearsIn\" : [ \"NEWHOPE\" , \"EMPIRE\" , \"JEDI\" ], \"starships\" : [ { \"name\" : \"Millenium Falcon\" }, { \"name\" : \"Imperial shuttle\" } ] } } }","title":"Execution"},{"location":"databases/GraphQL/#root-fields-resolvers","text":"At the top level of every GraphQL server is a type that represents all of the possible entry points into the GraphQL API, it's often called the Root type or the Query type. A resolver function accesses the database and returns the the field value, it receives four arguments: - obj The previous object, which for a field on the root Query type is often not used. - args The arguments provided to the field in the GraphQL query. - context A value which is provided to every resolver and holds important contextual information like the currently logged in user, or access to a database. - info A value which holds field-specific information relevant to the current query as well as the schema details, also refer to type GraphQLResolveInfo for more details . Resolvers can be asynchrnous, GraphQL will wait for Promises, Futures, and Tasks to complete before continuing and will do so with optimal concurrency. Query : { human ( obj , args , context , info ) { return context . db . loadHumanByID ( args . id ). then ( userData => new Human ( userData ) ) } }","title":"Root fields &amp; resolvers"},{"location":"databases/GraphQL/#scalar-coercion","text":"Human : { appearsIn ( obj ) { return obj . appearsIn // returns [ 4, 5, 6 ] } } Notice that our type system claims appearsIn will return Enum values with known values, the type system knows what to expect and will convert the values returned by a resolver function into something that upholds the API contract. In this case, there may be an Enum defined on our server which uses numbers like 4 , 5 , and 6 internally, but represents them as Enum values in the GraphQL type system.","title":"Scalar coercion"},{"location":"databases/GraphQL/#introspection","text":"It's often useful to ask a GraphQL schema for information about what queries it supports. GraphQL allows us to do so using introspection root fields like __schema , __type and __typename Examples: 1. List all available types { __schema { types { name } } } { \"data\" : { \"__schema\" : { \"types\" : [ { \"name\" : \"Query\" }, { \"name\" : \"String\" }, { \"name\" : \"ID\" }, { \"name\" : \"Mutation\" }, { \"name\" : \"Episode\" }, { \"name\" : \"Character\" }, { \"name\" : \"Int\" }, { \"name\" : \"LengthUnit\" }, { \"name\" : \"Human\" }, { \"name\" : \"Float\" }, { \"name\" : \"Droid\" }, { \"name\" : \"FriendsConnection\" }, { \"name\" : \"FriendsEdge\" }, { \"name\" : \"PageInfo\" }, { \"name\" : \"Boolean\" }, { \"name\" : \"Review\" }, { \"name\" : \"ReviewInput\" }, { \"name\" : \"Starship\" }, { \"name\" : \"SearchResult\" }, { \"name\" : \"__Schema\" }, { \"name\" : \"__Type\" }, { \"name\" : \"__TypeKind\" }, { \"name\" : \"__Field\" }, { \"name\" : \"__InputValue\" }, { \"name\" : \"__EnumValue\" }, { \"name\" : \"__Directive\" }, { \"name\" : \"__DirectiveLocation\" } ] } } } Check the fields of a given type { __type(name: \"Droid\") { name fields { name type { name kind ofType { name kind } } } } } { \"data\" : { \"__type\" : { \"name\" : \"Droid\" , \"fields\" : [ { \"name\" : \"id\" , \"type\" : { \"name\" : null , \"kind\" : \"NON_NULL\" , \"ofType\" : { \"name\" : \"ID\" , \"kind\" : \"SCALAR\" } } }, { \"name\" : \"name\" , \"type\" : { \"name\" : null , \"kind\" : \"NON_NULL\" , \"ofType\" : { \"name\" : \"String\" , \"kind\" : \"SCALAR\" } } }, { \"name\" : \"friends\" , \"type\" : { \"name\" : null , \"kind\" : \"LIST\" , \"ofType\" : { \"name\" : \"Character\" , \"kind\" : \"INTERFACE\" } } }, { \"name\" : \"friendsConnection\" , \"type\" : { \"name\" : null , \"kind\" : \"NON_NULL\" , \"ofType\" : { \"name\" : \"FriendsConnection\" , \"kind\" : \"OBJECT\" } } }, { \"name\" : \"appearsIn\" , \"type\" : { \"name\" : null , \"kind\" : \"NON_NULL\" , \"ofType\" : { \"name\" : null , \"kind\" : \"LIST\" } } }, { \"name\" : \"primaryFunction\" , \"type\" : { \"name\" : \"String\" , \"kind\" : \"SCALAR\" , \"ofType\" : null } } ] } } }","title":"Introspection"},{"location":"databases/Influxdb/","text":"Concepts Measurement The measurement acts as a container for tags, fields, and the time column, and the measurement name is the description of the data that are stored in the associated fields. Measurement names are strings, and, for any SQL users out there, a measurement is conceptually similar to a table. A single measurement can belong to different retention policies. A retention policy describes how long InfluxDB keeps data (DURATION) and how many copies of those data are stored in the cluster (REPLICATION). If you\u2019re interested in reading more about retention policies, check out Database Management. Attention Replication factors do not serve a purpose with single node instances. autogen retention policy. InfluxDB automatically creates that retention policy; it has an infinite duration and a replication factor set to one. Fields Fields are actual data and are not indexed, so queries that uses fields as filters scan over all the data and thus are not performant Fields are required Tags Tags are made up of tag keys and tag values. Both tag keys and tag values are stored as strings and record metadata Tags are not required Tags are indexed. This means that queries on tags are faster and that tags are ideal for storing commonly-queried metadata. Tagsets Tagset is the different combinations of all the tag key-value pairs. Series a series is the collection of data that share a retention policy, measurement, and tag set (). Point a point is the field set in the same series with the same timestamp Database An InfluxDB database is similar to traditional relational databases and serves as a logical container for users, retention policies, continuous queries, and, of course, your time series data Getting started with InfluxDB OSS Moving average with time aggregation file:///home/mohammedi/websites/influxdb/docs.influxdata.com/influxdb/v1.7/query_language/functions/index.html#examples-of-advanced-syntax-16 Querying data Select statemants should contain at least one field. GROUP BY time() queries don't return timestamps that occur after now(). Time pretty: precision rfc3339 Moving average with N >= number of messages, returns nothing Queries select moving_average(mean(hr),3) from hr_bucketed where time >= '2019-02-10T16:53:45Z' and time <= '2019-02-17T16:53:45Z' group by bucket,time(1d)","title":"Concepts"},{"location":"databases/Influxdb/#concepts","text":"","title":"Concepts"},{"location":"databases/Influxdb/#measurement","text":"The measurement acts as a container for tags, fields, and the time column, and the measurement name is the description of the data that are stored in the associated fields. Measurement names are strings, and, for any SQL users out there, a measurement is conceptually similar to a table. A single measurement can belong to different retention policies. A retention policy describes how long InfluxDB keeps data (DURATION) and how many copies of those data are stored in the cluster (REPLICATION). If you\u2019re interested in reading more about retention policies, check out Database Management.","title":"Measurement"},{"location":"databases/Influxdb/#attention","text":"Replication factors do not serve a purpose with single node instances. autogen retention policy. InfluxDB automatically creates that retention policy; it has an infinite duration and a replication factor set to one.","title":"Attention"},{"location":"databases/Influxdb/#fields","text":"Fields are actual data and are not indexed, so queries that uses fields as filters scan over all the data and thus are not performant Fields are required","title":"Fields"},{"location":"databases/Influxdb/#tags","text":"Tags are made up of tag keys and tag values. Both tag keys and tag values are stored as strings and record metadata Tags are not required Tags are indexed. This means that queries on tags are faster and that tags are ideal for storing commonly-queried metadata.","title":"Tags"},{"location":"databases/Influxdb/#tagsets","text":"Tagset is the different combinations of all the tag key-value pairs.","title":"Tagsets"},{"location":"databases/Influxdb/#series","text":"a series is the collection of data that share a retention policy, measurement, and tag set ().","title":"Series"},{"location":"databases/Influxdb/#point","text":"a point is the field set in the same series with the same timestamp","title":"Point"},{"location":"databases/Influxdb/#database","text":"An InfluxDB database is similar to traditional relational databases and serves as a logical container for users, retention policies, continuous queries, and, of course, your time series data","title":"Database"},{"location":"databases/Influxdb/#getting-started-with-influxdb-oss","text":"","title":"Getting started with InfluxDB OSS"},{"location":"databases/Influxdb/#moving-average-with-time-aggregation","text":"file:///home/mohammedi/websites/influxdb/docs.influxdata.com/influxdb/v1.7/query_language/functions/index.html#examples-of-advanced-syntax-16","title":"Moving average with time aggregation"},{"location":"databases/Influxdb/#querying-data","text":"Select statemants should contain at least one field. GROUP BY time() queries don't return timestamps that occur after now(). Time pretty: precision rfc3339 Moving average with N >= number of messages, returns nothing","title":"Querying data"},{"location":"databases/Influxdb/#queries","text":"select moving_average(mean(hr),3) from hr_bucketed where time >= '2019-02-10T16:53:45Z' and time <= '2019-02-17T16:53:45Z' group by bucket,time(1d)","title":"Queries"},{"location":"databases/MongoDB/","text":"MongoDB Schema Design Best Practices Source: https://www.mongodb.com/developer/products/mongodb/mongodb-schema-design-best-practices/ Designing a proper MongoDB schema is the most critical part of deploying a scalable, fast, and affordable database Don't think of it as a legacy relational design, document databases have a rich vocabulary that is capable of expressing data relationships in more nuanced ways than SQL. There are many things to consider when picking a schema. Is your app read or write heavy? What data is frequently accessed together? What are your performance considerations? How will your data set grow and scale? Schema Design Approaches \u2013 Relational vs. MongoDB We normalize relational schemas using 3rd normal form which aims to split data in tables, so we don't duplicate it. We don't do that in mongo, there are no rules or format process to desing the database, we have to take into consideration three points though: storing the data, providing a good query performance and requiring a reasonalbe amount of hardware Example: ![[mongodb-example.png]] { \"first_name\" : \"Paul\" , \"surname\" : \"Miller\" , \"cell\" : \"447557505611\" , \"city\" : \"London\" , \"location\" : [ 45.123 , 47.232 ], \"profession\" : [ \"banking\" , \"finance\" , \"trader\" ], \"cars\" : [ { \"model\" : \"Bentley\" , \"year\" : 1973 }, { \"model\" : \"Rolls Royce\" , \"year\" : 1965 } ] } Embedding vs. Referencing Embedding allows getting all the data in a single query, referencing means we have to run at least two queries or following $lookup operator Embedding makes updates Atomic by default, however relying too much on that is anti-pattern (I don't know why !) Embedding could results in large documents and we may hit the 16mb limit sooner, also large documents means more overhead (poor query performance) if most fields aren't relevant Referecing reduces the amount of data duplication, we shouldn't avoid that if it results in a better schema though Rules Rule 1 : Favor embedding unless there is a compelling reason not to. Rule 2 : Needing to access an object on its own is a compelling reason not to embed it. Rule 3 : Avoid joins/lookups if possible, but don't be afraid if they can provide a better schema design. Rule 4 : Arrays should not grow without bound. If there are more than a couple of hundred documents on the \"many\" side, don't embed them; if there are more than a few thousand documents on the \"many\" side, don't use an array of ObjectID references. High-cardinality arrays are a compelling reason not to embed. Rule 5 : As always, with MongoDB, how you model your data depends \u2013 entirely \u2013 on your particular application's data access patterns. You want to structure your data to match the ways that your application queries and updates it. Type of Relationships One-to-One - Prefer key value pairs within the document One-to-Few - Prefer embedding One-to-Many - Prefer embedding One-to-Squillions - Prefer Referencing Many-to-Many - Prefer Referencing","title":"MongoDB Schema Design Best Practices"},{"location":"databases/MongoDB/#mongodb-schema-design-best-practices","text":"Source: https://www.mongodb.com/developer/products/mongodb/mongodb-schema-design-best-practices/ Designing a proper MongoDB schema is the most critical part of deploying a scalable, fast, and affordable database Don't think of it as a legacy relational design, document databases have a rich vocabulary that is capable of expressing data relationships in more nuanced ways than SQL. There are many things to consider when picking a schema. Is your app read or write heavy? What data is frequently accessed together? What are your performance considerations? How will your data set grow and scale?","title":"MongoDB Schema Design Best Practices"},{"location":"databases/MongoDB/#schema-design-approaches-relational-vs-mongodb","text":"We normalize relational schemas using 3rd normal form which aims to split data in tables, so we don't duplicate it. We don't do that in mongo, there are no rules or format process to desing the database, we have to take into consideration three points though: storing the data, providing a good query performance and requiring a reasonalbe amount of hardware Example: ![[mongodb-example.png]] { \"first_name\" : \"Paul\" , \"surname\" : \"Miller\" , \"cell\" : \"447557505611\" , \"city\" : \"London\" , \"location\" : [ 45.123 , 47.232 ], \"profession\" : [ \"banking\" , \"finance\" , \"trader\" ], \"cars\" : [ { \"model\" : \"Bentley\" , \"year\" : 1973 }, { \"model\" : \"Rolls Royce\" , \"year\" : 1965 } ] }","title":"Schema Design Approaches \u2013 Relational vs.\u00a0MongoDB"},{"location":"databases/MongoDB/#embedding-vs-referencing","text":"Embedding allows getting all the data in a single query, referencing means we have to run at least two queries or following $lookup operator Embedding makes updates Atomic by default, however relying too much on that is anti-pattern (I don't know why !) Embedding could results in large documents and we may hit the 16mb limit sooner, also large documents means more overhead (poor query performance) if most fields aren't relevant Referecing reduces the amount of data duplication, we shouldn't avoid that if it results in a better schema though","title":"Embedding vs.\u00a0Referencing"},{"location":"databases/MongoDB/#rules","text":"Rule 1 : Favor embedding unless there is a compelling reason not to. Rule 2 : Needing to access an object on its own is a compelling reason not to embed it. Rule 3 : Avoid joins/lookups if possible, but don't be afraid if they can provide a better schema design. Rule 4 : Arrays should not grow without bound. If there are more than a couple of hundred documents on the \"many\" side, don't embed them; if there are more than a few thousand documents on the \"many\" side, don't use an array of ObjectID references. High-cardinality arrays are a compelling reason not to embed. Rule 5 : As always, with MongoDB, how you model your data depends \u2013 entirely \u2013 on your particular application's data access patterns. You want to structure your data to match the ways that your application queries and updates it.","title":"Rules"},{"location":"databases/MongoDB/#type-of-relationships","text":"One-to-One - Prefer key value pairs within the document One-to-Few - Prefer embedding One-to-Many - Prefer embedding One-to-Squillions - Prefer Referencing Many-to-Many - Prefer Referencing","title":"Type of Relationships"},{"location":"databases/Redis/","text":"Introduction to Redis Source: https://redis.io/topics/introduction Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache, and message broker. Redis provides data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes, and streams. Redis has built-in replication, Lua scripting, LRU eviction, transactions, and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster. You can run atomic operations on these types, like appending to a string; incrementing the value in a hash; pushing an element to a list; computing set intersection, union and difference; or getting the member with highest ranking in a sorted set. To achieve top performance, Redis works with an in-memory dataset. Depending on your use case, you can persist your data either by periodically dumping the dataset to disk or by appending each command to a disk-based log. You can also disable persistence if you just need a feature-rich, networked, in-memory cache. Redis also supports asynchronous replication, with very fast non-blocking first synchronization, auto-reconnection with partial resynchronization on net split. Other features include: Transactions Pub/Sub Lua scripting Keys with a limited time-to-live LRU eviction of keys Automatic failover Data Types Strings: you can append, increment, decrement Lists: O(1) access to head and tail, O(n) access to other elements Sets: O(1) access, not repeated elements SortedSets, O(log(n)) access, sorted not repeated elements Hash: same as Python dicts Bitmaps and hyperlogslogs: Strings with more functionalites","title":"Redis"},{"location":"databases/Redis/#introduction-to-redis","text":"Source: https://redis.io/topics/introduction Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache, and message broker. Redis provides data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes, and streams. Redis has built-in replication, Lua scripting, LRU eviction, transactions, and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster. You can run atomic operations on these types, like appending to a string; incrementing the value in a hash; pushing an element to a list; computing set intersection, union and difference; or getting the member with highest ranking in a sorted set. To achieve top performance, Redis works with an in-memory dataset. Depending on your use case, you can persist your data either by periodically dumping the dataset to disk or by appending each command to a disk-based log. You can also disable persistence if you just need a feature-rich, networked, in-memory cache. Redis also supports asynchronous replication, with very fast non-blocking first synchronization, auto-reconnection with partial resynchronization on net split. Other features include: Transactions Pub/Sub Lua scripting Keys with a limited time-to-live LRU eviction of keys Automatic failover","title":"Introduction to Redis"},{"location":"databases/Redis/#data-types","text":"Strings: you can append, increment, decrement Lists: O(1) access to head and tail, O(n) access to other elements Sets: O(1) access, not repeated elements SortedSets, O(log(n)) access, sorted not repeated elements Hash: same as Python dicts Bitmaps and hyperlogslogs: Strings with more functionalites","title":"Data Types"},{"location":"high_output_management/SUMMARY/","text":"Summary Introduction The Basics of Production: Delivering a Breakfast Chapter 2","title":"Summary"},{"location":"high_output_management/SUMMARY/#summary","text":"Introduction The Basics of Production: Delivering a Breakfast Chapter 2","title":"Summary"},{"location":"high_output_management/chapter1/","text":"The Basics of Production: Delivering a Breakfast Limiting step The most time-consuming, or expensive, or critical step in a production flow (bloing the egg in the example of preparing the breakfast) Offset The time separating the start of the diffrent production steps based on the limiting step time. Production Operations process, assemble and test.","title":"The Basics of Production: Delivering a Breakfast"},{"location":"high_output_management/chapter1/#the-basics-of-production-delivering-a-breakfast","text":"","title":"The Basics of Production: Delivering a Breakfast"},{"location":"high_output_management/chapter1/#limiting-step","text":"The most time-consuming, or expensive, or critical step in a production flow (bloing the egg in the example of preparing the breakfast)","title":"Limiting step"},{"location":"high_output_management/chapter1/#offset","text":"The time separating the start of the diffrent production steps based on the limiting step time.","title":"Offset"},{"location":"high_output_management/chapter1/#production-operations","text":"process, assemble and test.","title":"Production Operations"},{"location":"high_output_management/chapter2/","text":"Managing the Breakfast Factory Indicators as a Key Tool Indicators help you measure the quality and the throughput of the proccess. a genuinely effective indicator will cover the output of the work unit and not simply the activity involved you measure should be a physical , countable thing Paring indicators indicators should be paired together so that together both effect and counter-effect are measured Example In the inventory example, you need to monitor both inventory levels and the incidence of shortages. A rise in the latter will obviously lead you to do things to keep inventories from becoming too low. Advantages You have a way to measure he objectives of an employe or a group. You can compare employes doing the same thing, or maybe use that measure to motivate them to compete each others. The Black Box We can think of all sort of process as black box who consume inputs and produce outputs just like a function in computer science :p Leading indicators Give you one way to look inside the black box by showing you in advance what the future might look like. And because they give you time to take corrective action, they make it possible for you to avoid problems. Example Daily monitors for machine downtime or customer satisfaction linearity indicator Draw a line of output against time to be able to see how far you are from the goal and thus make decision before deadline arrive. Trend indicators measured against time performance this month versus performance over a series of previous months, and also against some standard or expected level. Stagger charts Forecast the value of an output for the next months, this will allow you to get insights about the future and also to improve your forecasting model (backpropagation :D) Controlling Future Output Two ways for controlling the future Build to order Like when you need a customized color for a card that is not available Build to forecast Try to forecast the orders, build products for them with the risk of not materialized orders. Like companies that recruits employes based on forecasted work. Building the product and selling it are two separate processes that have different live cycle. Ideally, the order for the product and the product itself should arrive on the shipping dock at the same time. Forecasts should not be done by one person and performed by others, what it works better ask both the manufacturing and the sales departments to prepare a forecast, so that people are responsible for performing against their own predictions. If we have choosen carefully our indicators, and we beleive in them, even administravie work whcih lack objective estimation could be forecast. Assuring quality We must run inspection point on every process stage, it's vitally important to reject an invetory in the earliest stage where it has the lowes-price possible called incoming material inspection or receiving inspection . After detecting a defective invetory we can return them the vendor , use it in order to not stop the production process , such decision are made by managers in different departements (design, manufacturing and quality inssurance). We should be aware that no rejecting defective invetory could cause a complete failure \u2014 a reliability problem \u2014 for our customer and the cost is much more that an financial problem. Example Rejecting the idea of using SQL to store streaming data before you even face problem of scalability is much more from wast you time building the app and face the problem and then reiterate to another technology like Kafka . Balancing between Quality, Inspections and minimun Disturbance Gate-like inspection inspection is done in the input, if it pass the product pass to the next stage, if not it's returned to the previous one for reworking or rejected. Variable inspection is not common because we are creature and we have habit to keep doing what we used to do. Productivity Productivity is the ratio between the output and the labor required to generate that output. Increase Productivity: Our lives purpose We can increase productivity by doing whatever we were doing faster . We can also, more importantly, increase productivity by increasing the leverage of the activities , simplifying the manufacturing process steps (remove not important, inherited or duplicated steps). We have to draw a production chart of all steps and then ask the question why every step is here ?","title":"Managing the Breakfast Factory"},{"location":"high_output_management/chapter2/#managing-the-breakfast-factory","text":"","title":"Managing the Breakfast Factory"},{"location":"high_output_management/chapter2/#indicators-as-a-key-tool","text":"Indicators help you measure the quality and the throughput of the proccess. a genuinely effective indicator will cover the output of the work unit and not simply the activity involved you measure should be a physical , countable thing","title":"Indicators as a Key Tool"},{"location":"high_output_management/chapter2/#paring-indicators","text":"indicators should be paired together so that together both effect and counter-effect are measured Example In the inventory example, you need to monitor both inventory levels and the incidence of shortages. A rise in the latter will obviously lead you to do things to keep inventories from becoming too low.","title":"Paring indicators"},{"location":"high_output_management/chapter2/#advantages","text":"You have a way to measure he objectives of an employe or a group. You can compare employes doing the same thing, or maybe use that measure to motivate them to compete each others.","title":"Advantages"},{"location":"high_output_management/chapter2/#the-black-box","text":"We can think of all sort of process as black box who consume inputs and produce outputs just like a function in computer science :p","title":"The Black Box"},{"location":"high_output_management/chapter2/#leading-indicators","text":"Give you one way to look inside the black box by showing you in advance what the future might look like. And because they give you time to take corrective action, they make it possible for you to avoid problems. Example Daily monitors for machine downtime or customer satisfaction","title":"Leading indicators"},{"location":"high_output_management/chapter2/#linearity-indicator","text":"Draw a line of output against time to be able to see how far you are from the goal and thus make decision before deadline arrive.","title":"linearity indicator"},{"location":"high_output_management/chapter2/#trend-indicators","text":"measured against time performance this month versus performance over a series of previous months, and also against some standard or expected level.","title":"Trend indicators"},{"location":"high_output_management/chapter2/#stagger-charts","text":"Forecast the value of an output for the next months, this will allow you to get insights about the future and also to improve your forecasting model (backpropagation :D)","title":"Stagger charts"},{"location":"high_output_management/chapter2/#controlling-future-output","text":"Two ways for controlling the future","title":"Controlling Future Output"},{"location":"high_output_management/chapter2/#build-to-order","text":"Like when you need a customized color for a card that is not available","title":"Build to order"},{"location":"high_output_management/chapter2/#build-to-forecast","text":"Try to forecast the orders, build products for them with the risk of not materialized orders. Like companies that recruits employes based on forecasted work. Building the product and selling it are two separate processes that have different live cycle. Ideally, the order for the product and the product itself should arrive on the shipping dock at the same time. Forecasts should not be done by one person and performed by others, what it works better ask both the manufacturing and the sales departments to prepare a forecast, so that people are responsible for performing against their own predictions. If we have choosen carefully our indicators, and we beleive in them, even administravie work whcih lack objective estimation could be forecast.","title":"Build to forecast"},{"location":"high_output_management/chapter2/#assuring-quality","text":"We must run inspection point on every process stage, it's vitally important to reject an invetory in the earliest stage where it has the lowes-price possible called incoming material inspection or receiving inspection . After detecting a defective invetory we can return them the vendor , use it in order to not stop the production process , such decision are made by managers in different departements (design, manufacturing and quality inssurance). We should be aware that no rejecting defective invetory could cause a complete failure \u2014 a reliability problem \u2014 for our customer and the cost is much more that an financial problem. Example Rejecting the idea of using SQL to store streaming data before you even face problem of scalability is much more from wast you time building the app and face the problem and then reiterate to another technology like Kafka .","title":"Assuring quality"},{"location":"high_output_management/chapter2/#balancing-between-quality-inspections-and-minimun-disturbance","text":"Gate-like inspection inspection is done in the input, if it pass the product pass to the next stage, if not it's returned to the previous one for reworking or rejected. Variable inspection is not common because we are creature and we have habit to keep doing what we used to do.","title":"Balancing between Quality, Inspections and minimun Disturbance"},{"location":"high_output_management/chapter2/#productivity","text":"Productivity is the ratio between the output and the labor required to generate that output.","title":"Productivity"},{"location":"high_output_management/chapter2/#increase-productivity-our-lives-purpose","text":"We can increase productivity by doing whatever we were doing faster . We can also, more importantly, increase productivity by increasing the leverage of the activities , simplifying the manufacturing process steps (remove not important, inherited or duplicated steps). We have to draw a production chart of all steps and then ask the question why every step is here ?","title":"Increase Productivity: Our lives purpose"},{"location":"high_output_management/chapter3/","text":"Managerial Leverage There is a big difference between manager's output and manager's activities. What Is a Manager\u2019s Output? A manager\u2019s output = The output of his organization + The output of the neighboring organizations under his influence Any sort of work that can help increasing output is part of manager's work, the manager does not only order , direct and suggest to his direct supervision employees. \"Daddy, What Do You Really Do?\" There's no obvious patterns we can extract from a manager workflow, managers often have to shift his energy and attention between different concerns, keep many balls in the air at the same time. The goal is, as always, improving his output which includes the output of his organization and the output of the neighboring organizations under his influence. Information Gathering Many ways are used to gather information: reading standard reports, talk to people inside and outside the company, customer complaints. Most useful information comes from casual verbal exchanges Reports are more a medium of self-discipline than a way to communicate information. Writing the report is important; reading it often is not. Improve your Information Gathering You need to understand how information comes to you, verbal sources are the most valuable but it's incomplete and inaccurate that's the first step on the hierarchy, just like reading a headline on a newspaper, then read the article to go deeper and finally you may read a book to confirm the information you got. When you need information from an employee going to his workplace is much more efficient that inviting him to your office for social reasons. Manager is also a Source of Information Managers should transmit your preferences, priorities to the subordinates in order for them to know how to make decision that are most likely to be acceptable. Decision making Decision are either forward-looking or backward-looking * which involves respectively future or past staff. Information-gathering is the base of decision-making and all manager's tasks. \"Nudging\" is not decision-making , nudging are actions managers do to direct some actions to what they believe can increase his output. A manger is a Role Model The single most important resource is our own time because it's the only finite resource. Meetings provide an occasion for managerial activities, mostly information-gathering and -giving, but also decision-making and nudging Leverage of Managerial Activity Leverage ? Leverage is a measure of the output generated by any managerial activ Leverage is the measure of the output generated by any given managerial activity Managerial Output = Output of organization = L1 \u00d7 A1 + L2 \u00d7 A2 + ... Managerial productivity\u2014that is, the output of a manager per unit of time worked\u2014can be increased in three ways: 1. Increasing the rate with which a manager performs his activities, speeding up his work. 2. Increasing the leverage associated with the various managerial activities. 3. Shifting the mix of a manager\u2019s activities from those with lower to those with higher leverage. High-leverage Activities These can be achieved when many people are affected by one manager or when a person's activity over a long period of time is affected by a manager's brief, well-focused set of words or actions.","title":"Managerial Leverage"},{"location":"high_output_management/chapter3/#managerial-leverage","text":"There is a big difference between manager's output and manager's activities.","title":"Managerial Leverage"},{"location":"high_output_management/chapter3/#what-is-a-managers-output","text":"A manager\u2019s output = The output of his organization + The output of the neighboring organizations under his influence Any sort of work that can help increasing output is part of manager's work, the manager does not only order , direct and suggest to his direct supervision employees.","title":"What Is a Manager\u2019s Output?"},{"location":"high_output_management/chapter3/#daddy-what-do-you-really-do","text":"There's no obvious patterns we can extract from a manager workflow, managers often have to shift his energy and attention between different concerns, keep many balls in the air at the same time. The goal is, as always, improving his output which includes the output of his organization and the output of the neighboring organizations under his influence.","title":"\"Daddy, What Do You Really Do?\""},{"location":"high_output_management/chapter3/#information-gathering","text":"Many ways are used to gather information: reading standard reports, talk to people inside and outside the company, customer complaints. Most useful information comes from casual verbal exchanges Reports are more a medium of self-discipline than a way to communicate information. Writing the report is important; reading it often is not.","title":"Information Gathering"},{"location":"high_output_management/chapter3/#improve-your-information-gathering","text":"You need to understand how information comes to you, verbal sources are the most valuable but it's incomplete and inaccurate that's the first step on the hierarchy, just like reading a headline on a newspaper, then read the article to go deeper and finally you may read a book to confirm the information you got. When you need information from an employee going to his workplace is much more efficient that inviting him to your office for social reasons.","title":"Improve your Information Gathering"},{"location":"high_output_management/chapter3/#manager-is-also-a-source-of-information","text":"Managers should transmit your preferences, priorities to the subordinates in order for them to know how to make decision that are most likely to be acceptable.","title":"Manager is also a Source of Information"},{"location":"high_output_management/chapter3/#decision-making","text":"Decision are either forward-looking or backward-looking * which involves respectively future or past staff. Information-gathering is the base of decision-making and all manager's tasks. \"Nudging\" is not decision-making , nudging are actions managers do to direct some actions to what they believe can increase his output.","title":"Decision making"},{"location":"high_output_management/chapter3/#a-manger-is-a-role-model","text":"The single most important resource is our own time because it's the only finite resource. Meetings provide an occasion for managerial activities, mostly information-gathering and -giving, but also decision-making and nudging","title":"A manger is a Role Model"},{"location":"high_output_management/chapter3/#leverage-of-managerial-activity","text":"","title":"Leverage of Managerial Activity"},{"location":"high_output_management/chapter3/#leverage","text":"Leverage is a measure of the output generated by any managerial activ Leverage is the measure of the output generated by any given managerial activity Managerial Output = Output of organization = L1 \u00d7 A1 + L2 \u00d7 A2 + ... Managerial productivity\u2014that is, the output of a manager per unit of time worked\u2014can be increased in three ways: 1. Increasing the rate with which a manager performs his activities, speeding up his work. 2. Increasing the leverage associated with the various managerial activities. 3. Shifting the mix of a manager\u2019s activities from those with lower to those with higher leverage.","title":"Leverage ?"},{"location":"high_output_management/chapter3/#high-leverage-activities","text":"These can be achieved when many people are affected by one manager or when a person's activity over a long period of time is affected by a manager's brief, well-focused set of words or actions.","title":"High-leverage Activities"},{"location":"high_output_management/introduction/","text":"Introduction Prepare for the unexpected Let chaos reign, then rein in chaos Team Work The output of a manager is the output of the organizational units under his or her supervision or influence One-on-one metting one-on-one meeting, the supervisor teaches the subordinate his skills and know-how, and suggests ways to approach things. At the same time, the subordinate provides the supervisor with detailed information about what he is doing and what he is concerned about Managing your own career The key task is to manage your career so that you do not become a casualty you are in a business with one employee: yourself You should ask yourself: Are you adding real value Are you plugged into what\u2019s happening around you? Are you trying new ideas, new techniques, and new technologies","title":"Introduction"},{"location":"high_output_management/introduction/#introduction","text":"","title":"Introduction"},{"location":"high_output_management/introduction/#prepare-for-the-unexpected","text":"Let chaos reign, then rein in chaos","title":"Prepare for the unexpected"},{"location":"high_output_management/introduction/#team-work","text":"The output of a manager is the output of the organizational units under his or her supervision or influence","title":"Team Work"},{"location":"high_output_management/introduction/#one-on-one-metting","text":"one-on-one meeting, the supervisor teaches the subordinate his skills and know-how, and suggests ways to approach things. At the same time, the subordinate provides the supervisor with detailed information about what he is doing and what he is concerned about","title":"One-on-one metting"},{"location":"high_output_management/introduction/#managing-your-own-career","text":"The key task is to manage your career so that you do not become a casualty you are in a business with one employee: yourself You should ask yourself: Are you adding real value Are you plugged into what\u2019s happening around you? Are you trying new ideas, new techniques, and new technologies","title":"Managing your own career"},{"location":"other/Startups/","text":"Notes from my first start to learn about Startups :p The single biggest reason why startups succeed Timing is the most important reason to success, I have to really study the ability of the costumers to buy the product Your college roommate's approval does not mean market demand Co-founders Think about what happens if you run out of money. How comfortable is everyone in the parternship with things that my happen in the absolute worst-case and in the absolute best-cases. Perfect Vs Done You have to push the product to the market the earlier possible to get feedback. Productive Vs Impcatful You need to focus on the most import things frist Create Velocity Great line from a movie is not so helpful in practice with startups We have to figure out a way to get people to learn about the product and share it with others Word-to-mouth Social Media Blogging Advertising Pitching by telling a story Who's the hero of the project What's the problem ? The opportunity ? How is your solution is going to be unique ? Important ? Tying what you're doing to trends. Team Building How to convince people to accept a far less salary from what they would have taken elsewhere ? Lurking xD What's really matters: Work they will be doing. People they will be working with. Work Environment. Recruiting Have a military mindset. Eager to learn Be able to share his knowledge.","title":"Startups"},{"location":"other/Startups/#the-single-biggest-reason-why-startups-succeed","text":"Timing is the most important reason to success, I have to really study the ability of the costumers to buy the product Your college roommate's approval does not mean market demand","title":"The single biggest reason why startups succeed"},{"location":"other/Startups/#co-founders","text":"Think about what happens if you run out of money. How comfortable is everyone in the parternship with things that my happen in the absolute worst-case and in the absolute best-cases.","title":"Co-founders"},{"location":"other/Startups/#perfect-vs-done","text":"You have to push the product to the market the earlier possible to get feedback.","title":"Perfect Vs Done"},{"location":"other/Startups/#productive-vs-impcatful","text":"You need to focus on the most import things frist","title":"Productive Vs Impcatful"},{"location":"other/Startups/#create-velocity","text":"Great line from a movie is not so helpful in practice with startups We have to figure out a way to get people to learn about the product and share it with others Word-to-mouth Social Media Blogging Advertising Pitching by telling a story Who's the hero of the project What's the problem ? The opportunity ? How is your solution is going to be unique ? Important ? Tying what you're doing to trends.","title":"Create Velocity"},{"location":"other/Startups/#team-building","text":"How to convince people to accept a far less salary from what they would have taken elsewhere ? Lurking xD What's really matters: Work they will be doing. People they will be working with. Work Environment.","title":"Team Building"},{"location":"other/Startups/#recruiting","text":"Have a military mindset. Eager to learn Be able to share his knowledge.","title":"Recruiting"},{"location":"other/beat-the-python/","text":"Beat the Python 30 April online session on Zoom with Midev We did an online session from 2pm to 7pm, we explainded: - What, who, why and where - How to install, Jupyter (breveily) - Variable - Input/Outputs - Operations, Comparisons - Str - Types - conditions - loops - lists - dicts Lessons Don't use an application on production without testing it, Zoom caused me a lot of trouble during session: too many popups/windows during screen cast having to accept people in the waiting room conversation defaults to the people in the waiting room conversation shifts to private message if someone sends me a private messages I'm not sure what I've done but suddenly someone took control over my mouse and start disturbing the session Until I sent him back to the waiting room I couldn't delete the user who took control over the mouse, I don't know why, I clicked on the button so many times without success Don't let them take the mic whenever they want make sure it's open (no waiting room for zoom) or closed for a restricted number of people who can access without confirmation, if this is not possible put someone in charge, otherwise it'll disturb me, people (if they have the possibility to get the mic) start asking to add their fellows ... Fix a time for everything, don't let it open, Example: we'll be giving 10 minutes for this exercice and we'll move on, even if there is someone who didn't understand try not to block everyone for just one person Try hard to get people that have the prerequisites, at least tell them at the begining what are those prereqistiques and if they don't have them, they shouldn't expect to get much and they should consider not blocking others by questions because of that TL;DR: Prepare everything !!! Please everything, execute a dry run of the scenario and see if it could be scaled to the intended number of users you have Long story: I didn't create the users before, then I wasted a long time to create them: ask them to write their emails to google docs (everyone is late ...), then copy the emails and put them into a text file, didn't find the .py script to create the users, I was obliged to buy it from Ahmed and it costs me 50 dzd as always, the script didn't work, I tired to debug it and I was under stress because there was over 70 participants waiting for their Pravda accounts to be cooked, then create a single user for them all and asked them to create a notebook that have their names, it was a mess, too many notebooks, people use other's notebooks, then remembered they need a directory to git clone \"beat-the-python\" repository, ask them again to create directory, then they wonder why we did create a file, then a directory, THAT'S BECAUSE IT WASN'T PREPARED ENOUGH, I WAS DECIDING THINGS ON THE FLY. Then when they created the directory, asked them to run a terminal then cd to their directory some directories have spaces, we should've handle that, explaining why and how to fix it, I didn't do it and ended by a bunch of people complaining about command not found error, people don't know how to clone a repository, so they wonder what I did in GitHub ....","title":"Beat the Python"},{"location":"other/beat-the-python/#beat-the-python","text":"","title":"Beat the Python"},{"location":"other/beat-the-python/#30-april-online-session-on-zoom-with-midev","text":"We did an online session from 2pm to 7pm, we explainded: - What, who, why and where - How to install, Jupyter (breveily) - Variable - Input/Outputs - Operations, Comparisons - Str - Types - conditions - loops - lists - dicts","title":"30 April online session on Zoom with Midev"},{"location":"other/beat-the-python/#lessons","text":"Don't use an application on production without testing it, Zoom caused me a lot of trouble during session: too many popups/windows during screen cast having to accept people in the waiting room conversation defaults to the people in the waiting room conversation shifts to private message if someone sends me a private messages I'm not sure what I've done but suddenly someone took control over my mouse and start disturbing the session Until I sent him back to the waiting room I couldn't delete the user who took control over the mouse, I don't know why, I clicked on the button so many times without success Don't let them take the mic whenever they want make sure it's open (no waiting room for zoom) or closed for a restricted number of people who can access without confirmation, if this is not possible put someone in charge, otherwise it'll disturb me, people (if they have the possibility to get the mic) start asking to add their fellows ... Fix a time for everything, don't let it open, Example: we'll be giving 10 minutes for this exercice and we'll move on, even if there is someone who didn't understand try not to block everyone for just one person Try hard to get people that have the prerequisites, at least tell them at the begining what are those prereqistiques and if they don't have them, they shouldn't expect to get much and they should consider not blocking others by questions because of that TL;DR: Prepare everything !!! Please everything, execute a dry run of the scenario and see if it could be scaled to the intended number of users you have Long story: I didn't create the users before, then I wasted a long time to create them: ask them to write their emails to google docs (everyone is late ...), then copy the emails and put them into a text file, didn't find the .py script to create the users, I was obliged to buy it from Ahmed and it costs me 50 dzd as always, the script didn't work, I tired to debug it and I was under stress because there was over 70 participants waiting for their Pravda accounts to be cooked, then create a single user for them all and asked them to create a notebook that have their names, it was a mess, too many notebooks, people use other's notebooks, then remembered they need a directory to git clone \"beat-the-python\" repository, ask them again to create directory, then they wonder why we did create a file, then a directory, THAT'S BECAUSE IT WASN'T PREPARED ENOUGH, I WAS DECIDING THINGS ON THE FLY. Then when they created the directory, asked them to run a terminal then cd to their directory some directories have spaces, we should've handle that, explaining why and how to fix it, I didn't do it and ended by a bunch of people complaining about command not found error, people don't know how to clone a repository, so they wonder what I did in GitHub ....","title":"Lessons"},{"location":"other/books/","text":"The ownders of manual the brain","title":"Books"},{"location":"other/commands/","text":"Split mkv video mkvtoolnix: https://mkvtoolnix.download/downloads.html#ubuntu PGP key sudo wget - O / usr / share / keyrings / gpg - pub - moritzbunkus . gpg https : // mkvtoolnix . download / gpg - pub - moritzbunkus . gpg Add apt repo \u276f cat /etc/apt/sources.list.d/mkvtoolnix.list deb [arch=amd64 signed-by=/usr/share/keyrings/gpg-pub-moritzbunkus.gpg] https://mkvtoolnix.download/ubuntu/ impish main Install sudo apt update sudo apt install mkvtoolnix mkvtoolnix - gui Use mkvmerge --split parts:00:02:13-00:03:00 input.mkv -o output.mkv Analyze disk usage ncdu -1xo- / | gzip > /tmp/ncdu_export.gz # ...some time later: zcat /tmp/ncdu_export.gz | ncdu -f- Convert a text file to an image convert -size 1920x1080 xc:black -font /home/mohammedi/.fonts/SourceCodePro-Medium.otf -pointsize 30 -fill white -annotate +15+20 \"@ascii.txt\" image.png Default security policy in /etc/ImageMagick-6/policy.xml prevents writing a text file to image, you have to disable this behavior by commenting this line: <policy domain= \"path\" rights= \"none\" pattern= \"@*\" /> Convert an HTML to an image sudo apt-get install texlive-latex-base texlive-fonts-recommended texlive-fonts-extra texlive-latex-extra pandoc tasks.html -o tasks.pdf convert tasks.pdf tasks.png Set a background (doesn't work for ubuntu/gnome) sudo apt install feh feh --bg-scale image.png gsettings set org.gnome.desktop.background picture-uri \"file:////tmp/tasks.png\" I3 Rofi sudo apt install rofi polybar sudo apt install polybar sudo apt install mpd sudo apt install bspwm sudo apt install xbacklight Natural scrolling https://askubuntu.com/questions/1122513/how-to-add-natural-inverted-mouse-scrolling-in-i3-window-manager Add Option \"NaturalScrolling\" \"True\" to /usr/share/X11/xorg.conf.d/40-libinput.conf For mouse look for \"pointer catch all\", for touchpad look for \"touchpad catch all\" Autolock answer: https://faq.i3wm.org/question/83/how-to-run-i3lock-after-computer-inactivity.1.html sudo apt install xautolock cat ~/bin/fuzzy_lock.sh : #!/bin/sh -e # Take a screenshot scrot /tmp/screen_locked.png # Pixellate it 10x mogrify -scale 10 % -scale 1000 % /tmp/screen_locked.png # Lock screen displaying this image. i3lock -i /tmp/screen_locked.png # Turn the screen off after a delay. # What about if I unlock the screen before this 60 seconds completes ? # sleep 60; pgrep i3lock && xset dpms force off KDE on Ubuntu https://askubuntu.com/questions/135267/whats-the-difference-between-kde-packages SDDM insted of GDM sudo apt install sddm Get rid of the virtual keyboard that takes most of the screen when typing password sudo cat / etc / sddm . conf [ General ] InputMethod = Keyboard shortcuts The PATH is not yet loaded, so we can't for example assume the scripts in ~/bin are accessible by their names, we have to use the full path, ~ is also acceptable and will be, as expected, replaced by $HOME I couldn't set a keyboard shortcut to a command with parameter like \"command arg\" If we setup a keyboard shortcut to a bash script that has no #!/bin/bash this error will pop up when we click that shortcut \"execvp: Exec format error\" Snapd applications are not present in the menu Which also makes applications that uses \"xdg-open\" (maybe alo kio-client) to fail to open their links from the browser like when trying to log in to a Slack workspace sudo ln - s / var / lib / snapd / desktop / applications /* / usr / share / applications / Watch Don't starve folder and commit changes #!/bin/bash # run inotify command echo \"Start watching for close_write,create,delete and move events with inotify ...\" # we use close_write instead of modify because modify could be emited many times, it depends on how much write() was called # whereas the close_write is only emited once when the file being edited (opened in write mode) is closed # see: https://stackoverflow.com/a/32424150/7573221 inotifywait -r -q -m -e close_write -e delete -e move --exclude = '.git/*' --format = \"echo 'Committing: %e on %w%f' && git add . && git commit -m'auto commit: %e on %w%f' && sleep 1 && git push\" . | zsh echo \"Stopped watching ...\" [program:auto-push-dont-starve] environment = USER=mohammedi,HOME=/home/mohammedi user = mohammedi autostart = True directory = /home/mohammedi/.steam/steam/userdata/1072787539/219740 command = zsh -c \"sudo -u mohammedi /home/mohammedi/.steam/steam/userdata/1072787539/219740/watch.sh\" stderr_logfile = /var/log/auto-push-dont-starve/error.log stdout_logfile = /var/log/auto-push-dont-starve/out.log","title":"Split mkv video"},{"location":"other/commands/#split-mkv-video","text":"mkvtoolnix: https://mkvtoolnix.download/downloads.html#ubuntu PGP key sudo wget - O / usr / share / keyrings / gpg - pub - moritzbunkus . gpg https : // mkvtoolnix . download / gpg - pub - moritzbunkus . gpg Add apt repo \u276f cat /etc/apt/sources.list.d/mkvtoolnix.list deb [arch=amd64 signed-by=/usr/share/keyrings/gpg-pub-moritzbunkus.gpg] https://mkvtoolnix.download/ubuntu/ impish main Install sudo apt update sudo apt install mkvtoolnix mkvtoolnix - gui Use mkvmerge --split parts:00:02:13-00:03:00 input.mkv -o output.mkv","title":"Split mkv video"},{"location":"other/commands/#analyze-disk-usage","text":"ncdu -1xo- / | gzip > /tmp/ncdu_export.gz # ...some time later: zcat /tmp/ncdu_export.gz | ncdu -f-","title":"Analyze disk usage"},{"location":"other/commands/#convert-a-text-file-to-an-image","text":"convert -size 1920x1080 xc:black -font /home/mohammedi/.fonts/SourceCodePro-Medium.otf -pointsize 30 -fill white -annotate +15+20 \"@ascii.txt\" image.png Default security policy in /etc/ImageMagick-6/policy.xml prevents writing a text file to image, you have to disable this behavior by commenting this line: <policy domain= \"path\" rights= \"none\" pattern= \"@*\" />","title":"Convert a text file to an image"},{"location":"other/commands/#convert-an-html-to-an-image","text":"sudo apt-get install texlive-latex-base texlive-fonts-recommended texlive-fonts-extra texlive-latex-extra pandoc tasks.html -o tasks.pdf convert tasks.pdf tasks.png","title":"Convert an HTML to an image"},{"location":"other/commands/#set-a-background-doesnt-work-for-ubuntugnome","text":"sudo apt install feh feh --bg-scale image.png gsettings set org.gnome.desktop.background picture-uri \"file:////tmp/tasks.png\"","title":"Set a background (doesn't work for ubuntu/gnome)"},{"location":"other/commands/#i3","text":"","title":"I3"},{"location":"other/commands/#rofi","text":"sudo apt install rofi","title":"Rofi"},{"location":"other/commands/#polybar","text":"sudo apt install polybar sudo apt install mpd sudo apt install bspwm sudo apt install xbacklight","title":"polybar"},{"location":"other/commands/#natural-scrolling","text":"https://askubuntu.com/questions/1122513/how-to-add-natural-inverted-mouse-scrolling-in-i3-window-manager Add Option \"NaturalScrolling\" \"True\" to /usr/share/X11/xorg.conf.d/40-libinput.conf For mouse look for \"pointer catch all\", for touchpad look for \"touchpad catch all\"","title":"Natural scrolling"},{"location":"other/commands/#autolock","text":"answer: https://faq.i3wm.org/question/83/how-to-run-i3lock-after-computer-inactivity.1.html sudo apt install xautolock cat ~/bin/fuzzy_lock.sh : #!/bin/sh -e # Take a screenshot scrot /tmp/screen_locked.png # Pixellate it 10x mogrify -scale 10 % -scale 1000 % /tmp/screen_locked.png # Lock screen displaying this image. i3lock -i /tmp/screen_locked.png # Turn the screen off after a delay. # What about if I unlock the screen before this 60 seconds completes ? # sleep 60; pgrep i3lock && xset dpms force off","title":"Autolock"},{"location":"other/commands/#kde-on-ubuntu","text":"https://askubuntu.com/questions/135267/whats-the-difference-between-kde-packages","title":"KDE on Ubuntu"},{"location":"other/commands/#sddm-insted-of-gdm","text":"sudo apt install sddm Get rid of the virtual keyboard that takes most of the screen when typing password sudo cat / etc / sddm . conf [ General ] InputMethod =","title":"SDDM insted of GDM"},{"location":"other/commands/#keyboard-shortcuts","text":"The PATH is not yet loaded, so we can't for example assume the scripts in ~/bin are accessible by their names, we have to use the full path, ~ is also acceptable and will be, as expected, replaced by $HOME I couldn't set a keyboard shortcut to a command with parameter like \"command arg\" If we setup a keyboard shortcut to a bash script that has no #!/bin/bash this error will pop up when we click that shortcut \"execvp: Exec format error\"","title":"Keyboard shortcuts"},{"location":"other/commands/#snapd-applications-are-not-present-in-the-menu","text":"Which also makes applications that uses \"xdg-open\" (maybe alo kio-client) to fail to open their links from the browser like when trying to log in to a Slack workspace sudo ln - s / var / lib / snapd / desktop / applications /* / usr / share / applications /","title":"Snapd applications are not present in the menu"},{"location":"other/commands/#watch-dont-starve-folder-and-commit-changes","text":"#!/bin/bash # run inotify command echo \"Start watching for close_write,create,delete and move events with inotify ...\" # we use close_write instead of modify because modify could be emited many times, it depends on how much write() was called # whereas the close_write is only emited once when the file being edited (opened in write mode) is closed # see: https://stackoverflow.com/a/32424150/7573221 inotifywait -r -q -m -e close_write -e delete -e move --exclude = '.git/*' --format = \"echo 'Committing: %e on %w%f' && git add . && git commit -m'auto commit: %e on %w%f' && sleep 1 && git push\" . | zsh echo \"Stopped watching ...\" [program:auto-push-dont-starve] environment = USER=mohammedi,HOME=/home/mohammedi user = mohammedi autostart = True directory = /home/mohammedi/.steam/steam/userdata/1072787539/219740 command = zsh -c \"sudo -u mohammedi /home/mohammedi/.steam/steam/userdata/1072787539/219740/watch.sh\" stderr_logfile = /var/log/auto-push-dont-starve/error.log stdout_logfile = /var/log/auto-push-dont-starve/out.log","title":"Watch Don't starve folder and commit changes"},{"location":"other/emacs/","text":"Tkharbich Gtags Spacemacs layer: https://github.com/syl20bnr/spacemacs/tree/master/layers/%2Btags/gtags helm-gtags and ggtags are clients for GNU Global. GNU Global is a source code tagging system that allows querying symbol locations in source code, such as definitions or references. Adding the gtags layer enables both of these modes. Eldoc Website: https://www.emacswiki.org/emacs/ElDoc A very simple but effective thing, eldoc-mode is a Minor Mode which shows you, in the echo area, the argument list of the function call you are currently writing. Very handy. By Noah Friedman. Part of Emacs. Useful commands spacemacs/describe-variable : show documentation of a variable, such as the layer's configurations variables FAQ 1.14 Should I place my settings in user-init or user-config? Any variable that layer configuration code will read and act on must be set in user-init, and any variable that Spacemacs explicitly sets but you wish to override must be set in user-config. Anything that isn't just setting a variable should 99% be in user-config. Note that at time of writing files supplied as command line arguments to emacs will be read before user-config is executed. (Hence to yield consistent behaviour, mode hooks should be set in user-init.) 1.20 Why do I get files starting with .#? These are lockfiles, created by Emacs to prevent editing conflicts which occur when the same file is edited simultaneously by two different programs. To disable this behaviour: ( setq create-lockfiles nil ) 1.23 Why am I getting a message about environment variables on startup?\u00b6 Spacemacs uses the exec-path-from-shell package to set the executable path when Emacs starts up. This is done by launching a shell and reading the values of variables such as PATH and MANPATH from it. If your shell configuration sets the values of these variables inconsistently, this could be problematic. It is recommended to set such variables in shell configuration files that are sourced unconditionally, such as .profile, .bash_profile or .zshenv, as opposed to files that are sourced only for interactive shells, such as .bashrc or .zshrc. If you are willing to neglect this advice, you may disable the warning, e.g. from dotspacemacs/user-init: ( setq exec-path-from-shell-check-startup-files nil ) You can also disable this feature entirely by adding exec-path-from-shell to the list dotspacemacs-excluded-packages if you prefer setting exec-path yourself.","title":"Emacs"},{"location":"other/emacs/#tkharbich","text":"","title":"Tkharbich"},{"location":"other/emacs/#gtags","text":"Spacemacs layer: https://github.com/syl20bnr/spacemacs/tree/master/layers/%2Btags/gtags helm-gtags and ggtags are clients for GNU Global. GNU Global is a source code tagging system that allows querying symbol locations in source code, such as definitions or references. Adding the gtags layer enables both of these modes.","title":"Gtags"},{"location":"other/emacs/#eldoc","text":"Website: https://www.emacswiki.org/emacs/ElDoc A very simple but effective thing, eldoc-mode is a Minor Mode which shows you, in the echo area, the argument list of the function call you are currently writing. Very handy. By Noah Friedman. Part of Emacs.","title":"Eldoc"},{"location":"other/emacs/#useful-commands","text":"spacemacs/describe-variable : show documentation of a variable, such as the layer's configurations variables","title":"Useful commands"},{"location":"other/emacs/#faq","text":"","title":"FAQ"},{"location":"other/emacs/#114-should-i-place-my-settings-in-user-init-or-user-config","text":"Any variable that layer configuration code will read and act on must be set in user-init, and any variable that Spacemacs explicitly sets but you wish to override must be set in user-config. Anything that isn't just setting a variable should 99% be in user-config. Note that at time of writing files supplied as command line arguments to emacs will be read before user-config is executed. (Hence to yield consistent behaviour, mode hooks should be set in user-init.)","title":"1.14 Should I place my settings in user-init or user-config?"},{"location":"other/emacs/#120-why-do-i-get-files-starting-with","text":"These are lockfiles, created by Emacs to prevent editing conflicts which occur when the same file is edited simultaneously by two different programs. To disable this behaviour: ( setq create-lockfiles nil )","title":"1.20 Why do I get files starting with .#?"},{"location":"other/emacs/#123-why-am-i-getting-a-message-about-environment-variables-on-startup","text":"Spacemacs uses the exec-path-from-shell package to set the executable path when Emacs starts up. This is done by launching a shell and reading the values of variables such as PATH and MANPATH from it. If your shell configuration sets the values of these variables inconsistently, this could be problematic. It is recommended to set such variables in shell configuration files that are sourced unconditionally, such as .profile, .bash_profile or .zshenv, as opposed to files that are sourced only for interactive shells, such as .bashrc or .zshrc. If you are willing to neglect this advice, you may disable the warning, e.g. from dotspacemacs/user-init: ( setq exec-path-from-shell-check-startup-files nil ) You can also disable this feature entirely by adding exec-path-from-shell to the list dotspacemacs-excluded-packages if you prefer setting exec-path yourself.","title":"1.23 Why am I getting a message about environment variables on startup?\u00b6"},{"location":"other/english/","text":"Talk fluently Speak English Fluently - 5 Steps to Improve Your English Fluency Fluency means speaking smoothly without stopping or hesitating both physically (mouth) and mentally (generate phrases) Get out there and have conversations: 2-3 hours / week in a minimum, you can start by getting into social groups about your subjects of interests Get used to pressure, it's natural to have difficulties speaking a foreign language, just accept that and have conversations Speed reading: get a text fairly easy for you, read it aloud once, start to get faster and faster ..., it'll make your mouth practice english sounds Songs: listen to a song while reading the lyrics, go for faster songs ... Learn language in chunks i.e phrases not words","title":"Talk fluently"},{"location":"other/english/#talk-fluently","text":"Speak English Fluently - 5 Steps to Improve Your English Fluency Fluency means speaking smoothly without stopping or hesitating both physically (mouth) and mentally (generate phrases) Get out there and have conversations: 2-3 hours / week in a minimum, you can start by getting into social groups about your subjects of interests Get used to pressure, it's natural to have difficulties speaking a foreign language, just accept that and have conversations Speed reading: get a text fairly easy for you, read it aloud once, start to get faster and faster ..., it'll make your mouth practice english sounds Songs: listen to a song while reading the lyrics, go for faster songs ... Learn language in chunks i.e phrases not words","title":"Talk fluently"},{"location":"other/entrepreneurship/","text":"Enterpreneurship Things that are important: Perseverance Work ethics Confidence Common traits Confident Gregarious (Social) Forward thinking Action oriented (Just do it) Most important Risk tolerant : Accept failing, keep failing. The enterpreuneur mindset They tend to have a: 1. A defined view of the future. 2. Optimistic about it. Business life cycle Pre-launch: planning to tackle a market. Startup: Just to do it. Small operations. Doing everything necessary Growth: more clients/employeers Enterpreneur stop doing everything Maturity: The market is so developped. Typically pivot or doing something else to get more. Enterpreneur typicall leave and start a new adventure. Types of income Active income Passive income Investing in stocks Dividends Recurring revenue: pay repeatidly (\"Cours de soutien xD\") Planned obsolescence: phones Process of opportunity Recognizing the opportunity (Problem to solve ..) Conceptualization: might asking costumers/friends looking at market/competitors Planning: gathering resources come up with a Business model Launching the business Paths to go for Lifestyle enterpreuneur Start business to have a good life, they focused on money They by design don't make a tons of money as it should take too much time They don't seek a brand new idea or a key diffrenciator Side business enterpreuneur Look for things that don't require a lot of time Little risk, little return Plateforms: Etsy, fiverr, ebay, craigslist->ebay Startup enterpreuneur A startup is a business who is designed to grow as big as possible. Typically 100% time require. Social enterpreuneur Primary produces good to everyone and making money. \"Bee coorporations\": making money while delivering value to social. Service vs Product Service incure cost each time you sell it. Products not. Business Model Def: The way in which you make money Different types: - Advertising: Google. - E-commmerce: Amazone. - Subscription: New york times. Business model could be changed along the way Old business models Retail \"Brick and mortar\" ? \"Brick and click\": Most clothing brands, you can buy physically and online Franchise (point de vente officielle) New business models On demand: Uber, Shyp, Washio, Postmates, lyft. Sharing economy: aribnb, \"The power drill dilemma\" Crowdsourcing: get ideas from the crowd, techcrunch. Freemuim: give it to free, they get addicted, then they can't just live without you, example: Evernote, money lover. Direct to consumer: Dell computers. Scalability Def: how to grow your business to thoughsands, millions of users. Does your business have a lot of marginal cost (every time you sell something how much it costs you) ? Examples: - SAAS: scalable business. - Painting: not scalable. - Large market easy to scale, smaller not. - Coaching: not scalable -> Online course: - Startups need (as in MUST) to be scalable Notes \"Out of context\" The \"Marchamrmalomon\" test \"Survivor's bias\" Extroverts vs Introverts Subscription plans are goood IMHO, they make the business scalable: we should think of one: maybe a plateforme here people get trainings, can ask questions, get orientation and also can make money by applying for freelance or job offers, the added value here is our evaluation of the person (whether who took a course/ solve some exercice .. etc) which would be time-consuming operation for hires.","title":"Enterpreneurship"},{"location":"other/entrepreneurship/#enterpreneurship","text":"","title":"Enterpreneurship"},{"location":"other/entrepreneurship/#things-that-are-important","text":"Perseverance Work ethics Confidence","title":"Things that are important:"},{"location":"other/entrepreneurship/#common-traits","text":"Confident Gregarious (Social) Forward thinking Action oriented (Just do it)","title":"Common traits"},{"location":"other/entrepreneurship/#most-important","text":"Risk tolerant : Accept failing, keep failing.","title":"Most important"},{"location":"other/entrepreneurship/#the-enterpreuneur-mindset","text":"They tend to have a: 1. A defined view of the future. 2. Optimistic about it.","title":"The enterpreuneur mindset"},{"location":"other/entrepreneurship/#business-life-cycle","text":"Pre-launch: planning to tackle a market. Startup: Just to do it. Small operations. Doing everything necessary Growth: more clients/employeers Enterpreneur stop doing everything Maturity: The market is so developped. Typically pivot or doing something else to get more. Enterpreneur typicall leave and start a new adventure.","title":"Business life cycle"},{"location":"other/entrepreneurship/#types-of-income","text":"Active income Passive income Investing in stocks Dividends Recurring revenue: pay repeatidly (\"Cours de soutien xD\") Planned obsolescence: phones","title":"Types of income"},{"location":"other/entrepreneurship/#process-of-opportunity","text":"Recognizing the opportunity (Problem to solve ..) Conceptualization: might asking costumers/friends looking at market/competitors Planning: gathering resources come up with a Business model Launching the business","title":"Process of opportunity"},{"location":"other/entrepreneurship/#paths-to-go-for","text":"","title":"Paths to go for"},{"location":"other/entrepreneurship/#lifestyle-enterpreuneur","text":"Start business to have a good life, they focused on money They by design don't make a tons of money as it should take too much time They don't seek a brand new idea or a key diffrenciator","title":"Lifestyle enterpreuneur"},{"location":"other/entrepreneurship/#side-business-enterpreuneur","text":"Look for things that don't require a lot of time Little risk, little return Plateforms: Etsy, fiverr, ebay, craigslist->ebay","title":"Side business enterpreuneur"},{"location":"other/entrepreneurship/#startup-enterpreuneur","text":"A startup is a business who is designed to grow as big as possible. Typically 100% time require.","title":"Startup enterpreuneur"},{"location":"other/entrepreneurship/#social-enterpreuneur","text":"Primary produces good to everyone and making money. \"Bee coorporations\": making money while delivering value to social.","title":"Social enterpreuneur"},{"location":"other/entrepreneurship/#service-vs-product","text":"Service incure cost each time you sell it. Products not.","title":"Service vs Product"},{"location":"other/entrepreneurship/#business-model","text":"Def: The way in which you make money Different types: - Advertising: Google. - E-commmerce: Amazone. - Subscription: New york times. Business model could be changed along the way","title":"Business Model"},{"location":"other/entrepreneurship/#old-business-models","text":"","title":"Old business models"},{"location":"other/entrepreneurship/#retail","text":"\"Brick and mortar\" ? \"Brick and click\": Most clothing brands, you can buy physically and online Franchise (point de vente officielle)","title":"Retail"},{"location":"other/entrepreneurship/#new-business-models","text":"On demand: Uber, Shyp, Washio, Postmates, lyft. Sharing economy: aribnb, \"The power drill dilemma\" Crowdsourcing: get ideas from the crowd, techcrunch. Freemuim: give it to free, they get addicted, then they can't just live without you, example: Evernote, money lover. Direct to consumer: Dell computers.","title":"New business models"},{"location":"other/entrepreneurship/#scalability","text":"Def: how to grow your business to thoughsands, millions of users. Does your business have a lot of marginal cost (every time you sell something how much it costs you) ? Examples: - SAAS: scalable business. - Painting: not scalable. - Large market easy to scale, smaller not. - Coaching: not scalable -> Online course: - Startups need (as in MUST) to be scalable","title":"Scalability"},{"location":"other/entrepreneurship/#notes-out-of-context","text":"The \"Marchamrmalomon\" test \"Survivor's bias\" Extroverts vs Introverts Subscription plans are goood IMHO, they make the business scalable: we should think of one: maybe a plateforme here people get trainings, can ask questions, get orientation and also can make money by applying for freelance or job offers, the added value here is our evaluation of the person (whether who took a course/ solve some exercice .. etc) which would be time-consuming operation for hires.","title":"Notes \"Out of context\""},{"location":"other/facebook_ads/","text":"Facebook ads Terminology Campaign This is the highest level, you can have multiple ads sets, each ad set can have multiple ads. Ad Set Here we can create DETAILED TARGETING to only show our ad to the people most likely to buy it. Ad This is what the consumer actually see. Custom audience Create audience based on some criteria. We can create an audience giving a list of emails and facebook ads can figure out the common attributes between the given users. Lookalike audience Create a lookalike audience for people who like your page. Facebook Pixel Let us tell facebook about our facebook visitors. we can configure that with particular pages and so on. We can for example create a lookalike audience from people who have visited our order page. Facebook business manager Create separate ad accounts for every client or business you serve, pay for ads with different payment methods, and organize by objective for reporting. Campaign types Traffic Add more traffic to your website, page, instagram ..etc Engagement Get people to like/share/comment and react to your ad, this add credibility to the ad. Conversion You want to convert some audience (who visited your page) to buy your product or service. Audience insights It gives statistiques about facebook users: genders, location, interests, actions, devices It makes it easy to tell who is you audience, you can filter by age, location, interest, page likes and so on link: https://www.facebook.com/ads/audience-insights Browse others ads https://adespresso.com/ https://swiped.co/","title":"Facebook ads"},{"location":"other/facebook_ads/#facebook-ads","text":"","title":"Facebook ads"},{"location":"other/facebook_ads/#terminology","text":"","title":"Terminology"},{"location":"other/facebook_ads/#campaign","text":"This is the highest level, you can have multiple ads sets, each ad set can have multiple ads.","title":"Campaign"},{"location":"other/facebook_ads/#ad-set","text":"Here we can create DETAILED TARGETING to only show our ad to the people most likely to buy it.","title":"Ad Set"},{"location":"other/facebook_ads/#ad","text":"This is what the consumer actually see.","title":"Ad"},{"location":"other/facebook_ads/#custom-audience","text":"Create audience based on some criteria. We can create an audience giving a list of emails and facebook ads can figure out the common attributes between the given users.","title":"Custom audience"},{"location":"other/facebook_ads/#lookalike-audience","text":"Create a lookalike audience for people who like your page.","title":"Lookalike audience"},{"location":"other/facebook_ads/#facebook-pixel","text":"Let us tell facebook about our facebook visitors. we can configure that with particular pages and so on. We can for example create a lookalike audience from people who have visited our order page.","title":"Facebook Pixel"},{"location":"other/facebook_ads/#facebook-business-manager","text":"Create separate ad accounts for every client or business you serve, pay for ads with different payment methods, and organize by objective for reporting.","title":"Facebook business manager"},{"location":"other/facebook_ads/#campaign-types","text":"","title":"Campaign types"},{"location":"other/facebook_ads/#traffic","text":"Add more traffic to your website, page, instagram ..etc","title":"Traffic"},{"location":"other/facebook_ads/#engagement","text":"Get people to like/share/comment and react to your ad, this add credibility to the ad.","title":"Engagement"},{"location":"other/facebook_ads/#conversion","text":"You want to convert some audience (who visited your page) to buy your product or service.","title":"Conversion"},{"location":"other/facebook_ads/#audience-insights","text":"It gives statistiques about facebook users: genders, location, interests, actions, devices It makes it easy to tell who is you audience, you can filter by age, location, interest, page likes and so on link: https://www.facebook.com/ads/audience-insights","title":"Audience insights"},{"location":"other/facebook_ads/#browse-others-ads","text":"https://adespresso.com/ https://swiped.co/","title":"Browse others ads"},{"location":"other/fix_stuck_apt/","text":"Apt and Dpkg stuck at install/remove packages I was about to install docker in groovy (ubuntu 20.10) before the packages were published to the apt repository, I changed the apt list entry to use \"focal\" (ubuntu 20.04) instead and tried to install as I usually do when packages aren't yet published to a specific ubuntu version, the installation failed and then it stucks at installing/uninstalling docker, no other apt command works it said I should run \"sudo dpkg --configure -a\" because some packags are not configured then it stuck there when it tries to mess with docker What I did ? Remove the packages information of docker-ce* and containerd from /var/lib/dpkg/status Remove all related files in var/lib/dpkg/info/ sudo dpkg --remove --force-remove-reinstreq docker-ce","title":"Apt and Dpkg stuck at install/remove packages"},{"location":"other/fix_stuck_apt/#apt-and-dpkg-stuck-at-installremove-packages","text":"I was about to install docker in groovy (ubuntu 20.10) before the packages were published to the apt repository, I changed the apt list entry to use \"focal\" (ubuntu 20.04) instead and tried to install as I usually do when packages aren't yet published to a specific ubuntu version, the installation failed and then it stucks at installing/uninstalling docker, no other apt command works it said I should run \"sudo dpkg --configure -a\" because some packags are not configured then it stuck there when it tries to mess with docker","title":"Apt and Dpkg stuck at install/remove packages"},{"location":"other/fix_stuck_apt/#what-i-did","text":"Remove the packages information of docker-ce* and containerd from /var/lib/dpkg/status Remove all related files in var/lib/dpkg/info/ sudo dpkg --remove --force-remove-reinstreq docker-ce","title":"What I did ?"},{"location":"other/git-server/","text":"Admin Setup the server Link: https://git-scm.com/book/en/v2/Git-on-the-Server-Setting-Up-the-Server Create the git user and the .ssh/authorized_keys file sudo adduser git su git cd mkdir .ssh && chmod 700 .ssh touch .ssh/authorized_keys && chmod 600 .ssh/authorized_keys Add someone's SSH public key to the .ssh/authorized_keys file cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys Create the repo sudo su # Login a user root mkdir /srv/git chown git:git /srv/git sudo su git # Login as user git cd /srv/git mkdir project.git cd project.git git init --bare Initialized empty Git repository in /srv/git/project.git/ Sizing a server for git Link: https://gitolite.com/server-sizing.html TL;DR; Git is not hungry on resources and developpers interact with it 2-6 times a day on average any descent dual-core CPU can do the work A machine with about 512 MB free RAM will probaly work fine for most developement style repositories. this means a total of 1 GB. Security Prevents authorized users from getting a shell cat /etc/shells # see if git-shell is already in there. If not... which git-shell # make sure git-shell is installed on your system. sudo -e /etc/shells # and add the path to git-shell from last command to this sudo chsh git -s $( which git-shell ) # Set the shell to git-shell for git user Prevent them from SSH port forwarding by prepending their entry in the .ssh/authorized_keys with no-port-forwarding,no-X11-forwarding,no-agent-forwarding,no-pty ssh-rsa [ KEY ] Disable password authentication for git user Create a file /etc/ssh/sshd_config.d/disable_password_for_git_user.conf Match User git PasswordAuthentication no ```bash sudo systemctl restart ssh Top 20 OpenSSH Server Best Security Practices Developer Generate SSH Keys ssh-keygen Then share the public key with the Git server administrator to add you to the .ssh/authorized_keys file cat .ssh/id_rsa.pub Which will give you something like ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDX6nKWKYcpdyJtzl1m2exlSafRFFHcV02soXjwplwNht/WJIpRu2yYuTveWPztdHp5RZPxGVURpJHbwENC8v2n4LqTWb2TZNC3hjoOYHx+jUSuGkJRV8TigM5lwlL1vWyuaXxUJKEUwdgHGjJag7ywnRYRga2MaK3PN/Z4NllJaTfPyprrVkLLqHqY1YZSd7y5hnC9JWI+1NKgd1fUoANkKS/FIodcO6/AL7Rx+KNwW47Dq4/IT+6wRH+bMGvKSP7P6QODiSEOF24U444z42YhxX3PyXOXmuspHlCLs06Kd5WnId/KbUsS/mn2edxtGEtdXiw0CR6/jr+A3kxz8M3gsiwlTssvkiqTaz7Gr8lHtbj2AxGxhYSC9jyXeS5ynYQEi3QcubDvRsOP6Va4ihKt7IxPe+XBc8qZVRo9qHKY3T2xMn0EistZnV+HXZfNSV/hiGGRDiQqrIFwzSDEUdM5oodAYFHhNwZbbTAEeeVJAU7GM/n/ZqVt249k3drt5ss = azuread \\h arounemohammedi@DESKTOP-RR5NSCL Access the repo project is the directory where the the project files lives gitserver is the IP/domain of the Git server if the project is not a git repo cd project git init git add . git commit -m 'Initial commit' git remote add origin git@gitserver:/srv/git/project.git git push origin main if the project is already a git repo with configured remote (Gitlab/Github/AzureDevOps) cd project git remote set-url origin git@gitserver:/srv/git/project.git git push origin --all","title":"Admin"},{"location":"other/git-server/#admin","text":"","title":"Admin"},{"location":"other/git-server/#setup-the-server","text":"Link: https://git-scm.com/book/en/v2/Git-on-the-Server-Setting-Up-the-Server Create the git user and the .ssh/authorized_keys file sudo adduser git su git cd mkdir .ssh && chmod 700 .ssh touch .ssh/authorized_keys && chmod 600 .ssh/authorized_keys Add someone's SSH public key to the .ssh/authorized_keys file cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys Create the repo sudo su # Login a user root mkdir /srv/git chown git:git /srv/git sudo su git # Login as user git cd /srv/git mkdir project.git cd project.git git init --bare Initialized empty Git repository in /srv/git/project.git/","title":"Setup the server"},{"location":"other/git-server/#sizing-a-server-for-git","text":"Link: https://gitolite.com/server-sizing.html TL;DR; Git is not hungry on resources and developpers interact with it 2-6 times a day on average any descent dual-core CPU can do the work A machine with about 512 MB free RAM will probaly work fine for most developement style repositories. this means a total of 1 GB.","title":"Sizing a server for git"},{"location":"other/git-server/#security","text":"Prevents authorized users from getting a shell cat /etc/shells # see if git-shell is already in there. If not... which git-shell # make sure git-shell is installed on your system. sudo -e /etc/shells # and add the path to git-shell from last command to this sudo chsh git -s $( which git-shell ) # Set the shell to git-shell for git user Prevent them from SSH port forwarding by prepending their entry in the .ssh/authorized_keys with no-port-forwarding,no-X11-forwarding,no-agent-forwarding,no-pty ssh-rsa [ KEY ] Disable password authentication for git user Create a file /etc/ssh/sshd_config.d/disable_password_for_git_user.conf Match User git PasswordAuthentication no ```bash sudo systemctl restart ssh Top 20 OpenSSH Server Best Security Practices","title":"Security"},{"location":"other/git-server/#developer","text":"","title":"Developer"},{"location":"other/git-server/#generate-ssh-keys","text":"ssh-keygen Then share the public key with the Git server administrator to add you to the .ssh/authorized_keys file cat .ssh/id_rsa.pub Which will give you something like ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDX6nKWKYcpdyJtzl1m2exlSafRFFHcV02soXjwplwNht/WJIpRu2yYuTveWPztdHp5RZPxGVURpJHbwENC8v2n4LqTWb2TZNC3hjoOYHx+jUSuGkJRV8TigM5lwlL1vWyuaXxUJKEUwdgHGjJag7ywnRYRga2MaK3PN/Z4NllJaTfPyprrVkLLqHqY1YZSd7y5hnC9JWI+1NKgd1fUoANkKS/FIodcO6/AL7Rx+KNwW47Dq4/IT+6wRH+bMGvKSP7P6QODiSEOF24U444z42YhxX3PyXOXmuspHlCLs06Kd5WnId/KbUsS/mn2edxtGEtdXiw0CR6/jr+A3kxz8M3gsiwlTssvkiqTaz7Gr8lHtbj2AxGxhYSC9jyXeS5ynYQEi3QcubDvRsOP6Va4ihKt7IxPe+XBc8qZVRo9qHKY3T2xMn0EistZnV+HXZfNSV/hiGGRDiQqrIFwzSDEUdM5oodAYFHhNwZbbTAEeeVJAU7GM/n/ZqVt249k3drt5ss = azuread \\h arounemohammedi@DESKTOP-RR5NSCL","title":"Generate SSH Keys"},{"location":"other/git-server/#access-the-repo","text":"project is the directory where the the project files lives gitserver is the IP/domain of the Git server","title":"Access the repo"},{"location":"other/git-server/#if-the-project-is-not-a-git-repo","text":"cd project git init git add . git commit -m 'Initial commit' git remote add origin git@gitserver:/srv/git/project.git git push origin main","title":"if the project is not a git repo"},{"location":"other/git-server/#if-the-project-is-already-a-git-repo-with-configured-remote-gitlabgithubazuredevops","text":"cd project git remote set-url origin git@gitserver:/srv/git/project.git git push origin --all","title":"if the project is already a git repo with configured remote (Gitlab/Github/AzureDevOps)"},{"location":"other/grub/","text":"Runlevels To change the run level, press \"e\" in the grub menu, go to the line where starting with \"kernel\" and press \"e\" again, it'll go to the end of the line of the command add the number/name of the run level and press \"Entre\" then \"b\" to boot to the system 0 or single: Boot to the root user, allow you to change the passoword .. 3: Boot to non-graphical system init=/bin/bash: Bypass init and just drop me at a shell, nothing will start","title":"Grub"},{"location":"other/grub/#runlevels","text":"To change the run level, press \"e\" in the grub menu, go to the line where starting with \"kernel\" and press \"e\" again, it'll go to the end of the line of the command add the number/name of the run level and press \"Entre\" then \"b\" to boot to the system 0 or single: Boot to the root user, allow you to change the passoword .. 3: Boot to non-graphical system init=/bin/bash: Bypass init and just drop me at a shell, nothing will start","title":"Runlevels"},{"location":"other/habit/","text":"Learn How To Control Your Mind (USE This To BrainWash Yourself) Link: https://www.youtube.com/watch?v=v7KQsS2kLM4 What's a habit ? a habit is a redundant set of automatic, unconscious thoughts, behaviors and emotions that's acquired through repetition, a habit is that we've done something so many times that your buddy now knows how to do it better than your mind. Change ? The moment you decide to do the change, the moment you decide to make a different choice than what your body/subconscious used to GET READY because it's going to feel UNCOMFORTABLE. 8 Habits You Should Practice at Least Once a Week Link: https://www.youtube.com/watch?v=_NO1ldvMDXQ Call/Text someone you care about and didn't catch up for a moment Do an all-out workout Review the productivity system, habit trackers, notes .... Do a quick financial health check-up Plan and lead a high-density fun activity Budgeting: https://www.youtube.com/redirect?q=https%3A%2F%2Fcollegeinfogeek.com%2Fbudget-without-cutting-coffee%2F&redir_token=hNZrSJkCW9PBqaJYwvyCLL0j2kV8MTU4ODYxOTU0MEAxNTg4NTMzMTQw&v=_NO1ldvMDXQ&event=video_description","title":"Learn How To Control Your Mind (USE This To BrainWash Yourself)"},{"location":"other/habit/#learn-how-to-control-your-mind-use-this-to-brainwash-yourself","text":"Link: https://www.youtube.com/watch?v=v7KQsS2kLM4","title":"Learn How To Control Your Mind (USE This To BrainWash Yourself)"},{"location":"other/habit/#whats-a-habit","text":"a habit is a redundant set of automatic, unconscious thoughts, behaviors and emotions that's acquired through repetition, a habit is that we've done something so many times that your buddy now knows how to do it better than your mind.","title":"What's a habit ?"},{"location":"other/habit/#change","text":"The moment you decide to do the change, the moment you decide to make a different choice than what your body/subconscious used to GET READY because it's going to feel UNCOMFORTABLE.","title":"Change ?"},{"location":"other/habit/#8-habits-you-should-practice-at-least-once-a-week","text":"Link: https://www.youtube.com/watch?v=_NO1ldvMDXQ Call/Text someone you care about and didn't catch up for a moment Do an all-out workout Review the productivity system, habit trackers, notes .... Do a quick financial health check-up Plan and lead a high-density fun activity Budgeting: https://www.youtube.com/redirect?q=https%3A%2F%2Fcollegeinfogeek.com%2Fbudget-without-cutting-coffee%2F&redir_token=hNZrSJkCW9PBqaJYwvyCLL0j2kV8MTU4ODYxOTU0MEAxNTg4NTMzMTQw&v=_NO1ldvMDXQ&event=video_description","title":"8 Habits You Should Practice at Least Once a Week"},{"location":"other/install_python_from_source/","text":"Inspired from: https://gist.github.com/jerblack/798718c1910ccdd4ede92481229043be sudo apt install build-essential libssl-dev zlib1g-dev libncurses5-dev libncursesw5-dev libreadline-dev libsqlite3-dev libgdbm-dev libdb5.3-dev libbz2-dev libexpat1-dev liblzma-dev tk-dev libffi-dev export PYTHON_VERSION = 3 .7.5 wget https://www.python.org/ftp/python/ $PYTHON_VERSION /Python- $PYTHON_VERSION .tar.xz tar xvf Python- $PYTHON_VERSION .tar.xz cd Python- $PYTHON_VERSION ./configure --enable-optimizations # 'make -j <x>' enables parallel execution of <x> make recipes simultaneously # Warning: I skipped it because it took too long sudo make -j 8 # altinstall does not alter original system python install sudo make altinstall","title":"Install python from source"},{"location":"other/jupyterlab/","text":"JupyterLab Extensions Common Extension Points","title":"JupyterLab Extensions"},{"location":"other/jupyterlab/#jupyterlab-extensions","text":"","title":"JupyterLab Extensions"},{"location":"other/jupyterlab/#common-extension-points","text":"","title":"Common Extension Points"},{"location":"other/kubernets/","text":"Kubernets Install Minikube: curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mkdir -p /usr/local/bin/ && sudo mv minikube /usr/local/bin/minikube && sudo ln -s /bin/minikube /usr/local/bin/minikube Kubectl: curl -LO https://storage.googleapis.com/kubernetes-release/release/ curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt /bin/linux/amd64/kubectl && chmod +x ./kubectl && sudo mv ./kubectl /usr/local/bin/kubectl && sudo ln -s /usr/local/bin/kubectl /bin/kubectl Autocomplete kubectl: echo \"if [ $commands[kubectl] ]; then source <(kubectl completion zsh); fi\" >> ~/.zshrc or echo \"source <(kubectl completion bash)\" >> ~/.bashrc Helm: wget --continue https://get.helm.sh/helm-v3.0.3-linux-amd64.tar.gz && tar xvf helm-v3.0.3-linux-amd64.tar.gz && cd linux-amd64 && chmod +x ./helm && sudo mv helm /usr/local/bin && sudo ln -s /usr/local/bin/helm /bin/helm j Install seldon-core kubectl create namespace seldon-system helm install seldon-core seldon-core-operator --repo https://storage.googleapis.com/seldon-charts --set usageMetrics.enabled=true --namespace seldon-system Seldon core analytics Seldon Core provides an example Helm analytics chart that displays the above Prometheus metrics in Grafana. You can install it with: helm install seldon-core-analytics seldon-core-analytics \\ --repo https://storage.googleapis.com/seldon-charts \\ --set grafana_prom_admin_password = password \\ --set persistence.enabled = false \\ --namespace seldon-system The available parameters are: grafana_prom_admin_password : The admin user Grafana password to use. persistence.enabled : Whether Prometheus persistence is enabled. Once running you can expose the Grafana dashboard with: kubectl port-forward $( kubectl get pods -n seldon-system -l app = grafana-prom-server -o jsonpath = '{.items[0].metadata.name}' ) 3000 :3000 -n seldon-system You can then view the dashboard at http://localhost:3000/dashboard/db/prediction-analytics?refresh=5s&orgId=1 Install ambassador From seldon.io helm repo add stable https://kubernetes-charts.storage.googleapis.com/ helm repo update helm install ambassador stable/ambassador --set crds.keep=false From official website helm repo add datawire https://www.getambassador.io kubectl create namespace ambassador helm install --name ambassador --namespace ambassador datawire/ambassador Install source-to-image wget --continue https://github.com/openshift/source-to-image/releases/download/v1.2.0/source-to-image-v1.2.0-2a579ecd-linux-amd64.tar.gz tar xvf source-to-image-v1.2.0-2a579ecd-linux-amd64.tar.gz sudo cp s2i sti /usr/local/bin/ sudo ln -s /usr/local/bin/s2i /bin/s2i sudo ln -s /usr/local/bin/sti /bin/sti","title":"Kubernets"},{"location":"other/kubernets/#kubernets","text":"","title":"Kubernets"},{"location":"other/kubernets/#install","text":"Minikube: curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mkdir -p /usr/local/bin/ && sudo mv minikube /usr/local/bin/minikube && sudo ln -s /bin/minikube /usr/local/bin/minikube Kubectl: curl -LO https://storage.googleapis.com/kubernetes-release/release/ curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt /bin/linux/amd64/kubectl && chmod +x ./kubectl && sudo mv ./kubectl /usr/local/bin/kubectl && sudo ln -s /usr/local/bin/kubectl /bin/kubectl Autocomplete kubectl: echo \"if [ $commands[kubectl] ]; then source <(kubectl completion zsh); fi\" >> ~/.zshrc or echo \"source <(kubectl completion bash)\" >> ~/.bashrc Helm: wget --continue https://get.helm.sh/helm-v3.0.3-linux-amd64.tar.gz && tar xvf helm-v3.0.3-linux-amd64.tar.gz && cd linux-amd64 && chmod +x ./helm && sudo mv helm /usr/local/bin && sudo ln -s /usr/local/bin/helm /bin/helm j","title":"Install"},{"location":"other/kubernets/#install-seldon-core","text":"kubectl create namespace seldon-system helm install seldon-core seldon-core-operator --repo https://storage.googleapis.com/seldon-charts --set usageMetrics.enabled=true --namespace seldon-system","title":"Install seldon-core"},{"location":"other/kubernets/#seldon-core-analytics","text":"Seldon Core provides an example Helm analytics chart that displays the above Prometheus metrics in Grafana. You can install it with: helm install seldon-core-analytics seldon-core-analytics \\ --repo https://storage.googleapis.com/seldon-charts \\ --set grafana_prom_admin_password = password \\ --set persistence.enabled = false \\ --namespace seldon-system The available parameters are: grafana_prom_admin_password : The admin user Grafana password to use. persistence.enabled : Whether Prometheus persistence is enabled. Once running you can expose the Grafana dashboard with: kubectl port-forward $( kubectl get pods -n seldon-system -l app = grafana-prom-server -o jsonpath = '{.items[0].metadata.name}' ) 3000 :3000 -n seldon-system You can then view the dashboard at http://localhost:3000/dashboard/db/prediction-analytics?refresh=5s&orgId=1","title":"Seldon core analytics"},{"location":"other/kubernets/#install-ambassador","text":"","title":"Install ambassador"},{"location":"other/kubernets/#from-seldonio","text":"helm repo add stable https://kubernetes-charts.storage.googleapis.com/ helm repo update helm install ambassador stable/ambassador --set crds.keep=false","title":"From seldon.io"},{"location":"other/kubernets/#from-official-website","text":"helm repo add datawire https://www.getambassador.io kubectl create namespace ambassador helm install --name ambassador --namespace ambassador datawire/ambassador","title":"From official website"},{"location":"other/kubernets/#install-source-to-image","text":"wget --continue https://github.com/openshift/source-to-image/releases/download/v1.2.0/source-to-image-v1.2.0-2a579ecd-linux-amd64.tar.gz tar xvf source-to-image-v1.2.0-2a579ecd-linux-amd64.tar.gz sudo cp s2i sti /usr/local/bin/ sudo ln -s /usr/local/bin/s2i /bin/s2i sudo ln -s /usr/local/bin/sti /bin/sti","title":"Install source-to-image"},{"location":"other/lessons/","text":"02 Mars 2020 Think twice: don't let ambition and being exciting about things makes you miss some details or just think it could be done without thinking of an actual plan. Mistake: I was thinking that It's possible to learn kids of 12-13 years old how to code in Java (variables, if, for, arrays) in 6-7 hours just because they have some basic scratch programming skills (about 20 hours), I was optimistic & excited so that I miss the fact that 20 hours will never be enough to grasp all the subtleties about variable assignments, condition, logic expressions, for loops are more sophisticated that just \"repeat\" block ... etc Never give anything without an official agreement paper. Mistake: We was about to sponsor DigitUs 2020 organized by Club Solei HEC, we gived them money (10m which was a big amount of that time for us) without any paper that states that we gived them that, we then wanted to rescind the agreement but we was frustrated on how could we be protected and how to ensure our money get back to us, it we had an agreement we wouldn't be that frustrated. Never teach technical and non-technical students alongside","title":"02 Mars 2020"},{"location":"other/lessons/#02-mars-2020","text":"Think twice: don't let ambition and being exciting about things makes you miss some details or just think it could be done without thinking of an actual plan. Mistake: I was thinking that It's possible to learn kids of 12-13 years old how to code in Java (variables, if, for, arrays) in 6-7 hours just because they have some basic scratch programming skills (about 20 hours), I was optimistic & excited so that I miss the fact that 20 hours will never be enough to grasp all the subtleties about variable assignments, condition, logic expressions, for loops are more sophisticated that just \"repeat\" block ... etc Never give anything without an official agreement paper. Mistake: We was about to sponsor DigitUs 2020 organized by Club Solei HEC, we gived them money (10m which was a big amount of that time for us) without any paper that states that we gived them that, we then wanted to rescind the agreement but we was frustrated on how could we be protected and how to ensure our money get back to us, it we had an agreement we wouldn't be that frustrated. Never teach technical and non-technical students alongside","title":"02 Mars 2020"},{"location":"other/linux/","text":"Capabilities Special permissions, example: running chown. man capabilities for more info. For the purpose of performing permission checks, traditional UNIX implementations distinguish two categories of processes: privileged processes (whose effective user ID is 0, referred to as superuser or root), and unprivileged processes (whose effective UID is nonzero). Privileged processes bypass all kernel permission checks, while unprivileged processes are subject to full permission checking based on the process's credentials (usually: effective UID, effective GID, and supplementary group list). Starting with kernel 2.2, Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled. Capabilities are a per-thread attribute. Signals SIGINT : interrupt, tell it to kill itself, the process can catch it and shutdown gracefully, terminal sends it when hitting Ctrl-C SIGTERM : terminate, tell it to kill itself gracefully or not SIGKILL : kill, kill the process, the process can't catch it so it can't cleanup before being killed SIGSTOP : stop or pause the process, terminal sends it when hitting Ctrl-Z. The shell uses pausing (and its counterpart, resuming via SIGCONT) to implement job control Relation Now that we understand more about signals, we can see how they relate to each other. The default action for SIGINT, SIGTERM, SIGQUIT, and SIGKILL is to terminate the process. However, SIGTERM, SIGQUIT, and SIGKILL are defined as signals to terminate the process, but SIGINT is defined as an interruption requested by the user. In particular, if we send SIGINT (or press Ctrl+C) depending on the process and the situation it can behave differently. So, we shouldn\u2019t depend solely on SIGINT to finish a process. As SIGINT is intended as a signal sent by the user, usually the processes communicate with each other using other signals. For instance, a parent process usually sends SIGTERM to its children to terminate them, even if SIGINT has the same effect. In the case of SIGQUIT, it generates a core dump which is useful for debugging. Now that we have this in mind, we can see we should choose SIGTERM on top of SIGKILL to terminate a process. SIGTERM is the preferred way as the process has the chance to terminate gracefully. As a process can override the default action for SIGINT, SIGTERM, and SIGQUIT, it can be the case that neither of them finishes the process. Also, if the process is hung it may not respond to any of those signals. In that case, we have SIGKILL as the last resort to terminate the process. Extra Root user can do everything, he lives in the kernel space and the normal capabilities, permissions ... don't apply to him /proc/sysrq-trigger: interface that gives access to the linux kernel, it allow us to communicate with kernel and tell him to do very low level stuff. Example Crash my system: echo c >/proc/sysrq-trigger . You can use it to test the reliability of a high available system -","title":"Linux"},{"location":"other/linux/#capabilities","text":"Special permissions, example: running chown. man capabilities for more info. For the purpose of performing permission checks, traditional UNIX implementations distinguish two categories of processes: privileged processes (whose effective user ID is 0, referred to as superuser or root), and unprivileged processes (whose effective UID is nonzero). Privileged processes bypass all kernel permission checks, while unprivileged processes are subject to full permission checking based on the process's credentials (usually: effective UID, effective GID, and supplementary group list). Starting with kernel 2.2, Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled. Capabilities are a per-thread attribute.","title":"Capabilities"},{"location":"other/linux/#signals","text":"SIGINT : interrupt, tell it to kill itself, the process can catch it and shutdown gracefully, terminal sends it when hitting Ctrl-C SIGTERM : terminate, tell it to kill itself gracefully or not SIGKILL : kill, kill the process, the process can't catch it so it can't cleanup before being killed SIGSTOP : stop or pause the process, terminal sends it when hitting Ctrl-Z. The shell uses pausing (and its counterpart, resuming via SIGCONT) to implement job control","title":"Signals"},{"location":"other/linux/#relation","text":"Now that we understand more about signals, we can see how they relate to each other. The default action for SIGINT, SIGTERM, SIGQUIT, and SIGKILL is to terminate the process. However, SIGTERM, SIGQUIT, and SIGKILL are defined as signals to terminate the process, but SIGINT is defined as an interruption requested by the user. In particular, if we send SIGINT (or press Ctrl+C) depending on the process and the situation it can behave differently. So, we shouldn\u2019t depend solely on SIGINT to finish a process. As SIGINT is intended as a signal sent by the user, usually the processes communicate with each other using other signals. For instance, a parent process usually sends SIGTERM to its children to terminate them, even if SIGINT has the same effect. In the case of SIGQUIT, it generates a core dump which is useful for debugging. Now that we have this in mind, we can see we should choose SIGTERM on top of SIGKILL to terminate a process. SIGTERM is the preferred way as the process has the chance to terminate gracefully. As a process can override the default action for SIGINT, SIGTERM, and SIGQUIT, it can be the case that neither of them finishes the process. Also, if the process is hung it may not respond to any of those signals. In that case, we have SIGKILL as the last resort to terminate the process.","title":"Relation"},{"location":"other/linux/#extra","text":"Root user can do everything, he lives in the kernel space and the normal capabilities, permissions ... don't apply to him /proc/sysrq-trigger: interface that gives access to the linux kernel, it allow us to communicate with kernel and tell him to do very low level stuff. Example Crash my system: echo c >/proc/sysrq-trigger . You can use it to test the reliability of a high available system -","title":"Extra"},{"location":"other/marketing/","text":"Traffic types Cold Warm Hot Strategy should work on converting people down the types (Cold -> Warm -> Hot) Media types: Own media Paid media User generated media: take consideration of content to be shearable Earned media: those who talk of you because you're so good :D Tony robinson Dean graziosi The one dolar strategy Lead generation: offer them free things to incist them to subscribe. Email Marketing https://youtu.be/XlNVHBVng2I Prune email from subscribers who don't read Review subscribers activity Facebook audience insights Test another email provider don't beg for email subscribing, not too litmus.com / mail-tester to test for spam filters Sign up for auto-responders One dollar strategy Link: https://app.hubspot.com/academy/6940115/lessons/239/1329 What makes a winning video it speaks to where your audience is in the buyer's journey (awareness -> engagement -> conversion). Awareness: they don't know you, how you can be benifical to him (make it emotional and patien). Lead with a hook in the first few seconds . Make it short. Design it to be viewed without sound :o ! Why: the problem we agree with How: how sereel solves this, our service","title":"Marketing"},{"location":"other/marketing/#traffic-types","text":"Cold Warm Hot Strategy should work on converting people down the types (Cold -> Warm -> Hot)","title":"Traffic types"},{"location":"other/marketing/#media-types","text":"Own media Paid media User generated media: take consideration of content to be shearable Earned media: those who talk of you because you're so good :D Tony robinson Dean graziosi The one dolar strategy Lead generation: offer them free things to incist them to subscribe.","title":"Media types:"},{"location":"other/marketing/#email-marketing","text":"","title":"Email Marketing"},{"location":"other/marketing/#httpsyoutubexlnvhbvng2i","text":"Prune email from subscribers who don't read Review subscribers activity Facebook audience insights Test another email provider don't beg for email subscribing, not too litmus.com / mail-tester to test for spam filters Sign up for auto-responders","title":"https://youtu.be/XlNVHBVng2I"},{"location":"other/marketing/#one-dollar-strategy","text":"Link: https://app.hubspot.com/academy/6940115/lessons/239/1329","title":"One dollar strategy"},{"location":"other/marketing/#what-makes-a-winning-video","text":"it speaks to where your audience is in the buyer's journey (awareness -> engagement -> conversion). Awareness: they don't know you, how you can be benifical to him (make it emotional and patien). Lead with a hook in the first few seconds . Make it short. Design it to be viewed without sound :o ! Why: the problem we agree with How: how sereel solves this, our service","title":"What makes a winning video"},{"location":"other/minio/","text":"Grant access to models bucket for my user Get the policy from pravda-dev pravda @pravda - dev : ~ $ mc admin policy info minio models { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:*\" ] , \"Resource\" : [ \"arn:aws:s3:::models\", \"arn:aws:s3:::models/*\" ] } ] } Copy the output to a JSON file Create the policy in my computer mc admin policy add minio models models-policy.json Add my user to models-bucket-masters group mc admin group add minio models-bucket-masters haroune Attribute the policy to the group mc admin policy set minio models group = models-bucket-masters","title":"Grant access to models bucket for my user"},{"location":"other/minio/#grant-access-to-models-bucket-for-my-user","text":"Get the policy from pravda-dev pravda @pravda - dev : ~ $ mc admin policy info minio models { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:*\" ] , \"Resource\" : [ \"arn:aws:s3:::models\", \"arn:aws:s3:::models/*\" ] } ] } Copy the output to a JSON file Create the policy in my computer mc admin policy add minio models models-policy.json Add my user to models-bucket-masters group mc admin group add minio models-bucket-masters haroune Attribute the policy to the group mc admin policy set minio models group = models-bucket-masters","title":"Grant access to models bucket for my user"},{"location":"other/mlflow/","text":"Concepts a Run records: - Code Version - Start & End time - Source (the file used to launch, or project) - Parameters - Metrics - Artifacts A set of Runs could be organized into an Experiment Where Runs Are Recorded Local file path specified as file:/my/local/dir Database encoded as <dialect>+<driver>://<username>:<password>@<host>:<port>/<database> HTTP server specified as https://my-server:5000 Databricks workspace Logging Data to Runs mlflow.set_tracking_uri() connects to a tracking URI. You can also set the MLFLOW_TRACKING_URI (See above for \"Where Runs Are Recorded\") mlflow.tracking.get_tracking_uri() returns the current tracking URI. mlflow.create_experiment() creates a new experiment and returns its ID. Runs can be launched under the experiment by passing the experiment ID to mlflow.start_run . mlflow.set_experiment() sets an experiment as active. If the experiment does not exist, creates a new experiment. If you do not specify an experiment in mlflow.start_run() , new runs are launched under this experiment. mlflow.start_run() returns the currently active run (if one exists), or starts a new run and returns a mlflow.ActiveRun object usable as a context manager for the current run. You do not need to call start_run explicitly: calling one of the logging functions with no active run automatically starts a new one. mlflow.end_run() ends the currently active run, if any, taking an optional run status. mlflow.active_run() returns a mlflow.entities.Run object corresponding to the currently active run, if any. mlflow.log_param() logs a single key-value param in the currently active run. The key and value are both strings. Use mlflow.log_params() to log multiple params at once. mlflow.log_metric() logs a single key-value metric. The value must always be a number. MLflow remembers the history of values for each metric. Use mlflow.log_metrics() to log multiple metrics at once. mlflow.set_tag() sets a single key-value tag in the currently active run. The key and value are both strings. Use mlflow.set_tags() to set multiple tags at once. mlflow.log_artifact() logs a local file or directory as an artifact, optionally taking an artifact_path to place it in within the run\u2019s artifact URI. Run artifacts can be organized into directories, so you can place the artifact in a directory this way. mlflow.log_artifacts() logs all the files in a given directory as artifacts, again taking an optional artifact_path. mlflow.get_artifact_uri() returns the URI that artifacts from the current run should be logged to.","title":"Mlflow"},{"location":"other/mlflow/#concepts","text":"a Run records: - Code Version - Start & End time - Source (the file used to launch, or project) - Parameters - Metrics - Artifacts A set of Runs could be organized into an Experiment","title":"Concepts"},{"location":"other/mlflow/#where-runs-are-recorded","text":"Local file path specified as file:/my/local/dir Database encoded as <dialect>+<driver>://<username>:<password>@<host>:<port>/<database> HTTP server specified as https://my-server:5000 Databricks workspace","title":"Where Runs Are Recorded"},{"location":"other/mlflow/#logging-data-to-runs","text":"mlflow.set_tracking_uri() connects to a tracking URI. You can also set the MLFLOW_TRACKING_URI (See above for \"Where Runs Are Recorded\") mlflow.tracking.get_tracking_uri() returns the current tracking URI. mlflow.create_experiment() creates a new experiment and returns its ID. Runs can be launched under the experiment by passing the experiment ID to mlflow.start_run . mlflow.set_experiment() sets an experiment as active. If the experiment does not exist, creates a new experiment. If you do not specify an experiment in mlflow.start_run() , new runs are launched under this experiment. mlflow.start_run() returns the currently active run (if one exists), or starts a new run and returns a mlflow.ActiveRun object usable as a context manager for the current run. You do not need to call start_run explicitly: calling one of the logging functions with no active run automatically starts a new one. mlflow.end_run() ends the currently active run, if any, taking an optional run status. mlflow.active_run() returns a mlflow.entities.Run object corresponding to the currently active run, if any. mlflow.log_param() logs a single key-value param in the currently active run. The key and value are both strings. Use mlflow.log_params() to log multiple params at once. mlflow.log_metric() logs a single key-value metric. The value must always be a number. MLflow remembers the history of values for each metric. Use mlflow.log_metrics() to log multiple metrics at once. mlflow.set_tag() sets a single key-value tag in the currently active run. The key and value are both strings. Use mlflow.set_tags() to set multiple tags at once. mlflow.log_artifact() logs a local file or directory as an artifact, optionally taking an artifact_path to place it in within the run\u2019s artifact URI. Run artifacts can be organized into directories, so you can place the artifact in a directory this way. mlflow.log_artifacts() logs all the files in a given directory as artifacts, again taking an optional artifact_path. mlflow.get_artifact_uri() returns the URI that artifacts from the current run should be logged to.","title":"Logging Data to Runs"},{"location":"other/productivity/","text":"The Mikhaila Peterson Podcast #8 - Chris Williamson (The productivity guy) Link: https://www.youtube.com/watch?v=xbIPC6K0D6o Things 3 App is very usefull: a To-do list App Getting things done by David Allen: is a personal productivity methodology that redefines how you approach your life and work. Chris william has an E-book for life hacks Brain is a thoughts engine but not a library, means we should get rid of thoughts that keeps poping up into our head to free up some space for the engine to work well Get your phone away from your bed when going to sleep. Get a sunlight alarm clock Build a structure of the begining of your day Don't touch phone just after waking up Eeat some lemon, salt, water: a coctail of adrelanil Get some sunlight exposure Have a cold shower Reset by tranining is much better than by sleeping Four points to consider for a morning routine: Move, Reflect, Learn and Move: get sunlihgt exposure and fresh air Refelect: Journling Artists pages by Julia Cameron: writing three pages of unbroken text The six minutes dairy: 3 questions in the morning, 3 others in the evening Meditation Headspace Sam haris waking up Insight timer is the most used meditation App Release into now the best course ever on mediation Books Atom habits by James clear Indestuctible by near al: how to focus on one task Deep work by Cal Newport Essentialism by Greg mcEwan Man's search for meaning by Viktor Frankl The wave of the superior man by David Deida How to date women modals by Marc Manson","title":"The Mikhaila Peterson Podcast #8 - Chris Williamson (The productivity guy)"},{"location":"other/productivity/#the-mikhaila-peterson-podcast-8-chris-williamson-the-productivity-guy","text":"Link: https://www.youtube.com/watch?v=xbIPC6K0D6o Things 3 App is very usefull: a To-do list App Getting things done by David Allen: is a personal productivity methodology that redefines how you approach your life and work. Chris william has an E-book for life hacks","title":"The Mikhaila Peterson Podcast #8 - Chris Williamson (The productivity guy)"},{"location":"other/productivity/#brain-is-a-thoughts-engine-but-not-a-library-means-we-should-get-rid-of-thoughts-that-keeps-poping-up-into-our-head-to-free-up-some-space-for-the-engine-to-work-well","text":"Get your phone away from your bed when going to sleep. Get a sunlight alarm clock Build a structure of the begining of your day Don't touch phone just after waking up Eeat some lemon, salt, water: a coctail of adrelanil Get some sunlight exposure Have a cold shower Reset by tranining is much better than by sleeping Four points to consider for a morning routine: Move, Reflect, Learn and Move: get sunlihgt exposure and fresh air Refelect: Journling Artists pages by Julia Cameron: writing three pages of unbroken text The six minutes dairy: 3 questions in the morning, 3 others in the evening Meditation Headspace Sam haris waking up Insight timer is the most used meditation App Release into now the best course ever on mediation Books Atom habits by James clear Indestuctible by near al: how to focus on one task Deep work by Cal Newport Essentialism by Greg mcEwan Man's search for meaning by Viktor Frankl The wave of the superior man by David Deida How to date women modals by Marc Manson","title":"Brain is a thoughts engine but not a library, means we should get rid of thoughts that keeps poping up into our head to free up some space for the engine to work well"},{"location":"other/python_workshop/","text":"pep8 metaprogramming variable unpacking try, execpt, else context manager extend import functionality decorator Saving Memory with __slots__ Controlling What Can Be Imported and What Not with ' all ' Comparison Operators the Easy Way with functools.total_oredering Sliceable namedtuple logging requests click or argparser defaultdict comprehensions sum, min, max, any, all descriptors Context manager class Connection : def __init__ ( self ): ... def __enter__ ( self ): # Initialize connection... def __exit__ ( self , type , value , traceback ): # Close connection... with Connection () as c : # __enter__() executes ... # conn.__exit__() executes from contextlib import contextmanager @contextmanager def tag ( name ): print ( f \"< { name } >\" ) yield print ( f \"</ { name } >\" ) with tag ( \"h1\" ): print ( \"This is Title.\" )","title":"Python workshop"},{"location":"other/python_workshop/#context-manager","text":"class Connection : def __init__ ( self ): ... def __enter__ ( self ): # Initialize connection... def __exit__ ( self , type , value , traceback ): # Close connection... with Connection () as c : # __enter__() executes ... # conn.__exit__() executes from contextlib import contextmanager @contextmanager def tag ( name ): print ( f \"< { name } >\" ) yield print ( f \"</ { name } >\" ) with tag ( \"h1\" ): print ( \"This is Title.\" )","title":"Context manager"},{"location":"other/resizegpc/","text":"Resize GCP disk This will be a TL;DR of this documentation Resize the disk from the \"Disks\" interface or using gcloud Tell the system Get the [DEVICE_ID] and [DEVICE_PARTITION] of the disk using: sudo df -h and sudo lsblk . Example: /dev/sda1 means: [DEVICE_ID]=1 and [DEVICE_PARTITION]=/dev/sda sudo apt -y install cloud-guest-utils sudo growpart /dev/[DEVICE_ID] [PARTITION_NUMBER] sudo resize2fs /dev/[DEVICE_ID][PARTITION_NUMBER]","title":"Resize GCP disk"},{"location":"other/resizegpc/#resize-gcp-disk","text":"This will be a TL;DR of this documentation Resize the disk from the \"Disks\" interface or using gcloud Tell the system Get the [DEVICE_ID] and [DEVICE_PARTITION] of the disk using: sudo df -h and sudo lsblk . Example: /dev/sda1 means: [DEVICE_ID]=1 and [DEVICE_PARTITION]=/dev/sda sudo apt -y install cloud-guest-utils sudo growpart /dev/[DEVICE_ID] [PARTITION_NUMBER] sudo resize2fs /dev/[DEVICE_ID][PARTITION_NUMBER]","title":"Resize GCP disk"},{"location":"other/seldon/","text":"Seldon.io A talk at Spark+AI Summit: https://www.youtube.com/watch?v=D6eSfd9w9eA Working with Bigmama last couple of years I did noticed that one of the main problems of machine learning projects is Deployement (As it is for Software Engineering Projects). Deployement means: make the models accessible to end users. Many solutions have approached that specific problem: mlflow, corext-ai, seldon.io We'll try to explain what makes seldon.io a good solution for machine learning deployments.","title":"Seldon.io"},{"location":"other/seldon/#seldonio","text":"A talk at Spark+AI Summit: https://www.youtube.com/watch?v=D6eSfd9w9eA Working with Bigmama last couple of years I did noticed that one of the main problems of machine learning projects is Deployement (As it is for Software Engineering Projects). Deployement means: make the models accessible to end users. Many solutions have approached that specific problem: mlflow, corext-ai, seldon.io We'll try to explain what makes seldon.io a good solution for machine learning deployments.","title":"Seldon.io"},{"location":"other/self-esteem/","text":"7 Simple Ways to Boost Your Self-Esteem Link: https://www.youtube.com/watch?v=x4NRIdDmtLw If you feel confused about your identity, you naturally lean to self criticism because in your head, someone who value themselves would know the reasons behind. Record your story: write down you stories will help remember what you're good at and what makes you happy, it may also help prove that you've done great things in the past if you are in low self esteem situation in the future. And while you're writing you will force yourself to be clear, specific and concrete, this will help eradicate eventual confusion A point of pride: set some legitimate and small goals that when accomplished you prove something to yourself, you'll be more confident and motivated by those small goals. Studying your self: you shouldn't under esteem yourself when talking Learning how to forgive your own mistakes: it's not about preserving the good it's about accepting and forgiving the bad Revisit your past: remember times when you thought you couldn't do something and you did it, remember the happiest moments of your life, try to understand what makes you happy, what changed and what makes it the way it was, also revisiting the past could be a chance to discover something new about yourself. Go your own way: don't try to fit in it will make your self esteem lower and lower, stop caring altogether, build your own path. Reclaim your values: remember yourself you what's the most important things for you, what makes your life worth while.","title":"Self esteem"},{"location":"other/self-esteem/#7-simple-ways-to-boost-your-self-esteem","text":"Link: https://www.youtube.com/watch?v=x4NRIdDmtLw If you feel confused about your identity, you naturally lean to self criticism because in your head, someone who value themselves would know the reasons behind. Record your story: write down you stories will help remember what you're good at and what makes you happy, it may also help prove that you've done great things in the past if you are in low self esteem situation in the future. And while you're writing you will force yourself to be clear, specific and concrete, this will help eradicate eventual confusion A point of pride: set some legitimate and small goals that when accomplished you prove something to yourself, you'll be more confident and motivated by those small goals. Studying your self: you shouldn't under esteem yourself when talking Learning how to forgive your own mistakes: it's not about preserving the good it's about accepting and forgiving the bad Revisit your past: remember times when you thought you couldn't do something and you did it, remember the happiest moments of your life, try to understand what makes you happy, what changed and what makes it the way it was, also revisiting the past could be a chance to discover something new about yourself. Go your own way: don't try to fit in it will make your self esteem lower and lower, stop caring altogether, build your own path. Reclaim your values: remember yourself you what's the most important things for you, what makes your life worth while.","title":"7 Simple Ways to Boost Your Self-Esteem"},{"location":"other/sleep/","text":"How to Get to Bed on Time and Stop Losing Sleep - College Info Geek Link: https://www.youtube.com/watch?v=LwLIQ_oC7Lo Set a pre-established bed time, build a habit of bed time try to wake up in the end of the sleep cycle, use https://sleepyti.me to calculate Sleep is regulated by : sleep-wake homeostatic process: internal timer, it records when you slept and when you awaked last circadian clock: it sinks the biological process with day-night cycle, means tells the minds to turn of when it's night (there is no light) create a wind-down ritual: some routine to go to bed, it could be having a shower, meditation, reading, pray, reading holy Quran ... Have an alarm for it Track your progress Stop using internet, use tools to stop you it's hard for you Remind yourself how great it feels to wake up naturally, before the alarm goes off, without sickening jolt into wakefulness. Then when you're surfing the internet at 11:30 pm, ask yourself, 'Am I making a good trade-off?' - Gretchen Rubin author of the happiness project and better than before Humans tend to discount the value of the future rewards and increase the value of the current reward, that's where procrastination comes from I think Going to 20 minutes walk when feeling tired when you wake up Expose yourself to sun","title":"Sleep"},{"location":"other/sleep/#how-to-get-to-bed-on-time-and-stop-losing-sleep-college-info-geek","text":"Link: https://www.youtube.com/watch?v=LwLIQ_oC7Lo Set a pre-established bed time, build a habit of bed time try to wake up in the end of the sleep cycle, use https://sleepyti.me to calculate Sleep is regulated by : sleep-wake homeostatic process: internal timer, it records when you slept and when you awaked last circadian clock: it sinks the biological process with day-night cycle, means tells the minds to turn of when it's night (there is no light) create a wind-down ritual: some routine to go to bed, it could be having a shower, meditation, reading, pray, reading holy Quran ... Have an alarm for it Track your progress Stop using internet, use tools to stop you it's hard for you Remind yourself how great it feels to wake up naturally, before the alarm goes off, without sickening jolt into wakefulness. Then when you're surfing the internet at 11:30 pm, ask yourself, 'Am I making a good trade-off?' - Gretchen Rubin author of the happiness project and better than before Humans tend to discount the value of the future rewards and increase the value of the current reward, that's where procrastination comes from I think Going to 20 minutes walk when feeling tired when you wake up Expose yourself to sun","title":"How to Get to Bed on Time and Stop Losing Sleep - College Info Geek"},{"location":"other/therapy/","text":"Books Recommended by a Reddit guy on r/DecidingToBeBetter : OP: Depression for Dummies Anxiety and Depression Workbook for Dummies The CBT Handbook Understand Psychology: How Your Mind Works and Why You Do the Things You Do Any Intro to Psychology book Comments: https://www.goodreads.com/book/show/65325.Safe_People https://www.goodreads.com/book/show/944267.Boundaries https://www.cci.health.wa.gov.au/Resources/Looking-After-Yourself Dr David Burns called \"Feeling Good\" The Science of Well-Being on coursera https://www.scribd.com/document/348546452/Releasing-Anger-Inner-Child-Scripts Where can I find them ? https://www.getselfhelp.co.uk/freedownloads2.htm https://www.therapistaid.com/","title":"Therapy"},{"location":"other/therapy/#books","text":"Recommended by a Reddit guy on r/DecidingToBeBetter : OP: Depression for Dummies Anxiety and Depression Workbook for Dummies The CBT Handbook Understand Psychology: How Your Mind Works and Why You Do the Things You Do Any Intro to Psychology book Comments: https://www.goodreads.com/book/show/65325.Safe_People https://www.goodreads.com/book/show/944267.Boundaries https://www.cci.health.wa.gov.au/Resources/Looking-After-Yourself Dr David Burns called \"Feeling Good\" The Science of Well-Being on coursera https://www.scribd.com/document/348546452/Releasing-Anger-Inner-Child-Scripts Where can I find them ? https://www.getselfhelp.co.uk/freedownloads2.htm https://www.therapistaid.com/","title":"Books"},{"location":"other/time/","text":"Time management How to Achieve Your Most Ambitious Goals | Stephen Duneier | TEDxTucson Link: https://www.youtube.com/watch?v=TQMbvJNRpLE The whole thing is to take big ambitious things and split them out to tiny manageable tasks, 5-10 minutes of focus and keep the rythme. as the prophet mohammed said: \"\u062e\u064a\u0631 \u0627\u0644\u0623\u0639\u0645\u0627\u0644 \u0623\u062f\u0648\u0645\u0647\u0627 \u0648 \u0625\u0646 \u0642\u0644\" How To Multiply Your Time | Rory Vaden | TEDxDouglasville Link: https://www.youtube.com/watch?v=y2X7c9TUQJ8 The whole thing is to do things that creates more time. The focus tunnel: Tasks -> Eliminate (Say no) -> Automate -> Delegate -> me -[1. Now | 2. Later (Go to the first step)] You multiply your time by giving yourself the emotional permission to spend time on things today that will give you more time tomorrow How to Get Your Brain to Focus | Chris Bailey | TEDxManchester Link: https://www.youtube.com/watch?v=Hu4Yvq-g7_Y He just through up his phone for a month as an experiment -> Just stop it the hard way. * He can focus more easily * He had more ideas, more plans and thoughts about the future In this world of distractions, researches found that we only focus for 40s before getting distracted by something else. Our brains are distracted (over stimulated, we crave distractions): our brains loves doing this. \"novelty bias\": we get dopamine when we check Facebook the same we get it when we do love, eat pizza .. etc Free your mind from phone and do physicall stuff pushes it to think more and more ideas will cam up to your mind. \"Over stimulation is the enemy of focus\" but not \"Distraction\", instead \"Distraction\" is a symptom of \"Over stimulation\" How to focus intensly Link: https://www.youtube.com/watch?v=wfKv2qG8d_w What does it means to not focus there is a difference between where an individual wants to direct their attention and where it's actually being directed. Why ? Stress: we should deal with stress first to be able to focus better. Extrinsic pleasure We don't see the future pleasure of hard work How ? Habit #1: abstaining from short-term pleasure seeking. Meditation will help you manage boredom and not be bothered by it. Deep work. Habit #2: Not to do list. Habit #3: Controlled amount of pleasure. Control media: Select time in the day when to check social media Having a quit time: knowing when to end helps you work hard for it, after quitting don't set any limit to your short-term pleasure Pomodoro technique Habit #4: Priority list A quote by steve jobs: People think focus means saying yes to the thing you've got to focus on. But that's not what it means at all. It means saying no to the hundred other good ideas that there are. You have to pick carefully. I'm actually as proud of the things we haven't done as the things I have done. Innovation is saying no to 1,000 things. Jordan Peterson's Life Advice Will Change Your Future (MUST WATCH) Link: https://www.youtube.com/watch?v=wqEsTPaUZF0 When you're setting up your schedule try to negotiate with yourself as you do when you assign work to someone else that you want to be productive still not exhausted. How to Stop Wasting Time - 5 Useful Time Management Tips Link: https://www.youtube.com/watch?v=xwsLuxlbY2w We tend to not feel guild when we take too many commitement and having an overlay busy schedule then we ended up producing low quality result A busy life is a wasted life -- Francis Crick 5 tips: - Try using a time tracking tool, see: https://www.youtube.com/watch?v=p-AMooYPO1Y Toggle: it's a manual time tracker - Get clear on priorities How to prioritize ? - What does my schedule look like whitout this ? - When I'm on my death bed, will I regret not doing this ? - Have a list of what you're doing right now - Batch tasks: task that needs to go out together, taks that required low mental energey (cleaning, ...), Note that he's using Todoist, he used labels to batch tasks - Learn to say no:","title":"Time management"},{"location":"other/time/#time-management","text":"","title":"Time management"},{"location":"other/time/#how-to-achieve-your-most-ambitious-goals-stephen-duneier-tedxtucson","text":"Link: https://www.youtube.com/watch?v=TQMbvJNRpLE The whole thing is to take big ambitious things and split them out to tiny manageable tasks, 5-10 minutes of focus and keep the rythme. as the prophet mohammed said: \"\u062e\u064a\u0631 \u0627\u0644\u0623\u0639\u0645\u0627\u0644 \u0623\u062f\u0648\u0645\u0647\u0627 \u0648 \u0625\u0646 \u0642\u0644\"","title":"How to Achieve Your Most Ambitious Goals | Stephen Duneier | TEDxTucson"},{"location":"other/time/#how-to-multiply-your-time-rory-vaden-tedxdouglasville","text":"Link: https://www.youtube.com/watch?v=y2X7c9TUQJ8 The whole thing is to do things that creates more time. The focus tunnel: Tasks -> Eliminate (Say no) -> Automate -> Delegate -> me -[1. Now | 2. Later (Go to the first step)] You multiply your time by giving yourself the emotional permission to spend time on things today that will give you more time tomorrow","title":"How To Multiply Your Time | Rory Vaden | TEDxDouglasville"},{"location":"other/time/#how-to-get-your-brain-to-focus-chris-bailey-tedxmanchester","text":"Link: https://www.youtube.com/watch?v=Hu4Yvq-g7_Y He just through up his phone for a month as an experiment -> Just stop it the hard way. * He can focus more easily * He had more ideas, more plans and thoughts about the future In this world of distractions, researches found that we only focus for 40s before getting distracted by something else. Our brains are distracted (over stimulated, we crave distractions): our brains loves doing this. \"novelty bias\": we get dopamine when we check Facebook the same we get it when we do love, eat pizza .. etc Free your mind from phone and do physicall stuff pushes it to think more and more ideas will cam up to your mind. \"Over stimulation is the enemy of focus\" but not \"Distraction\", instead \"Distraction\" is a symptom of \"Over stimulation\"","title":"How to Get Your Brain to Focus | Chris Bailey | TEDxManchester"},{"location":"other/time/#how-to-focus-intensly","text":"Link: https://www.youtube.com/watch?v=wfKv2qG8d_w","title":"How to focus intensly"},{"location":"other/time/#what-does-it-means-to-not-focus","text":"there is a difference between where an individual wants to direct their attention and where it's actually being directed.","title":"What does it means to not focus"},{"location":"other/time/#why","text":"Stress: we should deal with stress first to be able to focus better. Extrinsic pleasure We don't see the future pleasure of hard work","title":"Why ?"},{"location":"other/time/#how","text":"Habit #1: abstaining from short-term pleasure seeking. Meditation will help you manage boredom and not be bothered by it. Deep work. Habit #2: Not to do list. Habit #3: Controlled amount of pleasure. Control media: Select time in the day when to check social media Having a quit time: knowing when to end helps you work hard for it, after quitting don't set any limit to your short-term pleasure Pomodoro technique Habit #4: Priority list A quote by steve jobs: People think focus means saying yes to the thing you've got to focus on. But that's not what it means at all. It means saying no to the hundred other good ideas that there are. You have to pick carefully. I'm actually as proud of the things we haven't done as the things I have done. Innovation is saying no to 1,000 things.","title":"How ?"},{"location":"other/time/#jordan-petersons-life-advice-will-change-your-future-must-watch","text":"Link: https://www.youtube.com/watch?v=wqEsTPaUZF0 When you're setting up your schedule try to negotiate with yourself as you do when you assign work to someone else that you want to be productive still not exhausted.","title":"Jordan Peterson's Life Advice Will Change Your Future (MUST WATCH)"},{"location":"other/time/#how-to-stop-wasting-time-5-useful-time-management-tips","text":"Link: https://www.youtube.com/watch?v=xwsLuxlbY2w We tend to not feel guild when we take too many commitement and having an overlay busy schedule then we ended up producing low quality result A busy life is a wasted life -- Francis Crick 5 tips: - Try using a time tracking tool, see: https://www.youtube.com/watch?v=p-AMooYPO1Y Toggle: it's a manual time tracker - Get clear on priorities How to prioritize ? - What does my schedule look like whitout this ? - When I'm on my death bed, will I regret not doing this ? - Have a list of what you're doing right now - Batch tasks: task that needs to go out together, taks that required low mental energey (cleaning, ...), Note that he's using Todoist, he used labels to batch tasks - Learn to say no:","title":"How to Stop Wasting Time - 5 Useful Time Management Tips"},{"location":"other/workshop/","text":"Title: Python for everyone Description: The workshop will get you started with the python programming language basics and you'll be able to use those basics to build some useful tasks. Audience: Programmers & Computer science students interested in python or just exploring different programming languages Prerequisites: basic programming skills: you should have written a couple of programs with any programming language. Speaker: Mohammedi Haroune Bio: Co-fondateur de Sereel & Software Engineer chez Big mama","title":"Workshop"},{"location":"programming/MLOps/","text":"Data scientists Pain Points Tracking: parameters, metrics and models Reproducibilty Tracebility The concept of DevOps in ML GCP AI Platform https://github.com/GoogleCloudPlatform/vertex-ai-samples Hyperparameter tunning hpt = hypertune . HyperTune () hpt . report_hyperparameter_tuning_metric ( hyperparameter_metric_tag = \u2019 loss \u2019 , metric_value = loss , global_step = epochs ) trainingInput : scaleTier : CUSTOM masterType : complex_model_m workerType : complex_model_m parameterServerType : large_model workerCount : 9 parameterServerCount : 3 hyperparameters : goal : MAXIMIZE hyperparameterMetricTag : loss maxTrials : 30 maxParallelTrials : 1 enableTrialEarlyStopping : True params : - parameterName : hidden1 type : INTEGER minValue : 40 maxValue : 400 scaleType : UNIT_LINEAR_SCALE - parameterName : numRnnCells type : DISCRETE discreteValues : - 1 - 2 - 3 - 4 - parameterName : rnnCellType type : CATEGORICAL categoricalValues : - BasicLSTMCell - BasicRNNCell - GRUCell - LSTMCell - LayerNormBasicLSTMCell Package the script into a docker container FROM gcr.io/deeplearning-platform-release/base-cpu RUN pip install -U fire cloudml-hypertune scikit-learn == 0 .20.4 pandas == 0 .24.2 WORKDIR /app COPY train.py . ENTRYPOINT [ \"python\" , \"train.py\" ] BigQuery bq query \\ -n 0 \\ --destination_table covertype_dataset.validation \\ --replace \\ --use_legacy_sql = false \\ 'SELECT * \\ FROM `covertype_dataset.covertype` as cover \\ WHERE \\ MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) = 0' bq extract \\ --destination_format CSV \\ covertype_dataset.validation \\ $VALIDATION_FILE_PATH KubeFlow prebuilt components import kfp URI = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/' component_store = kfp . components . ComponentStore ( local_search_paths = None , url_search_prefixes = [ URI ]) bigquery_query_op = component_store . load_component ( 'bigquery/query' ) mlengine_train_op = component_store . load_component ( 'ml_engine/train' ) mlengine_deploy_op = component_store . load_component ( 'ml_engine/deploy' ) create_testing_split = bigquery_query_op ( query = query , project_id = project_id , datset_id = dataset_id , table_id = '' , output_gcs_path = testing_file_path , dataset_location = dataset_location , ) # TO DO: Use the bigquery_query_op kfp submit run parameters should be in the same order as the function paramters","title":"MLOps"},{"location":"programming/MLOps/#data-scientists-pain-points","text":"Tracking: parameters, metrics and models Reproducibilty Tracebility","title":"Data scientists Pain Points"},{"location":"programming/MLOps/#the-concept-of-devops-in-ml","text":"","title":"The concept of DevOps in ML"},{"location":"programming/MLOps/#gcp-ai-platform","text":"https://github.com/GoogleCloudPlatform/vertex-ai-samples","title":"GCP AI Platform"},{"location":"programming/MLOps/#hyperparameter-tunning","text":"hpt = hypertune . HyperTune () hpt . report_hyperparameter_tuning_metric ( hyperparameter_metric_tag = \u2019 loss \u2019 , metric_value = loss , global_step = epochs ) trainingInput : scaleTier : CUSTOM masterType : complex_model_m workerType : complex_model_m parameterServerType : large_model workerCount : 9 parameterServerCount : 3 hyperparameters : goal : MAXIMIZE hyperparameterMetricTag : loss maxTrials : 30 maxParallelTrials : 1 enableTrialEarlyStopping : True params : - parameterName : hidden1 type : INTEGER minValue : 40 maxValue : 400 scaleType : UNIT_LINEAR_SCALE - parameterName : numRnnCells type : DISCRETE discreteValues : - 1 - 2 - 3 - 4 - parameterName : rnnCellType type : CATEGORICAL categoricalValues : - BasicLSTMCell - BasicRNNCell - GRUCell - LSTMCell - LayerNormBasicLSTMCell","title":"Hyperparameter tunning"},{"location":"programming/MLOps/#package-the-script-into-a-docker-container","text":"FROM gcr.io/deeplearning-platform-release/base-cpu RUN pip install -U fire cloudml-hypertune scikit-learn == 0 .20.4 pandas == 0 .24.2 WORKDIR /app COPY train.py . ENTRYPOINT [ \"python\" , \"train.py\" ]","title":"Package the script into a docker container"},{"location":"programming/MLOps/#bigquery","text":"bq query \\ -n 0 \\ --destination_table covertype_dataset.validation \\ --replace \\ --use_legacy_sql = false \\ 'SELECT * \\ FROM `covertype_dataset.covertype` as cover \\ WHERE \\ MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) = 0' bq extract \\ --destination_format CSV \\ covertype_dataset.validation \\ $VALIDATION_FILE_PATH","title":"BigQuery"},{"location":"programming/MLOps/#kubeflow-prebuilt-components","text":"import kfp URI = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/' component_store = kfp . components . ComponentStore ( local_search_paths = None , url_search_prefixes = [ URI ]) bigquery_query_op = component_store . load_component ( 'bigquery/query' ) mlengine_train_op = component_store . load_component ( 'ml_engine/train' ) mlengine_deploy_op = component_store . load_component ( 'ml_engine/deploy' ) create_testing_split = bigquery_query_op ( query = query , project_id = project_id , datset_id = dataset_id , table_id = '' , output_gcs_path = testing_file_path , dataset_location = dataset_location , ) # TO DO: Use the bigquery_query_op kfp submit run parameters should be in the same order as the function paramters","title":"KubeFlow prebuilt components"},{"location":"programming/SUMMARY/","text":"Summary First Chapter Spacemacs","title":"Summary"},{"location":"programming/SUMMARY/#summary","text":"First Chapter Spacemacs","title":"Summary"},{"location":"programming/Unit%20Tests/","text":"Effective Unit Testing by Eliotte Rusty Harold Why do we write our tests ? We want our code to work We want to keep it work We make mistakes To code faster, with more confidence and fewer regressions What is a unit test Verify that a known, fixed input produces a known, fixed output Test like sqrt(9) should be 3 Guides Eliminate everything that makes input or output unclear. Don't use constant from the main source code to prevent changes of tests from main source code Get away from ousitde dependency: network access, file system, the time and other external things (the gravitiy :p), multithreading One assert per test Unit also means Independent from each others More than one test class to a source class if neccessary Success tests should not proudce any output (logs ... etc) Failing tests should produce clear ouptut to help you debug Don't use same data in every tests Be aware of system skew like os system dependenet assumptions, floating point roundoff, integer width, default encoding Avoid conditional logic in tests, make them two separate tests In case of bug write the test first to be sure you understand the bug. Write test before refactoring","title":"Unit Tests"},{"location":"programming/Unit%20Tests/#effective-unit-testing-by-eliotte-rusty-harold","text":"","title":"Effective Unit Testing by Eliotte Rusty Harold"},{"location":"programming/Unit%20Tests/#why-do-we-write-our-tests","text":"We want our code to work We want to keep it work We make mistakes To code faster, with more confidence and fewer regressions","title":"Why do we write our tests ?"},{"location":"programming/Unit%20Tests/#what-is-a-unit-test","text":"Verify that a known, fixed input produces a known, fixed output Test like sqrt(9) should be 3","title":"What is a unit test"},{"location":"programming/Unit%20Tests/#guides","text":"Eliminate everything that makes input or output unclear. Don't use constant from the main source code to prevent changes of tests from main source code Get away from ousitde dependency: network access, file system, the time and other external things (the gravitiy :p), multithreading One assert per test Unit also means Independent from each others More than one test class to a source class if neccessary Success tests should not proudce any output (logs ... etc) Failing tests should produce clear ouptut to help you debug Don't use same data in every tests Be aware of system skew like os system dependenet assumptions, floating point roundoff, integer width, default encoding Avoid conditional logic in tests, make them two separate tests In case of bug write the test first to be sure you understand the bug. Write test before refactoring","title":"Guides"},{"location":"programming/api/","text":"REST APIs REST REST is a well-defnied structural patterns for designing APIs that allows software to talk to each others, it doens't dictate any implementation details even the protocol but the most common is HTTP REST is the name that has been given to the architectural style of HTTP itself, described by one of the leading authors of the HTTP specifications. HTTP is the reality\u2014REST is a set of design ideas that shaped it. REST is Data Oriented, it focuses on the underlying entities rather than the functions that manipulate them, being Data Oriented minimize the learning time because users can guess things easily especially if we are following standards, ex if the user know that there is a resource /items to get the list of items, the'll guess it also accepts a POST request with a new item and most probably individual items are accessible (GET, POST, PATCH, DELETE) at /items/{id} Web API a Web API is the set of representations (data), URLs, requests and responses (method, URL, headers and body) that allow the users to interact with our system. RESTfull RESTfull isn't well-defined and it most probably refers to the APIs that adhers the most to the REST patterns HTTP HTTP is the most common protocol used for REST APIs, an HTTP reqeuest constitued of a Method: GET, POST, PATCH, PUT, DELETE ... URL: where to the request Headers: Content-Type, Authorization, we can define custom headers if you want Body: what's in the request Notes It's much more practicle to follow standard ways when we craft REST APIs, it help our experienced users to use their expertise of REST APIs and use our new APIs much more easily and it help new commers to learn something standard that'd befinical for them when dealing with other APIs Put yourself in the shoe of the user Use JSON, it's the most common representation used in REST APIs even though it only supports basic data types: int, string ... API Design Elements The following aspects of API design are all important, and together they define your API: - The representations of your resources: this includes the definition of the fields in the resources (assuming your resource representations are structured, rather than streams of bytes or characters), and the links to related resources. - The use of standard (and occasionally custom) HTTP headers. - The URLs and URI templates that define the query interface of your API for locating resources based on their data. - Required behaviors by clients: for example, DNS caching behaviors, retry behaviors, tolerance of fields that were not previously present in resources, and so on. Designing Representations Use JSON, keep it simple and human readable Include links in the resposne, this idea is sometimes called Hypermedia As The Engine Of Application State, or HATEOAS It allow a navigation experience like navigating in a browser It's much more discoverable than having to hit the Documentation, understand it, guess how to construct the URL using the returned data and then write code (and debug it) that do it. Two ways of include links, we can make them read-only or read-write, if they are read-only, it'll make our live easier, if they are read-write we have to deal with them appropritely","title":"REST APIs"},{"location":"programming/api/#rest-apis","text":"","title":"REST APIs"},{"location":"programming/api/#rest","text":"REST is a well-defnied structural patterns for designing APIs that allows software to talk to each others, it doens't dictate any implementation details even the protocol but the most common is HTTP REST is the name that has been given to the architectural style of HTTP itself, described by one of the leading authors of the HTTP specifications. HTTP is the reality\u2014REST is a set of design ideas that shaped it. REST is Data Oriented, it focuses on the underlying entities rather than the functions that manipulate them, being Data Oriented minimize the learning time because users can guess things easily especially if we are following standards, ex if the user know that there is a resource /items to get the list of items, the'll guess it also accepts a POST request with a new item and most probably individual items are accessible (GET, POST, PATCH, DELETE) at /items/{id}","title":"REST"},{"location":"programming/api/#web-api","text":"a Web API is the set of representations (data), URLs, requests and responses (method, URL, headers and body) that allow the users to interact with our system.","title":"Web API"},{"location":"programming/api/#restfull","text":"RESTfull isn't well-defined and it most probably refers to the APIs that adhers the most to the REST patterns","title":"RESTfull"},{"location":"programming/api/#http","text":"HTTP is the most common protocol used for REST APIs, an HTTP reqeuest constitued of a Method: GET, POST, PATCH, PUT, DELETE ... URL: where to the request Headers: Content-Type, Authorization, we can define custom headers if you want Body: what's in the request","title":"HTTP"},{"location":"programming/api/#notes","text":"It's much more practicle to follow standard ways when we craft REST APIs, it help our experienced users to use their expertise of REST APIs and use our new APIs much more easily and it help new commers to learn something standard that'd befinical for them when dealing with other APIs Put yourself in the shoe of the user Use JSON, it's the most common representation used in REST APIs even though it only supports basic data types: int, string ...","title":"Notes"},{"location":"programming/api/#api-design-elements","text":"The following aspects of API design are all important, and together they define your API: - The representations of your resources: this includes the definition of the fields in the resources (assuming your resource representations are structured, rather than streams of bytes or characters), and the links to related resources. - The use of standard (and occasionally custom) HTTP headers. - The URLs and URI templates that define the query interface of your API for locating resources based on their data. - Required behaviors by clients: for example, DNS caching behaviors, retry behaviors, tolerance of fields that were not previously present in resources, and so on.","title":"API Design Elements"},{"location":"programming/api/#designing-representations","text":"Use JSON, keep it simple and human readable Include links in the resposne, this idea is sometimes called Hypermedia As The Engine Of Application State, or HATEOAS It allow a navigation experience like navigating in a browser It's much more discoverable than having to hit the Documentation, understand it, guess how to construct the URL using the returned data and then write code (and debug it) that do it. Two ways of include links, we can make them read-only or read-write, if they are read-only, it'll make our live easier, if they are read-write we have to deal with them appropritely","title":"Designing Representations"},{"location":"programming/clean_code/","text":"Clean Code, Uncle Bob Source: https://www.youtube.com/watch?v=7EmboKQH8lM My Job as a programmer is not to write code that works , it's just the first and least important part of the work, the most important part is to write clean code that others could understand which will allow them (including me after a while) to maintain, fix, modify ... I have to be polie: I have to write functions such that the reader can exit at any level he want when he feels he got what they wanted just like a newspaper article where there a header, an abstract then the details are brought up \"haba haba\" from top to bottom A function does one thing when we can't extract another function from it. Quotes \"The only measuremenent of how well the code is written is WTFs/minute\" ~ Some guy \"Clean code could be read as well written prose\" ~ Some guy Soft rules Respect the level of abstractions between lines of code, don't travel too many levels No more than 20 lines of code for a function No more than 3 parameters for a function, write objects, use other strategies instead if you want more Do not pass booleans to functions Avoid using switch statements, us polymorphisme instead Stopped at: https://youtu.be/7EmboKQH8lM?t=4838","title":"Clean Code, Uncle Bob"},{"location":"programming/clean_code/#clean-code-uncle-bob","text":"Source: https://www.youtube.com/watch?v=7EmboKQH8lM My Job as a programmer is not to write code that works , it's just the first and least important part of the work, the most important part is to write clean code that others could understand which will allow them (including me after a while) to maintain, fix, modify ... I have to be polie: I have to write functions such that the reader can exit at any level he want when he feels he got what they wanted just like a newspaper article where there a header, an abstract then the details are brought up \"haba haba\" from top to bottom A function does one thing when we can't extract another function from it.","title":"Clean Code, Uncle Bob"},{"location":"programming/clean_code/#quotes","text":"\"The only measuremenent of how well the code is written is WTFs/minute\" ~ Some guy \"Clean code could be read as well written prose\" ~ Some guy","title":"Quotes"},{"location":"programming/clean_code/#soft-rules","text":"Respect the level of abstractions between lines of code, don't travel too many levels No more than 20 lines of code for a function No more than 3 parameters for a function, write objects, use other strategies instead if you want more Do not pass booleans to functions Avoid using switch statements, us polymorphisme instead Stopped at: https://youtu.be/7EmboKQH8lM?t=4838","title":"Soft rules"},{"location":"programming/css/","text":"Center a button inside a div https://www.w3schools.com/howto/howto_css_center_button.asp @ media only screen and ( min-width : 768px ) { . container { position : relative ; } . vertical-center { position : absolute ; top : 40 % ; } } Pure-css Putting a text element (ex: label) inside a pure-g directly makes it inherits a negative letter-spacing and makes the text mkhalat (overlap), it also happens if we don't set a pure-u class for a breakpoint (only tested with phones) Wtforms SelectField with int values Add coerce=int for god's sake!","title":"Css"},{"location":"programming/css/#center-a-button-inside-a-div","text":"https://www.w3schools.com/howto/howto_css_center_button.asp @ media only screen and ( min-width : 768px ) { . container { position : relative ; } . vertical-center { position : absolute ; top : 40 % ; } }","title":"Center a button inside a div"},{"location":"programming/css/#pure-css","text":"Putting a text element (ex: label) inside a pure-g directly makes it inherits a negative letter-spacing and makes the text mkhalat (overlap), it also happens if we don't set a pure-u class for a breakpoint (only tested with phones)","title":"Pure-css"},{"location":"programming/css/#wtforms-selectfield-with-int-values","text":"Add coerce=int for god's sake!","title":"Wtforms SelectField with int values"},{"location":"programming/design_patterns/","text":"Design Patterns What it's all about ? It's simply a general solution of a common problem is software design. History ? After the GoF book where 23 patterns where defined, a lot of the patterns where discovered every time a recurrent problem happens and someone found a good solution for it. Why should I even bother ? They are tested and proved to work design, knowing them would ease software design for you They define a common language between software developers, you could just say: \"Use the Strategy for that\" and everyone will understand. Criticism of patterns They are created due to the lack of the necessary programming language abstraction. For example, the Strategy pattern can be implemented with a simple anonymous (lambda) function in most modern programming languages. They try to unify a solution for a set of problems which pushes people to not adapt them and use them as the are. Unjustified use: novice that just known design patterns tend to use everywhere because they think it's always good to use them. Factory Method ... ... Applicability Use the Factory Method when: - You don\u2019t know beforehand the exact types and dependencies of the objects your code should work with. - You want to provide users of your library or framework with a way to extend its internal components. - You want to save system resources by reusing existing objects instead of rebuilding them each time. Oreilly class Singleton Singleton makes sure there is only one instance of some object, it can be used for example to implement database pools, configuration objects where we want them to be shared across the application. It's not a global variable because it's an object and an object is defined by what it does not what it contains You can't observe it just by looking into code you have to know how it's been used to know if it's a singleton, a utility object (like java.math) or just a bunch of global variables Solving singleton problems with double checking sucks It's not about the code, it's not about the structure, it's about how code behaves. Abstract Factory An Abstrac class (a.k.a the abstract factory) has a method that returns another abstract class (a.k.a the abstract product) and the concrete factory tries as hard as possible to not reveal the implementation details about it's concrete product, In java it's done with a private final class inside the concrete factory class. Note: Java has many many mecanisme that restricts/controls access to fields/methods ... (private, static, inner classes, ...) while Python does the exact opposite of that, Python literally exposes it's own implementations and allow you to do whatever you want and says: \"We are all adults here!\" ref1 , ref2 Adapters It's used to swap libraries that does the same thing without changing the base code Decorators Add functionality to an object, at first it semms like we can do the same with inheritance, let's see this example, we have this diagram: InputStream <- FileInputStream We want to have a BufferedInputStream that buffers characters before reading Inheritance InputStream <- FileInputStream <- BufferedFileInputStream But also for InputStream alone InputStream <- BufferedInputStream Decoration in = FileInputStream () in = BufferedInputStream ( in ) Here, we can see that using inheritance we should add 2 more classes whereas we only need one using decoration but it's not that much of a diffreence We want to have a GZIPInputStream that decodes the input as GZIP Inheritance InputStream <- FileInputStream <- BufferedInputStream <- GZIPBufferedFileInputStream But also without Bufered InputStream <- FileInputStream <- GZIPFileInputStream But also without File InputStream <- GZIPInputStream So we have to write three classes Decoration in = FileInputStream () in = BufferedInputStream ( in ) in = GZIPInputStream ( in ) Now, what happens if we want to add another implementation Inheritance InputStream <- FileInputStream <- BufferedInputStream <- GZIPBufferedInputStream <- GZIPBufferedFileCountInputStream But also without GZIP InputStream <- FileInputStream <- BufferedInputStream <- BufferdFileCountInputStream But also without Buffered InputStream <- FileInputStream <- FileCountInputStream But also without File InputStream <- CountInputStream And now we have to add 4 classes - Decoration in = FileInputStream () in = BufferedInputStream ( in ) in = GZIPInputStream ( in ) in = LineCountInputStream ( in ) You get it, it gows expentionally and usually we don't implement all the cases because there is just so much of them. Decorators is the way to go in those cases Bridge the purpouse of a Bridge is to isolate two subsystem so that we can modify them independently Note: The diffrence between design patters is the intent Unlike adapters and decorators bridges are doing much work, it has a lot of code Example: JDBC (Java Database Connector, maybe!), as long as I use the interfaces, I can swap the db implementations (sqlite, mysql, ...) changing the client code. * Wrap up: The intent of - Adapter: mix an interface into a class that we don't have the code for and that is not currently using that interface - Decorator: change the way that a method behaves - Bridge: isolate a subsystem There is absolutely no reason to not mixing them together Facade It's a false front, it's like a facade you see when you entre a bank: just the banker with the cache machine and some PCs but back door there is much more than that The purpouse is to simplify, it provides a simplifying inteface to another interface, it's not trying to isolate subsystems from each other.","title":"Design Patterns"},{"location":"programming/design_patterns/#design-patterns","text":"","title":"Design Patterns"},{"location":"programming/design_patterns/#what-its-all-about","text":"It's simply a general solution of a common problem is software design.","title":"What it's all about ?"},{"location":"programming/design_patterns/#history","text":"After the GoF book where 23 patterns where defined, a lot of the patterns where discovered every time a recurrent problem happens and someone found a good solution for it.","title":"History ?"},{"location":"programming/design_patterns/#why-should-i-even-bother","text":"They are tested and proved to work design, knowing them would ease software design for you They define a common language between software developers, you could just say: \"Use the Strategy for that\" and everyone will understand.","title":"Why should I even bother ?"},{"location":"programming/design_patterns/#criticism-of-patterns","text":"They are created due to the lack of the necessary programming language abstraction. For example, the Strategy pattern can be implemented with a simple anonymous (lambda) function in most modern programming languages. They try to unify a solution for a set of problems which pushes people to not adapt them and use them as the are. Unjustified use: novice that just known design patterns tend to use everywhere because they think it's always good to use them.","title":"Criticism of patterns"},{"location":"programming/design_patterns/#factory-method","text":"","title":"Factory Method"},{"location":"programming/design_patterns/#_1","text":"","title":"..."},{"location":"programming/design_patterns/#_2","text":"","title":"..."},{"location":"programming/design_patterns/#applicability","text":"Use the Factory Method when: - You don\u2019t know beforehand the exact types and dependencies of the objects your code should work with. - You want to provide users of your library or framework with a way to extend its internal components. - You want to save system resources by reusing existing objects instead of rebuilding them each time.","title":"Applicability"},{"location":"programming/design_patterns/#oreilly-class","text":"","title":"Oreilly class"},{"location":"programming/design_patterns/#singleton","text":"Singleton makes sure there is only one instance of some object, it can be used for example to implement database pools, configuration objects where we want them to be shared across the application. It's not a global variable because it's an object and an object is defined by what it does not what it contains You can't observe it just by looking into code you have to know how it's been used to know if it's a singleton, a utility object (like java.math) or just a bunch of global variables Solving singleton problems with double checking sucks It's not about the code, it's not about the structure, it's about how code behaves.","title":"Singleton"},{"location":"programming/design_patterns/#abstract-factory","text":"An Abstrac class (a.k.a the abstract factory) has a method that returns another abstract class (a.k.a the abstract product) and the concrete factory tries as hard as possible to not reveal the implementation details about it's concrete product, In java it's done with a private final class inside the concrete factory class. Note: Java has many many mecanisme that restricts/controls access to fields/methods ... (private, static, inner classes, ...) while Python does the exact opposite of that, Python literally exposes it's own implementations and allow you to do whatever you want and says: \"We are all adults here!\" ref1 , ref2","title":"Abstract Factory"},{"location":"programming/design_patterns/#adapters","text":"It's used to swap libraries that does the same thing without changing the base code","title":"Adapters"},{"location":"programming/design_patterns/#decorators","text":"Add functionality to an object, at first it semms like we can do the same with inheritance, let's see this example, we have this diagram: InputStream <- FileInputStream We want to have a BufferedInputStream that buffers characters before reading Inheritance InputStream <- FileInputStream <- BufferedFileInputStream But also for InputStream alone InputStream <- BufferedInputStream Decoration in = FileInputStream () in = BufferedInputStream ( in ) Here, we can see that using inheritance we should add 2 more classes whereas we only need one using decoration but it's not that much of a diffreence We want to have a GZIPInputStream that decodes the input as GZIP Inheritance InputStream <- FileInputStream <- BufferedInputStream <- GZIPBufferedFileInputStream But also without Bufered InputStream <- FileInputStream <- GZIPFileInputStream But also without File InputStream <- GZIPInputStream So we have to write three classes Decoration in = FileInputStream () in = BufferedInputStream ( in ) in = GZIPInputStream ( in ) Now, what happens if we want to add another implementation Inheritance InputStream <- FileInputStream <- BufferedInputStream <- GZIPBufferedInputStream <- GZIPBufferedFileCountInputStream But also without GZIP InputStream <- FileInputStream <- BufferedInputStream <- BufferdFileCountInputStream But also without Buffered InputStream <- FileInputStream <- FileCountInputStream But also without File InputStream <- CountInputStream And now we have to add 4 classes - Decoration in = FileInputStream () in = BufferedInputStream ( in ) in = GZIPInputStream ( in ) in = LineCountInputStream ( in ) You get it, it gows expentionally and usually we don't implement all the cases because there is just so much of them. Decorators is the way to go in those cases","title":"Decorators"},{"location":"programming/design_patterns/#bridge","text":"the purpouse of a Bridge is to isolate two subsystem so that we can modify them independently Note: The diffrence between design patters is the intent Unlike adapters and decorators bridges are doing much work, it has a lot of code Example: JDBC (Java Database Connector, maybe!), as long as I use the interfaces, I can swap the db implementations (sqlite, mysql, ...) changing the client code. * Wrap up: The intent of - Adapter: mix an interface into a class that we don't have the code for and that is not currently using that interface - Decorator: change the way that a method behaves - Bridge: isolate a subsystem There is absolutely no reason to not mixing them together","title":"Bridge"},{"location":"programming/design_patterns/#facade","text":"It's a false front, it's like a facade you see when you entre a bank: just the banker with the cache machine and some PCs but back door there is much more than that The purpouse is to simplify, it provides a simplifying inteface to another interface, it's not trying to isolate subsystems from each other.","title":"Facade"},{"location":"programming/design_patterns/#_3","text":"","title":""},{"location":"programming/go/","text":"Install wget https : // dl . google . com / go / go1 . 14.1 . linux - amd64 . tar . gz sudo tar - C / usr / local - xzf go1 . 14.1 . linux - amd64 . tar . gz export PATH =$ PATH : / usr / local / go / bin Notes There should be package statemant build using go build filename if package is not main: building don't output the executalbe file install package using go get github.com/.... import using the whole name: import \"github.com/....\" Only Uppercased elements are exported to be used in other packages","title":"Install"},{"location":"programming/go/#install","text":"wget https : // dl . google . com / go / go1 . 14.1 . linux - amd64 . tar . gz sudo tar - C / usr / local - xzf go1 . 14.1 . linux - amd64 . tar . gz export PATH =$ PATH : / usr / local / go / bin","title":"Install"},{"location":"programming/go/#notes","text":"There should be package statemant build using go build filename if package is not main: building don't output the executalbe file install package using go get github.com/.... import using the whole name: import \"github.com/....\" Only Uppercased elements are exported to be used in other packages","title":"Notes"},{"location":"programming/spacemacs/","text":"Spacemacs Useful Spacemacs commands SPC q q - quit SPC w / - split window vertically SPC w - - split window horizontally SPC 1 - switch to window 1 SPC 2 - switch to window 2 SPC w c - delete current window SPC TAB - switch to previous buffer SPC b b - switch buffers SPC f f - find a file SPC f s - save a file (:w also works) SPC p p - open project SPC p h - find a file in current project SPC b d - kill current buffer SPC b M - move buffer to another window SPC v - enter expand-region mode SPC b b - Helm mini; lists buffers & recent files CTRL SPC - Mark Items CTRL z - Actions SPC b B - ibuffer SPC f f - open files CTRL h - up a folder CTRL l - open a folder CTRL j - up CTRL k - down SPC p f - opens root of project SPC p p - opens projects SPC / - searches through project SPC s s - search in a file SPC s l - find all function definitons in a file SPC v - expand region SPC V - contract region s ( - put parens around a region SPC s e - multiple cursors n - jump N - jump SPC h d - help describe SPC h d f - help describe functions SPC h d v - help describe variables SPC f e h - help ALT / - snippet completion SPC t s - syntax checking SPC e - syntax checking options SPC a r - ranger SPC a d - deer unimpaired [e - Move line up ]e - Move line down [SPACE - Insert space above ]SPACE - Insert space below [p - Paste above current line ]p - Paste below current line evil-mc grm - make-all-cursors gru - undo-all-cursors grs - pause-cursors grr - resume-cursors grf - make-and-goto-first-cursor grl - make-and-goto-last-cursor grh - make-cursor-here M-n - make-and-goto-next-cursor grN - skip-and-goto-next-cursor M-p - make-and-goto-prev-cursor grP - skip-and-goto-prev-cursor C-n - make-and-goto-next-match grn - skip-and-goto-next-match C-t - skip-and-goto-next-match C-p - make-and-goto-prev-match grp - skip-and-goto-prev-match Eyebrowse gt - go to next workspace gT - go to previous workspace SPC l w n - create or switch to workspace n SPC l w TAB - switch to last active workspace SPC l w c - close current workspace SPC l w n or SPC l w l - switch to next workspace SPC l w N or SPC l w p or SPC l w h - switch to previous workspace SPC l w r - set a tag to the current workspace SPC l w w - switched to tagged workspace Find/Replace Alt % - query-replace; active region, or cursor point to end interactive find/replace y - do the replacement. n - skip ! - do this and all remaining replacements without asking. Ctrl+g - cancel. Git SPC g b open a magit blame SPC g B quit magit blame SPC g c commit changes SPC g C checkout branches SPC g d show diff prompt SPC g D show diff against current head SPC g e show ediff comparison SPC g E show ediff against current head SPC g f show fetch prompt SPC g F show pull prompt SPC g H c clear highlights SPC g H h highlight regions by age of commits SPC g H t highlight regions by last updated time SPC g i git init a given directory SPC g I open helm-gitignore SPC g l open a magit log SPC g L display the log for a file SPC g P show push prompt SPC g s open a magit status window SPC g S stage current file SPC g m display the last commit message of the current line SPC g t launch the git time machine SPC g U unstage current file Highlight by age of commit or last update time is provided by smeargle. Git time machine is provided by git-timemachine. Git last commit message per line is provided by git-messenger. 3.1 Magit Spacemacs uses magit to manage Git repositories. To open a status buffer, type in a buffer of a Git repository: SPC g s Spacemacs uses evil-magit for key bindings in magit buffers (unless your editing style is set to emacs, in which case you get the default magit bindings), which are the standard magit key bindings with some minimal changes to make them comfortable for evil users. Here are the often used bindings inside a status buffer: Key Binding Description / evil-search $ open command output buffer c c open a commit message buffer b b checkout a branch b c create a branch f f fetch changes F (r) u pull tracked branch and rebase gr refresh j goto next magit section C-j next visual line k goto previous magit section C-k previous visual line l l open log buffer n next search occurrence N previous search occurrence o revert item at point P u push to tracked branch P m push to matching branch (e.g., upstream/develop to origin/develop) q quit s on a file or hunk in a diff: stage the file or hunk x discard changes on a hunk: increase hunk size on a hunk: decrease hunk size S stage all TAB on a file: expand/collapse diff u on a staged file: unstage U unstage all staged files v or V select multiple lines z z stash changes 3.2 Staging lines Magit allows you to stage specific lines by selecting them in a diff and hitting s to stage. Due to inconsistencies between Vim and Emacs editing styles, if you enter visual line state with V, you will stage one more line than intended. To work around this, you can use v instead (since Magit only stages whole lines, in any case). 3.3 Commit message editing buffer In a commit message buffer press ~,c~ (if dotspacemacs-major-mode-leader-key is ~,~) or C-c C-c to commit the changes with the entered message. Pressing ~,a~ or C-c C-k will discard the commit message. Key Binding Description h go left j go down k go up l go right 3.4 Interactive rebase buffer Key Binding Description c or p pick e edit f fixup j go down gj move line down k go up gk move line up d or x kill line r reword s squash u undo y insert ! execute 3.5 Quick guide for recurring use cases in Magit Amend a commit: l l to open log buffer c a on the commit you want to amend ~,c~ or C-c C-c to submit the changes Squash last commit: l l to open log buffer r e on the second to last commit, it opens the rebase buffer j to put point on last commit s to squash it ~,c~ or C-c C-c to continue to the commit message buffer ~,c~ or C-c C-c again when you have finished to edit the commit message Force push a squashed commit: in the status buffer you should see the new commit unpushed and the old commit unpulled P -f P for force a push (beware usually it is not recommended to rewrite the history of a public repository, but if you are sure that you are the only one to work on a repository it is ok - i.e. in your fork). Add upstream remote (the parent repository you have forked): M to open the remote popup a to add a remote, type the name (i.e. upstream) and the URL Pull changes from upstream (the parent repository you have forked) and push: F -r C-u F and choose upstream or the name you gave to it P P to push the commit to origin 3.6 Git-Flow magit-gitflow provides git-flow commands in its own magit menu. Key Binding Description % open magit-gitflow menu 3.7 Git time machine git-timemachine allows to quickly browse the commits of the current buffer. Key Binding Description SPC g t start git timemachine and initiate micro-state c show current commit n show next commit N show previous commit p show previous commit q leave micro-state and git timemachine Y copy current commit hash Useful Vim key bindings movement 0 - beginning of line ^ - beginning of non-whitespace $ - end of line 9j - move down 9 lines w - move forward by word b - move backward by word gg - first line G - last line C-u - up half page C-d - down half page f/ - move forward to first \"/\" character t/ - move forward right before the first \"/\" character ; - repeat that command again H - head of the screen M - middle of the screen L - last of the screen } - move forward by paragraph or block { - move backwards by paragraph or block * - search for word under the cursor n - search again forward N - search again backwards # - search backwards for word under cursor / - search forward ? - search backward % - find matching brace, paren, etc ma - mark a line in a file with marker \"a\" `a - after moving around, go back to the exact position of marker \"a\" 'a - after moving around, go back to line of marker \"a\" :marks - view all the marks '' - go to the last place you were editing x - delete char under cursor X - delete char before cursor A - add to end of line I - insert at the beginning of the line dd - delete line D - delete from cursor to end of line di' - delete text inside single quotes yy - copy line Y - copy from cursor to end of line cc - change line C - change from cursor to end of line cit - change text inside html tag ci' - change text inside single quotes ci{ - change text inside curly brackets. ci... - etc p - paste after cursor P - paste before cursor o - add line below O - add line above . = repeat last comment r - replace character R - replace. (overwrite) (good for columns of text) J - join line (cursor can be anywhere on line) visual mode v - visual char mode V - visual line mode C-v - block visual mode","title":"Spacemacs"},{"location":"programming/spacemacs/#spacemacs","text":"","title":"Spacemacs"},{"location":"programming/spacemacs/#useful-spacemacs-commands","text":"SPC q q - quit SPC w / - split window vertically SPC w - - split window horizontally SPC 1 - switch to window 1 SPC 2 - switch to window 2 SPC w c - delete current window SPC TAB - switch to previous buffer SPC b b - switch buffers SPC f f - find a file SPC f s - save a file (:w also works) SPC p p - open project SPC p h - find a file in current project SPC b d - kill current buffer SPC b M - move buffer to another window SPC v - enter expand-region mode SPC b b - Helm mini; lists buffers & recent files CTRL SPC - Mark Items CTRL z - Actions SPC b B - ibuffer SPC f f - open files CTRL h - up a folder CTRL l - open a folder CTRL j - up CTRL k - down SPC p f - opens root of project SPC p p - opens projects SPC / - searches through project SPC s s - search in a file SPC s l - find all function definitons in a file SPC v - expand region SPC V - contract region s ( - put parens around a region SPC s e - multiple cursors n - jump N - jump SPC h d - help describe SPC h d f - help describe functions SPC h d v - help describe variables SPC f e h - help ALT / - snippet completion SPC t s - syntax checking SPC e - syntax checking options SPC a r - ranger SPC a d - deer","title":"Useful Spacemacs commands"},{"location":"programming/spacemacs/#unimpaired","text":"[e - Move line up ]e - Move line down [SPACE - Insert space above ]SPACE - Insert space below [p - Paste above current line ]p - Paste below current line","title":"unimpaired"},{"location":"programming/spacemacs/#evil-mc","text":"grm - make-all-cursors gru - undo-all-cursors grs - pause-cursors grr - resume-cursors grf - make-and-goto-first-cursor grl - make-and-goto-last-cursor grh - make-cursor-here M-n - make-and-goto-next-cursor grN - skip-and-goto-next-cursor M-p - make-and-goto-prev-cursor grP - skip-and-goto-prev-cursor C-n - make-and-goto-next-match grn - skip-and-goto-next-match C-t - skip-and-goto-next-match C-p - make-and-goto-prev-match grp - skip-and-goto-prev-match","title":"evil-mc"},{"location":"programming/spacemacs/#eyebrowse","text":"gt - go to next workspace gT - go to previous workspace SPC l w n - create or switch to workspace n SPC l w TAB - switch to last active workspace SPC l w c - close current workspace SPC l w n or SPC l w l - switch to next workspace SPC l w N or SPC l w p or SPC l w h - switch to previous workspace SPC l w r - set a tag to the current workspace SPC l w w - switched to tagged workspace","title":"Eyebrowse"},{"location":"programming/spacemacs/#findreplace","text":"Alt % - query-replace; active region, or cursor point to end interactive find/replace y - do the replacement. n - skip ! - do this and all remaining replacements without asking. Ctrl+g - cancel.","title":"Find/Replace"},{"location":"programming/spacemacs/#git","text":"SPC g b open a magit blame SPC g B quit magit blame SPC g c commit changes SPC g C checkout branches SPC g d show diff prompt SPC g D show diff against current head SPC g e show ediff comparison SPC g E show ediff against current head SPC g f show fetch prompt SPC g F show pull prompt SPC g H c clear highlights SPC g H h highlight regions by age of commits SPC g H t highlight regions by last updated time SPC g i git init a given directory SPC g I open helm-gitignore SPC g l open a magit log SPC g L display the log for a file SPC g P show push prompt SPC g s open a magit status window SPC g S stage current file SPC g m display the last commit message of the current line SPC g t launch the git time machine SPC g U unstage current file Highlight by age of commit or last update time is provided by smeargle. Git time machine is provided by git-timemachine. Git last commit message per line is provided by git-messenger. 3.1 Magit Spacemacs uses magit to manage Git repositories. To open a status buffer, type in a buffer of a Git repository: SPC g s Spacemacs uses evil-magit for key bindings in magit buffers (unless your editing style is set to emacs, in which case you get the default magit bindings), which are the standard magit key bindings with some minimal changes to make them comfortable for evil users. Here are the often used bindings inside a status buffer: Key Binding Description / evil-search $ open command output buffer c c open a commit message buffer b b checkout a branch b c create a branch f f fetch changes F (r) u pull tracked branch and rebase gr refresh j goto next magit section C-j next visual line k goto previous magit section C-k previous visual line l l open log buffer n next search occurrence N previous search occurrence o revert item at point P u push to tracked branch P m push to matching branch (e.g., upstream/develop to origin/develop) q quit s on a file or hunk in a diff: stage the file or hunk x discard changes on a hunk: increase hunk size on a hunk: decrease hunk size S stage all TAB on a file: expand/collapse diff u on a staged file: unstage U unstage all staged files v or V select multiple lines z z stash changes 3.2 Staging lines Magit allows you to stage specific lines by selecting them in a diff and hitting s to stage. Due to inconsistencies between Vim and Emacs editing styles, if you enter visual line state with V, you will stage one more line than intended. To work around this, you can use v instead (since Magit only stages whole lines, in any case). 3.3 Commit message editing buffer In a commit message buffer press ~,c~ (if dotspacemacs-major-mode-leader-key is ~,~) or C-c C-c to commit the changes with the entered message. Pressing ~,a~ or C-c C-k will discard the commit message. Key Binding Description h go left j go down k go up l go right 3.4 Interactive rebase buffer Key Binding Description c or p pick e edit f fixup j go down gj move line down k go up gk move line up d or x kill line r reword s squash u undo y insert ! execute 3.5 Quick guide for recurring use cases in Magit Amend a commit: l l to open log buffer c a on the commit you want to amend ~,c~ or C-c C-c to submit the changes Squash last commit: l l to open log buffer r e on the second to last commit, it opens the rebase buffer j to put point on last commit s to squash it ~,c~ or C-c C-c to continue to the commit message buffer ~,c~ or C-c C-c again when you have finished to edit the commit message Force push a squashed commit: in the status buffer you should see the new commit unpushed and the old commit unpulled P -f P for force a push (beware usually it is not recommended to rewrite the history of a public repository, but if you are sure that you are the only one to work on a repository it is ok - i.e. in your fork). Add upstream remote (the parent repository you have forked): M to open the remote popup a to add a remote, type the name (i.e. upstream) and the URL Pull changes from upstream (the parent repository you have forked) and push: F -r C-u F and choose upstream or the name you gave to it P P to push the commit to origin 3.6 Git-Flow magit-gitflow provides git-flow commands in its own magit menu. Key Binding Description % open magit-gitflow menu 3.7 Git time machine git-timemachine allows to quickly browse the commits of the current buffer. Key Binding Description SPC g t start git timemachine and initiate micro-state c show current commit n show next commit N show previous commit p show previous commit q leave micro-state and git timemachine Y copy current commit hash","title":"Git"},{"location":"programming/spacemacs/#useful-vim-key-bindings","text":"","title":"Useful Vim key bindings"},{"location":"programming/spacemacs/#movement","text":"0 - beginning of line ^ - beginning of non-whitespace $ - end of line 9j - move down 9 lines w - move forward by word b - move backward by word gg - first line G - last line C-u - up half page C-d - down half page f/ - move forward to first \"/\" character t/ - move forward right before the first \"/\" character ; - repeat that command again H - head of the screen M - middle of the screen L - last of the screen } - move forward by paragraph or block { - move backwards by paragraph or block * - search for word under the cursor n - search again forward N - search again backwards # - search backwards for word under cursor / - search forward ? - search backward % - find matching brace, paren, etc ma - mark a line in a file with marker \"a\" `a - after moving around, go back to the exact position of marker \"a\" 'a - after moving around, go back to line of marker \"a\" :marks - view all the marks '' - go to the last place you were","title":"movement"},{"location":"programming/spacemacs/#editing","text":"x - delete char under cursor X - delete char before cursor A - add to end of line I - insert at the beginning of the line dd - delete line D - delete from cursor to end of line di' - delete text inside single quotes yy - copy line Y - copy from cursor to end of line cc - change line C - change from cursor to end of line cit - change text inside html tag ci' - change text inside single quotes ci{ - change text inside curly brackets. ci... - etc p - paste after cursor P - paste before cursor o - add line below O - add line above . = repeat last comment r - replace character R - replace. (overwrite) (good for columns of text) J - join line (cursor can be anywhere on line)","title":"editing"},{"location":"programming/spacemacs/#visual-mode","text":"v - visual char mode V - visual line mode C-v - block visual mode","title":"visual mode"},{"location":"programming/ttd/","text":"TTD Why ? Because the civilization depends on us We can clean code with confidence Code will be testable which makes it decoupled Double <- Dummy <- Stub <- Spy <- Mock Double <- Fake Dummy: Do nothing. Example: return null in authenticate Stub: .Example: return True or False in authenticate Spy: Spies on how the object was used. Example: know how much time authenticate was called Mock: Babahom ga3, he does spies but knows what to expect from using the object. Fake: A simple simulator. Example: authenticate only one defined user.","title":"TTD"},{"location":"programming/ttd/#ttd","text":"","title":"TTD"},{"location":"programming/ttd/#why","text":"Because the civilization depends on us We can clean code with confidence Code will be testable which makes it decoupled Double <- Dummy <- Stub <- Spy <- Mock Double <- Fake Dummy: Do nothing. Example: return null in authenticate Stub: .Example: return True or False in authenticate Spy: Spies on how the object was used. Example: know how much time authenticate was called Mock: Babahom ga3, he does spies but knows what to expect from using the object. Fake: A simple simulator. Example: authenticate only one defined user.","title":"Why ?"},{"location":"programming/vscode/","text":"Black formatting Black not working because of (most probably) an incompatible version of click (click-7.1.2) but VSCode doesn't show any error in the \"Python\" logs Running the same black command that vscode users in a terminal: \u276f ~/ workspace / pravda / venv / bin / python - m black -- diff -- quiet ./ tests / test_app_docker . py Traceback ( most recent call last ): File \"/usr/lib/python3.9/runpy.py\" , line 188 , in _run_module_as_main mod_name , mod_spec , code = _get_module_details ( mod_name , _Error ) File \"/usr/lib/python3.9/runpy.py\" , line 147 , in _get_module_details return _get_module_details ( pkg_main_name , error ) File \"/usr/lib/python3.9/runpy.py\" , line 111 , in _get_module_details __import__ ( pkg_name ) File \"src/black/__init__.py\" , line 35 , in < module > ImportError : cannot import name 'ParameterSource' from 'click.core' ( / home / mohammedi / workspace / pravda / venv / lib / python3 .9 / site - packages / click / core . py )","title":"Black formatting"},{"location":"programming/vscode/#black-formatting","text":"Black not working because of (most probably) an incompatible version of click (click-7.1.2) but VSCode doesn't show any error in the \"Python\" logs Running the same black command that vscode users in a terminal: \u276f ~/ workspace / pravda / venv / bin / python - m black -- diff -- quiet ./ tests / test_app_docker . py Traceback ( most recent call last ): File \"/usr/lib/python3.9/runpy.py\" , line 188 , in _run_module_as_main mod_name , mod_spec , code = _get_module_details ( mod_name , _Error ) File \"/usr/lib/python3.9/runpy.py\" , line 147 , in _get_module_details return _get_module_details ( pkg_main_name , error ) File \"/usr/lib/python3.9/runpy.py\" , line 111 , in _get_module_details __import__ ( pkg_name ) File \"src/black/__init__.py\" , line 35 , in < module > ImportError : cannot import name 'ParameterSource' from 'click.core' ( / home / mohammedi / workspace / pravda / venv / lib / python3 .9 / site - packages / click / core . py )","title":"Black formatting"},{"location":"python/conda/","text":"Anaconda Create an environment conda create -n env36 python = 3 .6 Activate an environment conda activate env36 Disable (base) on the terminal conda config --set auto_activate_base False Use pip Run conda install pip . This will install pip to your venv directory. Install new packages by doing /anaconda/envs/venv_name/bin/pip install package_name .","title":"Anaconda"},{"location":"python/conda/#anaconda","text":"","title":"Anaconda"},{"location":"python/conda/#create-an-environment","text":"conda create -n env36 python = 3 .6","title":"Create an environment"},{"location":"python/conda/#activate-an-environment","text":"conda activate env36","title":"Activate an environment"},{"location":"python/conda/#disable-base-on-the-terminal","text":"conda config --set auto_activate_base False","title":"Disable (base) on the terminal"},{"location":"python/conda/#use-pip","text":"Run conda install pip . This will install pip to your venv directory. Install new packages by doing /anaconda/envs/venv_name/bin/pip install package_name .","title":"Use pip"},{"location":"python/descriptors/","text":"Descriptors Source: https://docs.python.org/3/howto/descriptor.html Descriptors let objects customize attribute lookup, storage, and deletion. Minimal useless example class Ten : def __get__ ( self , obj , objtype = None ): return 10 class A : x = 5 # Regular class attribute y = Ten () # Descriptor instance >>> a = A () # Make an instance of class A >>> a . x # Normal attribute lookup 5 >>> a . y # Descriptor lookup 10 Better example import os class DirectorySize : # self is a DirectorySize instance # obj is a Directory instance def __get__ ( self , obj , objtype = None ): return len ( os . listdir ( obj . dirname )) class Directory : size = DirectorySize () # Descriptor instance def __init__ ( self , dirname ): self . dirname = dirname # Regular instance attribute __set_name__ import logging logging . basicConfig ( level = logging . INFO ) class LoggedAccess : def __set_name__ ( self , owner , name ): self . public_name = name self . private_name = '_' + name def __get__ ( self , obj , objtype = None ): value = getattr ( obj , self . private_name ) logging . info ( 'Accessing %r giving %r ' , self . public_name , value ) return value def __set__ ( self , obj , value ): logging . info ( 'Updating %r to %r ' , self . public_name , value ) setattr ( obj , self . private_name , value ) class Person : name = LoggedAccess () # First descriptor instance age = LoggedAccess () # Second descriptor instance def __init__ ( self , name , age ): self . name = name # Calls the first descriptor self . age = age # Calls the second descriptor def birthday ( self ): self . age += 1 Closing Thoughts A descriptor is what we call any object that defines get (), set (), or delete (). Optionally, descriptors can have a set_name () method. This is only used in cases where a descriptor needs to know either the class where it was created or the name of class variable it was assigned to. (This method, if present, is called even if the class is not a descriptor.) Descriptors get invoked by the dot \u201coperator\u201d during attribute lookup. If a descriptor is accessed indirectly with vars(some_class)[descriptor_name], the descriptor instance is returned without invoking it. Descriptors only work when used as class variables. When put in instances, they have no effect. The main motivation for descriptors is to provide a hook allowing objects stored in class variables to control what happens during attribute lookup. Traditionally, the calling class controls what happens during lookup. Descriptors invert that relationship and allow the data being looked-up to have a say in the matter. Descriptors are used throughout the language. It is how functions turn into bound methods. Common tools like classmethod(), staticmethod(), property(), and functools.cached_property() are all implemented as descriptors. Complete Practical Example: Validator A validator is a descriptor for managed attribute access. Prior to storing any data, it verifies that the new value meets various type and range restrictions. If those restrictions aren\u2019t met, it raises an exception to prevent data corruption at its source. from abc import ABC , abstractmethod class Validator ( ABC ): def __set_name__ ( self , owner , name ): self . private_name = '_' + name def __get__ ( self , obj , objtype = None ): return getattr ( obj , self . private_name ) def __set__ ( self , obj , value ): self . validate ( value ) setattr ( obj , self . private_name , value ) @abstractmethod def validate ( self , value ): pass Custom Validators class OneOf ( Validator ): def __init__ ( self , * options ): self . options = set ( options ) def validate ( self , value ): if value not in self . options : raise ValueError ( f 'Expected { value !r} to be one of { self . options !r} ' ) class Number ( Validator ): def __init__ ( self , minvalue = None , maxvalue = None ): self . minvalue = minvalue self . maxvalue = maxvalue def validate ( self , value ): if not isinstance ( value , ( int , float )): raise TypeError ( f 'Expected { value !r} to be an int or float' ) if self . minvalue is not None and value < self . minvalue : raise ValueError ( f 'Expected { value !r} to be at least { self . minvalue !r} ' ) if self . maxvalue is not None and value > self . maxvalue : raise ValueError ( f 'Expected { value !r} to be no more than { self . maxvalue !r} ' ) class String ( Validator ): def __init__ ( self , minsize = None , maxsize = None , predicate = None ): self . minsize = minsize self . maxsize = maxsize self . predicate = predicate def validate ( self , value ): if not isinstance ( value , str ): raise TypeError ( f 'Expected { value !r} to be an str' ) if self . minsize is not None and len ( value ) < self . minsize : raise ValueError ( f 'Expected { value !r} to be no smaller than { self . minsize !r} ' ) if self . maxsize is not None and len ( value ) > self . maxsize : raise ValueError ( f 'Expected { value !r} to be no bigger than { self . maxsize !r} ' ) if self . predicate is not None and not self . predicate ( value ): raise ValueError ( f 'Expected { self . predicate } to be true for { value !r} ' ) Practical application class Component : name = String ( minsize = 3 , maxsize = 10 , predicate = str . isupper ) kind = OneOf ( 'wood' , 'metal' , 'plastic' ) quantity = Number ( minvalue = 0 ) def __init__ ( self , name , kind , quantity ): self . name = name self . kind = kind self . quantity = quantity >>> Component ( 'Widget' , 'metal' , 5 ) # Blocked: 'Widget' is not all uppercase Traceback ( most recent call last ): ... ValueError : Expected < method 'isupper' of 'str' objects > to be true for 'Widget' >>> Component ( 'WIDGET' , 'metle' , 5 ) # Blocked: 'metle' is misspelled Traceback ( most recent call last ): ... ValueError : Expected 'metle' to be one of { 'metal' , 'plastic' , 'wood' } >>> Component ( 'WIDGET' , 'metal' , - 5 ) # Blocked: -5 is negative Traceback ( most recent call last ): ... ValueError : Expected - 5 to be at least 0 >>> Component ( 'WIDGET' , 'metal' , 'V' ) # Blocked: 'V' isn't a number Traceback ( most recent call last ): ... TypeError : Expected 'V' to be an int or float >>> c = Component ( 'WIDGET' , 'metal' , 5 ) # Allowed: The inputs are valid Technical Tutorial Descriptors are a powerful, general purpose protocol. They are the mechanism behind properties, methods, static methods, class methods, and super(). They are used throughout Python itself. Descriptors simplify the underlying C code and offer a flexible set of new tools for everyday Python programs. Descriptor protocol descr . __get__ ( self , obj , type = None ) -> value descr . __set__ ( self , obj , value ) -> None descr . __delete__ ( self , obj ) -> None Data descriptors (doesn't define __set__ and __delete__ ) always override instance dictionaries. Non-data descriptors may be overridden by instance dictionaries. The logic for a dotted lookup is in object. getattribute () if getattr () exists, it is called whenever getattribute () raises AttributeError (either directly or in one of the descriptor calls) object. getattribute () and type. getattribute () make different calls to get (). The first includes the instance and may include the class. The second puts in None for the instance and always includes the class. I a descriptor define __set_name__(owner, name) , type metaclass will call it in type.__new__ (class creation), the owner is the class where the descriptor is used, and the name is the class variable the descriptor was assigned to Attribute Lookup priority data descriptor > instance variable > non-data descriptor > class variable Emulate @property class Property : \"Emulate PyProperty_Type() in Objects/descrobject.c\" def __init__ ( self , fget = None , fset = None , fdel = None , doc = None ): self . fget = fget self . fset = fset self . fdel = fdel if doc is None and fget is not None : doc = fget . __doc__ self . __doc__ = doc def __get__ ( self , obj , objtype = None ): if obj is None : return self if self . fget is None : raise AttributeError ( \"unreadable attribute\" ) return self . fget ( obj ) def __set__ ( self , obj , value ): if self . fset is None : raise AttributeError ( \"can't set attribute\" ) self . fset ( obj , value ) def __delete__ ( self , obj ): if self . fdel is None : raise AttributeError ( \"can't delete attribute\" ) self . fdel ( obj ) def getter ( self , fget ): return type ( self )( fget , self . fset , self . fdel , self . __doc__ ) def setter ( self , fset ): return type ( self )( self . fget , fset , self . fdel , self . __doc__ ) def deleter ( self , fdel ): return type ( self )( self . fget , self . fset , fdel , self . __doc__ ) Functions and methods Functions are non-data descriptors that returns the bound method ( MethodType below) when accessed from an instance (if the obj param of __get__ is not None), the underlying function is returned as it is when access from the class ( obj is None) Methods are callables that preprend self to the function's arguments when called, self is stored as __self__ and the underlying function as __func__ in the MethodType objects class MethodType : \"Emulate PyMethod_Type in Objects/classobject.c\" def __init__ ( self , func , obj ): self . __func__ = func self . __self__ = obj def __call__ ( self , * args , ** kwargs ): func = self . __func__ obj = self . __self__ return func ( obj , * args , ** kwargs ) class Function : ... def __get__ ( self , obj , objtype = None ): \"Simulate func_descr_get() in Objects/funcobject.c\" if obj is None : return self return MethodType ( self , obj )","title":"Descriptors"},{"location":"python/descriptors/#descriptors","text":"Source: https://docs.python.org/3/howto/descriptor.html Descriptors let objects customize attribute lookup, storage, and deletion.","title":"Descriptors"},{"location":"python/descriptors/#minimal-useless-example","text":"class Ten : def __get__ ( self , obj , objtype = None ): return 10 class A : x = 5 # Regular class attribute y = Ten () # Descriptor instance >>> a = A () # Make an instance of class A >>> a . x # Normal attribute lookup 5 >>> a . y # Descriptor lookup 10","title":"Minimal useless example"},{"location":"python/descriptors/#better-example","text":"import os class DirectorySize : # self is a DirectorySize instance # obj is a Directory instance def __get__ ( self , obj , objtype = None ): return len ( os . listdir ( obj . dirname )) class Directory : size = DirectorySize () # Descriptor instance def __init__ ( self , dirname ): self . dirname = dirname # Regular instance attribute","title":"Better example"},{"location":"python/descriptors/#__set_name__","text":"import logging logging . basicConfig ( level = logging . INFO ) class LoggedAccess : def __set_name__ ( self , owner , name ): self . public_name = name self . private_name = '_' + name def __get__ ( self , obj , objtype = None ): value = getattr ( obj , self . private_name ) logging . info ( 'Accessing %r giving %r ' , self . public_name , value ) return value def __set__ ( self , obj , value ): logging . info ( 'Updating %r to %r ' , self . public_name , value ) setattr ( obj , self . private_name , value ) class Person : name = LoggedAccess () # First descriptor instance age = LoggedAccess () # Second descriptor instance def __init__ ( self , name , age ): self . name = name # Calls the first descriptor self . age = age # Calls the second descriptor def birthday ( self ): self . age += 1","title":"__set_name__"},{"location":"python/descriptors/#closing-thoughts","text":"A descriptor is what we call any object that defines get (), set (), or delete (). Optionally, descriptors can have a set_name () method. This is only used in cases where a descriptor needs to know either the class where it was created or the name of class variable it was assigned to. (This method, if present, is called even if the class is not a descriptor.) Descriptors get invoked by the dot \u201coperator\u201d during attribute lookup. If a descriptor is accessed indirectly with vars(some_class)[descriptor_name], the descriptor instance is returned without invoking it. Descriptors only work when used as class variables. When put in instances, they have no effect. The main motivation for descriptors is to provide a hook allowing objects stored in class variables to control what happens during attribute lookup. Traditionally, the calling class controls what happens during lookup. Descriptors invert that relationship and allow the data being looked-up to have a say in the matter. Descriptors are used throughout the language. It is how functions turn into bound methods. Common tools like classmethod(), staticmethod(), property(), and functools.cached_property() are all implemented as descriptors.","title":"Closing Thoughts"},{"location":"python/descriptors/#complete-practical-example-validator","text":"A validator is a descriptor for managed attribute access. Prior to storing any data, it verifies that the new value meets various type and range restrictions. If those restrictions aren\u2019t met, it raises an exception to prevent data corruption at its source. from abc import ABC , abstractmethod class Validator ( ABC ): def __set_name__ ( self , owner , name ): self . private_name = '_' + name def __get__ ( self , obj , objtype = None ): return getattr ( obj , self . private_name ) def __set__ ( self , obj , value ): self . validate ( value ) setattr ( obj , self . private_name , value ) @abstractmethod def validate ( self , value ): pass","title":"Complete Practical Example: Validator"},{"location":"python/descriptors/#custom-validators","text":"class OneOf ( Validator ): def __init__ ( self , * options ): self . options = set ( options ) def validate ( self , value ): if value not in self . options : raise ValueError ( f 'Expected { value !r} to be one of { self . options !r} ' ) class Number ( Validator ): def __init__ ( self , minvalue = None , maxvalue = None ): self . minvalue = minvalue self . maxvalue = maxvalue def validate ( self , value ): if not isinstance ( value , ( int , float )): raise TypeError ( f 'Expected { value !r} to be an int or float' ) if self . minvalue is not None and value < self . minvalue : raise ValueError ( f 'Expected { value !r} to be at least { self . minvalue !r} ' ) if self . maxvalue is not None and value > self . maxvalue : raise ValueError ( f 'Expected { value !r} to be no more than { self . maxvalue !r} ' ) class String ( Validator ): def __init__ ( self , minsize = None , maxsize = None , predicate = None ): self . minsize = minsize self . maxsize = maxsize self . predicate = predicate def validate ( self , value ): if not isinstance ( value , str ): raise TypeError ( f 'Expected { value !r} to be an str' ) if self . minsize is not None and len ( value ) < self . minsize : raise ValueError ( f 'Expected { value !r} to be no smaller than { self . minsize !r} ' ) if self . maxsize is not None and len ( value ) > self . maxsize : raise ValueError ( f 'Expected { value !r} to be no bigger than { self . maxsize !r} ' ) if self . predicate is not None and not self . predicate ( value ): raise ValueError ( f 'Expected { self . predicate } to be true for { value !r} ' )","title":"Custom Validators"},{"location":"python/descriptors/#practical-application","text":"class Component : name = String ( minsize = 3 , maxsize = 10 , predicate = str . isupper ) kind = OneOf ( 'wood' , 'metal' , 'plastic' ) quantity = Number ( minvalue = 0 ) def __init__ ( self , name , kind , quantity ): self . name = name self . kind = kind self . quantity = quantity >>> Component ( 'Widget' , 'metal' , 5 ) # Blocked: 'Widget' is not all uppercase Traceback ( most recent call last ): ... ValueError : Expected < method 'isupper' of 'str' objects > to be true for 'Widget' >>> Component ( 'WIDGET' , 'metle' , 5 ) # Blocked: 'metle' is misspelled Traceback ( most recent call last ): ... ValueError : Expected 'metle' to be one of { 'metal' , 'plastic' , 'wood' } >>> Component ( 'WIDGET' , 'metal' , - 5 ) # Blocked: -5 is negative Traceback ( most recent call last ): ... ValueError : Expected - 5 to be at least 0 >>> Component ( 'WIDGET' , 'metal' , 'V' ) # Blocked: 'V' isn't a number Traceback ( most recent call last ): ... TypeError : Expected 'V' to be an int or float >>> c = Component ( 'WIDGET' , 'metal' , 5 ) # Allowed: The inputs are valid","title":"Practical application"},{"location":"python/descriptors/#technical-tutorial","text":"Descriptors are a powerful, general purpose protocol. They are the mechanism behind properties, methods, static methods, class methods, and super(). They are used throughout Python itself. Descriptors simplify the underlying C code and offer a flexible set of new tools for everyday Python programs.","title":"Technical Tutorial"},{"location":"python/descriptors/#descriptor-protocol","text":"descr . __get__ ( self , obj , type = None ) -> value descr . __set__ ( self , obj , value ) -> None descr . __delete__ ( self , obj ) -> None Data descriptors (doesn't define __set__ and __delete__ ) always override instance dictionaries. Non-data descriptors may be overridden by instance dictionaries. The logic for a dotted lookup is in object. getattribute () if getattr () exists, it is called whenever getattribute () raises AttributeError (either directly or in one of the descriptor calls) object. getattribute () and type. getattribute () make different calls to get (). The first includes the instance and may include the class. The second puts in None for the instance and always includes the class. I a descriptor define __set_name__(owner, name) , type metaclass will call it in type.__new__ (class creation), the owner is the class where the descriptor is used, and the name is the class variable the descriptor was assigned to","title":"Descriptor protocol"},{"location":"python/descriptors/#attribute-lookup-priority","text":"data descriptor > instance variable > non-data descriptor > class variable","title":"Attribute Lookup priority"},{"location":"python/descriptors/#emulate-property","text":"class Property : \"Emulate PyProperty_Type() in Objects/descrobject.c\" def __init__ ( self , fget = None , fset = None , fdel = None , doc = None ): self . fget = fget self . fset = fset self . fdel = fdel if doc is None and fget is not None : doc = fget . __doc__ self . __doc__ = doc def __get__ ( self , obj , objtype = None ): if obj is None : return self if self . fget is None : raise AttributeError ( \"unreadable attribute\" ) return self . fget ( obj ) def __set__ ( self , obj , value ): if self . fset is None : raise AttributeError ( \"can't set attribute\" ) self . fset ( obj , value ) def __delete__ ( self , obj ): if self . fdel is None : raise AttributeError ( \"can't delete attribute\" ) self . fdel ( obj ) def getter ( self , fget ): return type ( self )( fget , self . fset , self . fdel , self . __doc__ ) def setter ( self , fset ): return type ( self )( self . fget , fset , self . fdel , self . __doc__ ) def deleter ( self , fdel ): return type ( self )( self . fget , self . fset , fdel , self . __doc__ )","title":"Emulate @property"},{"location":"python/descriptors/#functions-and-methods","text":"Functions are non-data descriptors that returns the bound method ( MethodType below) when accessed from an instance (if the obj param of __get__ is not None), the underlying function is returned as it is when access from the class ( obj is None) Methods are callables that preprend self to the function's arguments when called, self is stored as __self__ and the underlying function as __func__ in the MethodType objects class MethodType : \"Emulate PyMethod_Type in Objects/classobject.c\" def __init__ ( self , func , obj ): self . __func__ = func self . __self__ = obj def __call__ ( self , * args , ** kwargs ): func = self . __func__ obj = self . __self__ return func ( obj , * args , ** kwargs ) class Function : ... def __get__ ( self , obj , objtype = None ): \"Simulate func_descr_get() in Objects/funcobject.c\" if obj is None : return self return MethodType ( self , obj )","title":"Functions and methods"},{"location":"python/effective-python/","text":"Forward The book is really effective, that is you read a small chunk of the book to get a really interesting ideas to write a better python code (Example: item 9 Consider Generator Expressions for Large Comprehensions), Pythonic ways to code most common patterns (Example: item 10 Prefer enumerate Over range and item 11 Use zip to Process Iterators in Parallel) and also advice for good practices (Example: item 1: Follow the PEP 8 Style Guide). What I really liked in the book is the author putting a lot of attention on writing readable, reusable and maintainable code, thinking a lot about newbies who will read your code and also about adding functionalities to to your code while preserving backward compatibility. Content Chapter 1: Pythonic Thinking Item 1: Know Which Version of Python You\u2019re Using There are two ways to inspect python interpreter version: 1. type python -v 2. use os.sys.version Item 2: Follow the PEP 8 Style Guide The PEP 8 contains guidelines about code formatting Item 3: Know the Differences Between bytes, str, and unicode In python 3: str is a sequence of unicode characters and bytes is a sequence of bytes In python 2: unicode is a sequence of unicode characters and str is a sequence of bytes Use helper functions ( isinstance ) to ensure that the inputs you operate on are the type of character sequence you expect (8-bit values, UTF-8 encoded characters, Unicode characters, etc.). If you want to read or write binary data to/from a file, always open the file using a binary mode (like 'rb' or 'wb'). Item 4: Write Helper Functions Instead of Complex Expressions Python expressiveness let you define complex expressions, thought for the sake of code readability and maintanbility prefer defining helper functions instead. Item 5: Know How to Slice Sequences Avoid being verbose: Don\u2019t supply 0 for the start index or the length of the sequence for the end index. Slicing is forgiving of start or end indexes that are out of bounds, making it easy to express slices on the front or back boundaries of a sequence (like a[:20] or a[-20:]). Assigning to a list slice will replace that range in the original sequence with what\u2019s referenced even if their lengths are different. Item 6: Avoid Using start, end, and stride in a Single Slice If it's the case use separate instruction for slicing from start to end and for stride Example l = l [:: 2 ] l = l [ 1 : 9 ] Item 7: Use List Comprehensions Instead of map and filter For the sake of readability a = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] # Prefer this squares = [ x ** 2 for x in a ] # Than this squares = map ( lambda x : x ** 2 , a ) Item 8: Avoid More Than Two Expressions in List Comprehensions For the sake of readability matrix = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]] # Prefer this filtered = [] for row in matrix : if sum ( row ) >= 10 for x in row : if x % 3 == 0 : filtered . append ( x ) # Than this filtered = [ x for row in matrix if sum ( row ) >= 10 for x in row if x % 3 == 0 ] Item 9: Consider Generator Expressions for Large Comprehensions Generator expressions avoid memory issues by producing outputs one at a time as an iterator. Generator expressions can be composed by passing the iterator from one generator expression into the for subexpression of another. Replacing the brackets [] with parenthesis () if for comprehensions will result in a generator Item 10: Prefer enumerate Over range Sometimes you want to loop over a sequence by index, python provide a utility method for that called enumerate for idx , item in enumerate ( list ): print ( \u2018 % d : % s \u2019 % ( i , item )) Item 11: Use zip to Process Iterators in Parallel Python provides a utility method called zip to process iterators in parallel for name , count in zip ( names , letters ): print ( ' %s : %s ' % ( name , count )) Item 12: Avoid else Blocks After for and while Loops Python supports else block in for and while but it's counterintuitive and can be confusing, you should avoid it as much as you can. Item 13: Take Advantage of Each Block in try/except/else/finally handle = open ( path , \u2018 r + \u2019 ) try : data = handle . read () op = json . loads ( data ) value = ( op [ \u2018 numerator \u2019 ] / op [ \u2018 denominator \u2019 ]) except ZeroDivisionError as e return else : op [ \u2018 result \u2019 ] = value result = json . dumps ( op ) handle . seek ( 0 ) handle . write ( result ) return value finally : handle . close () The else block helps you minimize the amount of code in try blocks and visually distinguish the success case from the try/except blocks. Chapter 2: Functions Item 14: Prefer Exceptions to Returning None Item 15: Know How Closures Interact with Variable Scope Item 16: Consider Generators Instead of Returning Lists Item 17: Be Defensive When Iterating Over Arguments Item 18: Reduce Visual Noise with Variable Positional Arguments Item 19: Provide Optional Behavior with Keyword Arguments Item 20: Use None and Docstrings to Specify Dynamic Default Arguments Item 21: Enforce Clarity with Keyword-Only Arguments Chapter 3: Classes and Inheritance Item 22: Prefer Helper Classes Over Bookkeeping with Dictionaries and Tuples Item 23: Accept Functions for Simple Interfaces Instead of Classes Item 24: Use @classmethod Polymorphism to Construct Objects Generically### Item 25: Initialize Parent Classes with super Item 26: Use Multiple Inheritance Only for Mix-in Utility Classes Item 27: Prefer Public Attributes Over Private Ones Item 28: Inherit from collections.abc for Custom Container Types Chapter 4: Metaclasses and Attributes Item 29: Use Plain Attributes Instead of Get and Set Methods Item 30: Consider @property Instead of Refactoring Attributes Item 31: Use Descriptors for Reusable @property Methods Item 32: Use getattr , getattribute , and setattr for Lazy Attributes Item 33: Validate Subclasses with Metaclasses Item 34: Register Class Existence with Metaclasses Item 35: Annotate Class Attributes with Metaclasses Chapter 5: Concurrency and Parallelism Item 36: Use subprocess to Manage Child Processes Item 37: Use Threads for Blocking I/O, Avoid for Parallelism Item 38: Use Lock to Prevent Data Races in Threads Item 39: Use Queue to Coordinate Work Between Threads Item 40: Consider Coroutines to Run Many Functions Concurrently Item 41: Consider concurrent.futures for True Parallelism Chapter 6: Built-in Modules Item 42: Define Function Decorators with functools.wraps Item 43: Consider contextlib and with Statements for Reusable try/finally Behavior Item 44: Make pickle Reliable with copyreg Item 45: Use datetime Instead of time for Local Clocks Item 46: Use Built-in Algorithms and Data Structures Item 47: Use decimal When Precision Is Paramount Item 48: Know Where to Find Community-Built Modules Chapter 7: Collaboration Item 49: Write Docstrings for Every Function, Class, and Module Item 50: Use Packages to Organize Modules and Provide Stable APIs### Item 51: Define a Root Exception to Insulate Callers from APIs Item 52: Know How to Break Circular Dependencies Item 53: Use Virtual Environments for Isolated and Reproducible Dependencies Chapter 8: Production Item 54: Consider Module-Scoped Code to Configure Deployment Environments Item 55: Use repr Strings for Debugging Output Item 56: Test Everything with unittest Item 57: Consider Interactive Debugging with pdb Item 58: Profile Before Optimizing Item 59: Use tracemalloc to Understand Memory Usage and Leaks","title":"Forward"},{"location":"python/effective-python/#forward","text":"The book is really effective, that is you read a small chunk of the book to get a really interesting ideas to write a better python code (Example: item 9 Consider Generator Expressions for Large Comprehensions), Pythonic ways to code most common patterns (Example: item 10 Prefer enumerate Over range and item 11 Use zip to Process Iterators in Parallel) and also advice for good practices (Example: item 1: Follow the PEP 8 Style Guide). What I really liked in the book is the author putting a lot of attention on writing readable, reusable and maintainable code, thinking a lot about newbies who will read your code and also about adding functionalities to to your code while preserving backward compatibility.","title":"Forward"},{"location":"python/effective-python/#content","text":"","title":"Content"},{"location":"python/effective-python/#chapter-1-pythonic-thinking","text":"","title":"Chapter 1: Pythonic Thinking"},{"location":"python/effective-python/#item-1-know-which-version-of-python-youre-using","text":"There are two ways to inspect python interpreter version: 1. type python -v 2. use os.sys.version","title":"Item 1: Know Which Version of Python You\u2019re Using"},{"location":"python/effective-python/#item-2-follow-the-pep-8-style-guide","text":"The PEP 8 contains guidelines about code formatting","title":"Item 2: Follow the PEP 8 Style Guide"},{"location":"python/effective-python/#item-3-know-the-differences-between-bytes-str-and-unicode","text":"In python 3: str is a sequence of unicode characters and bytes is a sequence of bytes In python 2: unicode is a sequence of unicode characters and str is a sequence of bytes Use helper functions ( isinstance ) to ensure that the inputs you operate on are the type of character sequence you expect (8-bit values, UTF-8 encoded characters, Unicode characters, etc.). If you want to read or write binary data to/from a file, always open the file using a binary mode (like 'rb' or 'wb').","title":"Item 3: Know the Differences Between bytes, str, and unicode"},{"location":"python/effective-python/#item-4-write-helper-functions-instead-of-complex-expressions","text":"Python expressiveness let you define complex expressions, thought for the sake of code readability and maintanbility prefer defining helper functions instead.","title":"Item 4: Write Helper Functions Instead of Complex Expressions"},{"location":"python/effective-python/#item-5-know-how-to-slice-sequences","text":"Avoid being verbose: Don\u2019t supply 0 for the start index or the length of the sequence for the end index. Slicing is forgiving of start or end indexes that are out of bounds, making it easy to express slices on the front or back boundaries of a sequence (like a[:20] or a[-20:]). Assigning to a list slice will replace that range in the original sequence with what\u2019s referenced even if their lengths are different.","title":"Item 5: Know How to Slice Sequences"},{"location":"python/effective-python/#item-6-avoid-using-start-end-and-stride-in-a-single-slice","text":"If it's the case use separate instruction for slicing from start to end and for stride Example l = l [:: 2 ] l = l [ 1 : 9 ]","title":"Item 6: Avoid Using start, end, and stride in a Single Slice"},{"location":"python/effective-python/#item-7-use-list-comprehensions-instead-of-map-and-filter","text":"For the sake of readability a = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] # Prefer this squares = [ x ** 2 for x in a ] # Than this squares = map ( lambda x : x ** 2 , a )","title":"Item 7: Use List Comprehensions Instead of map and filter"},{"location":"python/effective-python/#item-8-avoid-more-than-two-expressions-in-list-comprehensions","text":"For the sake of readability matrix = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]] # Prefer this filtered = [] for row in matrix : if sum ( row ) >= 10 for x in row : if x % 3 == 0 : filtered . append ( x ) # Than this filtered = [ x for row in matrix if sum ( row ) >= 10 for x in row if x % 3 == 0 ]","title":"Item 8: Avoid More Than Two Expressions in List Comprehensions"},{"location":"python/effective-python/#item-9-consider-generator-expressions-for-large-comprehensions","text":"Generator expressions avoid memory issues by producing outputs one at a time as an iterator. Generator expressions can be composed by passing the iterator from one generator expression into the for subexpression of another. Replacing the brackets [] with parenthesis () if for comprehensions will result in a generator","title":"Item 9: Consider Generator Expressions for Large Comprehensions"},{"location":"python/effective-python/#item-10-prefer-enumerate-over-range","text":"Sometimes you want to loop over a sequence by index, python provide a utility method for that called enumerate for idx , item in enumerate ( list ): print ( \u2018 % d : % s \u2019 % ( i , item ))","title":"Item 10: Prefer enumerate Over range"},{"location":"python/effective-python/#item-11-use-zip-to-process-iterators-in-parallel","text":"Python provides a utility method called zip to process iterators in parallel for name , count in zip ( names , letters ): print ( ' %s : %s ' % ( name , count ))","title":"Item 11: Use zip to Process Iterators in Parallel"},{"location":"python/effective-python/#item-12-avoid-else-blocks-after-for-and-while-loops","text":"Python supports else block in for and while but it's counterintuitive and can be confusing, you should avoid it as much as you can.","title":"Item 12: Avoid else Blocks After for and while Loops"},{"location":"python/effective-python/#item-13-take-advantage-of-each-block-in-tryexceptelsefinally","text":"handle = open ( path , \u2018 r + \u2019 ) try : data = handle . read () op = json . loads ( data ) value = ( op [ \u2018 numerator \u2019 ] / op [ \u2018 denominator \u2019 ]) except ZeroDivisionError as e return else : op [ \u2018 result \u2019 ] = value result = json . dumps ( op ) handle . seek ( 0 ) handle . write ( result ) return value finally : handle . close () The else block helps you minimize the amount of code in try blocks and visually distinguish the success case from the try/except blocks.","title":"Item 13: Take Advantage of Each Block in try/except/else/finally"},{"location":"python/effective-python/#chapter-2-functions","text":"","title":"Chapter 2: Functions"},{"location":"python/effective-python/#item-14-prefer-exceptions-to-returning-none","text":"","title":"Item 14: Prefer Exceptions to Returning None"},{"location":"python/effective-python/#item-15-know-how-closures-interact-with-variable-scope","text":"","title":"Item 15: Know How Closures Interact with Variable Scope"},{"location":"python/effective-python/#item-16-consider-generators-instead-of-returning-lists","text":"","title":"Item 16: Consider Generators Instead of Returning Lists"},{"location":"python/effective-python/#item-17-be-defensive-when-iterating-over-arguments","text":"","title":"Item 17: Be Defensive When Iterating Over Arguments"},{"location":"python/effective-python/#item-18-reduce-visual-noise-with-variable-positional-arguments","text":"","title":"Item 18: Reduce Visual Noise with Variable Positional Arguments"},{"location":"python/effective-python/#item-19-provide-optional-behavior-with-keyword-arguments","text":"","title":"Item 19: Provide Optional Behavior with Keyword Arguments"},{"location":"python/effective-python/#item-20-use-none-and-docstrings-to-specify-dynamic-default-arguments","text":"","title":"Item 20: Use None and Docstrings to Specify Dynamic Default Arguments"},{"location":"python/effective-python/#item-21-enforce-clarity-with-keyword-only-arguments","text":"","title":"Item 21: Enforce Clarity with Keyword-Only Arguments"},{"location":"python/effective-python/#chapter-3-classes-and-inheritance","text":"","title":"Chapter 3: Classes and Inheritance"},{"location":"python/effective-python/#item-22-prefer-helper-classes-over-bookkeeping-with-dictionaries-and-tuples","text":"","title":"Item 22: Prefer Helper Classes Over Bookkeeping with Dictionaries and Tuples"},{"location":"python/effective-python/#item-23-accept-functions-for-simple-interfaces-instead-of-classes","text":"","title":"Item 23: Accept Functions for Simple Interfaces Instead of Classes"},{"location":"python/effective-python/#item-24-use-classmethod-polymorphism-to-construct-objects-generically-item-25-initialize-parent-classes-with-super","text":"","title":"Item 24: Use @classmethod Polymorphism to Construct Objects Generically### Item 25: Initialize Parent Classes with super"},{"location":"python/effective-python/#item-26-use-multiple-inheritance-only-for-mix-in-utility-classes","text":"","title":"Item 26: Use Multiple Inheritance Only for Mix-in Utility Classes"},{"location":"python/effective-python/#item-27-prefer-public-attributes-over-private-ones","text":"","title":"Item 27: Prefer Public Attributes Over Private Ones"},{"location":"python/effective-python/#item-28-inherit-from-collectionsabc-for-custom-container-types","text":"","title":"Item 28: Inherit from collections.abc for Custom Container Types"},{"location":"python/effective-python/#chapter-4-metaclasses-and-attributes","text":"","title":"Chapter 4: Metaclasses and Attributes"},{"location":"python/effective-python/#item-29-use-plain-attributes-instead-of-get-and-set-methods","text":"","title":"Item 29: Use Plain Attributes Instead of Get and Set Methods"},{"location":"python/effective-python/#item-30-consider-property-instead-of-refactoring-attributes","text":"","title":"Item 30: Consider @property Instead of Refactoring Attributes"},{"location":"python/effective-python/#item-31-use-descriptors-for-reusable-property-methods","text":"","title":"Item 31: Use Descriptors for Reusable @property Methods"},{"location":"python/effective-python/#item-32-use-getattr-getattribute-and-setattr-for-lazy-attributes","text":"","title":"Item 32: Use getattr, getattribute, and setattr for Lazy Attributes"},{"location":"python/effective-python/#item-33-validate-subclasses-with-metaclasses","text":"","title":"Item 33: Validate Subclasses with Metaclasses"},{"location":"python/effective-python/#item-34-register-class-existence-with-metaclasses","text":"","title":"Item 34: Register Class Existence with Metaclasses"},{"location":"python/effective-python/#item-35-annotate-class-attributes-with-metaclasses","text":"","title":"Item 35: Annotate Class Attributes with Metaclasses"},{"location":"python/effective-python/#chapter-5-concurrency-and-parallelism","text":"","title":"Chapter 5: Concurrency and Parallelism"},{"location":"python/effective-python/#item-36-use-subprocess-to-manage-child-processes","text":"","title":"Item 36: Use subprocess to Manage Child Processes"},{"location":"python/effective-python/#item-37-use-threads-for-blocking-io-avoid-for-parallelism","text":"","title":"Item 37: Use Threads for Blocking I/O, Avoid for Parallelism"},{"location":"python/effective-python/#item-38-use-lock-to-prevent-data-races-in-threads","text":"","title":"Item 38: Use Lock to Prevent Data Races in Threads"},{"location":"python/effective-python/#item-39-use-queue-to-coordinate-work-between-threads","text":"","title":"Item 39: Use Queue to Coordinate Work Between Threads"},{"location":"python/effective-python/#item-40-consider-coroutines-to-run-many-functions-concurrently","text":"","title":"Item 40: Consider Coroutines to Run Many Functions Concurrently"},{"location":"python/effective-python/#item-41-consider-concurrentfutures-for-true-parallelism","text":"","title":"Item 41: Consider concurrent.futures for True Parallelism"},{"location":"python/effective-python/#chapter-6-built-in-modules","text":"","title":"Chapter 6: Built-in Modules"},{"location":"python/effective-python/#item-42-define-function-decorators-with-functoolswraps","text":"","title":"Item 42: Define Function Decorators with functools.wraps"},{"location":"python/effective-python/#item-43-consider-contextlib-and-with-statements-for-reusable-tryfinally-behavior","text":"","title":"Item 43: Consider contextlib and with Statements for Reusable try/finally Behavior"},{"location":"python/effective-python/#item-44-make-pickle-reliable-with-copyreg","text":"","title":"Item 44: Make pickle Reliable with copyreg"},{"location":"python/effective-python/#item-45-use-datetime-instead-of-time-for-local-clocks","text":"","title":"Item 45: Use datetime Instead of time for Local Clocks"},{"location":"python/effective-python/#item-46-use-built-in-algorithms-and-data-structures","text":"","title":"Item 46: Use Built-in Algorithms and Data Structures"},{"location":"python/effective-python/#item-47-use-decimal-when-precision-is-paramount","text":"","title":"Item 47: Use decimal When Precision Is Paramount"},{"location":"python/effective-python/#item-48-know-where-to-find-community-built-modules","text":"","title":"Item 48: Know Where to Find Community-Built Modules"},{"location":"python/effective-python/#chapter-7-collaboration","text":"","title":"Chapter 7: Collaboration"},{"location":"python/effective-python/#item-49-write-docstrings-for-every-function-class-and-module","text":"","title":"Item 49: Write Docstrings for Every Function, Class, and Module"},{"location":"python/effective-python/#item-50-use-packages-to-organize-modules-and-provide-stable-apis-item-51-define-a-root-exception-to-insulate-callers-from-apis","text":"","title":"Item 50: Use Packages to Organize Modules and Provide Stable APIs### Item 51: Define a Root Exception to Insulate Callers from APIs"},{"location":"python/effective-python/#item-52-know-how-to-break-circular-dependencies","text":"","title":"Item 52: Know How to Break Circular Dependencies"},{"location":"python/effective-python/#item-53-use-virtual-environments-for-isolated-and-reproducible-dependencies","text":"","title":"Item 53: Use Virtual Environments for Isolated and Reproducible Dependencies"},{"location":"python/effective-python/#chapter-8-production","text":"","title":"Chapter 8: Production"},{"location":"python/effective-python/#item-54-consider-module-scoped-code-to-configure-deployment-environments","text":"","title":"Item 54: Consider Module-Scoped Code to Configure Deployment Environments"},{"location":"python/effective-python/#item-55-use-repr-strings-for-debugging-output","text":"","title":"Item 55: Use repr Strings for Debugging Output"},{"location":"python/effective-python/#item-56-test-everything-with-unittest","text":"","title":"Item 56: Test Everything with unittest"},{"location":"python/effective-python/#item-57-consider-interactive-debugging-with-pdb","text":"","title":"Item 57: Consider Interactive Debugging with pdb"},{"location":"python/effective-python/#item-58-profile-before-optimizing","text":"","title":"Item 58: Profile Before Optimizing"},{"location":"python/effective-python/#item-59-use-tracemalloc-to-understand-memory-usage-and-leaks","text":"","title":"Item 59: Use tracemalloc to Understand Memory Usage and Leaks"},{"location":"python/firebase/","text":"Python Firebase SDK Initialize an app import os import firebase_admin from firebase_admin import credentials cred_file = os . environ . get ( \"FIREBASE_CREDENTIALS\" ) database_url = os . environ . get ( \"FIREBASE_DATABASE\" ) cred = credentials . Certificate ( cred_file ) default_app = firebase_admin . initialize_app ( cred , { 'databaseURL' : database_url }) Get Data from firebase_admin import db watches = db . reference ( '/watches' ) . get () Listen for events watches_ref = db . reference ( '/watches' ) def process ( event ): print ( event . event_type , event . data ) watches_ref . listen ( process ) Snnipets Database queries","title":"[Python Firebase SDK](https://firebase.google.com/docs/reference/admin/python/)"},{"location":"python/firebase/#python-firebase-sdk","text":"","title":"Python Firebase SDK"},{"location":"python/firebase/#initialize-an-app","text":"import os import firebase_admin from firebase_admin import credentials cred_file = os . environ . get ( \"FIREBASE_CREDENTIALS\" ) database_url = os . environ . get ( \"FIREBASE_DATABASE\" ) cred = credentials . Certificate ( cred_file ) default_app = firebase_admin . initialize_app ( cred , { 'databaseURL' : database_url })","title":"Initialize an app"},{"location":"python/firebase/#get-data","text":"from firebase_admin import db watches = db . reference ( '/watches' ) . get ()","title":"Get Data"},{"location":"python/firebase/#listen-for-events","text":"watches_ref = db . reference ( '/watches' ) def process ( event ): print ( event . event_type , event . data ) watches_ref . listen ( process )","title":"Listen for events"},{"location":"python/firebase/#snnipets","text":"Database queries","title":"Snnipets"},{"location":"python/flask/","text":"Flask Databases Flask-SQLAlchemy, an extension that provides a Flask-friendly wrapper to the popular SQLAlchemy package, which is an Object Relational Mapper or ORM. pip install flask-sqlalchemy Flask-Migrate a database migration framework for SQLAlchemy. pip install flask-migrate Configurations Most important configuration is SQLALCHEMY_DATABASE_URI Models app/models.py from datetime import datetime from app import db class User ( db . Model ): id = db . Column ( db . Integer , primary_key = True ) username = db . Column ( db . String ( 64 ), index = True , unique = True ) email = db . Column ( db . String ( 120 ), index = True , unique = True ) password_hash = db . Column ( db . String ( 128 )) posts = db . relationship ( 'Post' , backref = 'author' , lazy = 'dynamic' ) def __repr__ ( self ): return '<User {} >' . format ( self . username ) class Post ( db . Model ): id = db . Column ( db . Integer , primary_key = True ) body = db . Column ( db . String ( 140 )) timestamp = db . Column ( db . DateTime , index = True , default = datetime . utcnow ) user_id = db . Column ( db . Integer , db . ForeignKey ( 'user.id' )) def __repr__ ( self ): return '<Post {} >' . format ( self . body ) Database Migrations Note FLASK_APP should be set for these commands to execute properly Create the migrations repository flask db init Generate automatic migrations flask db migrate We can provide a -m optional argument that adds a short descriptive text to the migration. flask db migrate -m \"added users table\" Upgrade The flask db migrate command does not make any changes to the database, it just generates the migration script. To apply the changes to the database, the flask db upgrade command must be used. Downgrade flask db downgrade Blueprints Blueprints is the way flask achieve modularity, reusablity and extensibility. Here's a simple hello world blueprint. . \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 hello \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 routes.py \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 index.html \u2502 \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 flask_learning.py app/__init__.py from flask import Flask from app.hello import blueprint def create_app (): app = Flask ( __name__ ) app . register_blueprint ( blueprint ) return app app/hello/__init__.py from flask import Blueprint blueprint = Blueprint ( 'hello_blueprint' , __name__ , template_folder = 'templates' ) from app.hello import routes app/hello/routes.py from flask import render_template from app.hello import blueprint @blueprint . route ( \"/<string:name>\" ) def index ( name ): return render_template ( \"index.html\" , name = name ) flask_learning.py from app import create_app app = create_app () Configuring the app Flask provides many ways of loading configurations to the application. One way is to define a configuration object and load configuration from there using app.config.from_object method. Note from_object loads only the uppercase attributes of the module/class Using objects for configuration we can separate different configuration for developpement , testing and production class Config ( object ): SOME_CONFIG = 'default-value' class Devlopement ( Config ): pass class Production ( Config ): SOME_CONFIG = 'production-value' class Testing ( Config ): SOME_CONFIG = 'testing-config' Jinja Basic template_dir = os . path . dirname ( template_path ) template_name = os . path . basename ( template_path ) env = Environment ( loader = FileSystemLoader ( template_dir ), ** env_kwargs ) template = env . get_template ( template_name ) Raise exception def raise_from_jinja ( * args , ** kwargs ): raise Exception ( * args , ** kwargs ) # And add it to the globals of the template template . globals . update ({ 'raise' : raise_from_jinja })","title":"Flask"},{"location":"python/flask/#flask","text":"","title":"Flask"},{"location":"python/flask/#databases","text":"Flask-SQLAlchemy, an extension that provides a Flask-friendly wrapper to the popular SQLAlchemy package, which is an Object Relational Mapper or ORM. pip install flask-sqlalchemy Flask-Migrate a database migration framework for SQLAlchemy. pip install flask-migrate","title":"Databases"},{"location":"python/flask/#configurations","text":"Most important configuration is SQLALCHEMY_DATABASE_URI","title":"Configurations"},{"location":"python/flask/#models","text":"","title":"Models"},{"location":"python/flask/#appmodelspy","text":"from datetime import datetime from app import db class User ( db . Model ): id = db . Column ( db . Integer , primary_key = True ) username = db . Column ( db . String ( 64 ), index = True , unique = True ) email = db . Column ( db . String ( 120 ), index = True , unique = True ) password_hash = db . Column ( db . String ( 128 )) posts = db . relationship ( 'Post' , backref = 'author' , lazy = 'dynamic' ) def __repr__ ( self ): return '<User {} >' . format ( self . username ) class Post ( db . Model ): id = db . Column ( db . Integer , primary_key = True ) body = db . Column ( db . String ( 140 )) timestamp = db . Column ( db . DateTime , index = True , default = datetime . utcnow ) user_id = db . Column ( db . Integer , db . ForeignKey ( 'user.id' )) def __repr__ ( self ): return '<Post {} >' . format ( self . body )","title":"app/models.py"},{"location":"python/flask/#database-migrations","text":"Note FLASK_APP should be set for these commands to execute properly","title":"Database Migrations"},{"location":"python/flask/#create-the-migrations-repository","text":"flask db init","title":"Create the migrations repository"},{"location":"python/flask/#generate-automatic-migrations","text":"flask db migrate We can provide a -m optional argument that adds a short descriptive text to the migration. flask db migrate -m \"added users table\"","title":"Generate automatic migrations"},{"location":"python/flask/#upgrade","text":"The flask db migrate command does not make any changes to the database, it just generates the migration script. To apply the changes to the database, the flask db upgrade command must be used.","title":"Upgrade"},{"location":"python/flask/#downgrade","text":"flask db downgrade","title":"Downgrade"},{"location":"python/flask/#blueprints","text":"Blueprints is the way flask achieve modularity, reusablity and extensibility. Here's a simple hello world blueprint. . \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 hello \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 routes.py \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 index.html \u2502 \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 flask_learning.py","title":"Blueprints"},{"location":"python/flask/#app__init__py","text":"from flask import Flask from app.hello import blueprint def create_app (): app = Flask ( __name__ ) app . register_blueprint ( blueprint ) return app","title":"app/__init__.py"},{"location":"python/flask/#apphello__init__py","text":"from flask import Blueprint blueprint = Blueprint ( 'hello_blueprint' , __name__ , template_folder = 'templates' ) from app.hello import routes","title":"app/hello/__init__.py"},{"location":"python/flask/#apphelloroutespy","text":"from flask import render_template from app.hello import blueprint @blueprint . route ( \"/<string:name>\" ) def index ( name ): return render_template ( \"index.html\" , name = name )","title":"app/hello/routes.py"},{"location":"python/flask/#flask_learningpy","text":"from app import create_app app = create_app ()","title":"flask_learning.py"},{"location":"python/flask/#configuring-the-app","text":"Flask provides many ways of loading configurations to the application. One way is to define a configuration object and load configuration from there using app.config.from_object method. Note from_object loads only the uppercase attributes of the module/class Using objects for configuration we can separate different configuration for developpement , testing and production class Config ( object ): SOME_CONFIG = 'default-value' class Devlopement ( Config ): pass class Production ( Config ): SOME_CONFIG = 'production-value' class Testing ( Config ): SOME_CONFIG = 'testing-config'","title":"Configuring the app"},{"location":"python/flask/#jinja","text":"","title":"Jinja"},{"location":"python/flask/#basic","text":"template_dir = os . path . dirname ( template_path ) template_name = os . path . basename ( template_path ) env = Environment ( loader = FileSystemLoader ( template_dir ), ** env_kwargs ) template = env . get_template ( template_name )","title":"Basic"},{"location":"python/flask/#raise-exception","text":"def raise_from_jinja ( * args , ** kwargs ): raise Exception ( * args , ** kwargs ) # And add it to the globals of the template template . globals . update ({ 'raise' : raise_from_jinja })","title":"Raise exception"},{"location":"python/functools/","text":"Functools Source: https://docs.python.org/3/library/functools.html @functools.cached_property(func) Transform a method of a class into a property whose value is computed once and then cached as a normal attribute for the life of the instance. Similar to property(), with the addition of caching. Useful for expensive computed properties of instances that are otherwise effectively immutable. functools.cmp_to_key(func) Transform an old-style comparison function to a key function. Used with tools that accept key functions (such as sorted(), min(), max(), heapq.nlargest(), heapq.nsmallest(), itertools.groupby()). This function is primarily used as a transition tool for programs being converted from Python 2 which supported the use of comparison functions. A comparison function is any callable that accept two arguments, compares them, and returns a negative number for less-than, zero for equality, or a positive number for greater-than. A key function is a callable that accepts one argument and returns another value to be used as the sort key. Example: sorted ( iterable , key = cmp_to_key ( locale . strcoll )) # locale-aware sort order @functools.total_ordering Given a class defining one or more rich comparison ordering methods, this class decorator supplies the rest. This simplifies the effort involved in specifying all of the possible rich comparison operations: The class must define one of __lt__() , __le__() , __gt__() , or __ge__() . In addition, the class should supply an __eq__() method. class functools.partialmethod(func, /, *args, **keywords) Needs more investigation @functools.singledispatch Example: from functools import singledispatch @singledispatch def fun ( arg , verbose = False ): if verbose : print ( \"Let me just say,\" , end = \" \" ) print ( arg ) @fun . register def _ ( arg : int , verbose = False ): if verbose : print ( \"Strength in numbers, eh?\" , end = \" \" ) print ( arg ) @fun . register def _ ( arg : list , verbose = False ): if verbose : print ( \"Enumerate this:\" ) for i , elem in enumerate ( arg ): print ( i , elem ) class functools.singledispatchmethod(func) Needs more investigation functools.update_wrapper(wrapper, wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES) Update a wrapper function to look like the wrapped function. The optional arguments are tuples to specify which attributes of the original function are assigned directly to the matching attributes on the wrapper function and which attributes of the wrapper function are updated with the corresponding attributes from the original function. The default values for these arguments are the module level constants WRAPPER_ASSIGNMENTS (which assigns to the wrapper function\u2019s module , name , qualname , annotations and doc , the documentation string) and WRAPPER_UPDATES (which updates the wrapper function\u2019s dict , i.e. the instance dictionary). Datamodel Source: https://docs.python.org/3/reference/datamodel.html","title":"Functools"},{"location":"python/functools/#functools","text":"Source: https://docs.python.org/3/library/functools.html","title":"Functools"},{"location":"python/functools/#functoolscached_propertyfunc","text":"Transform a method of a class into a property whose value is computed once and then cached as a normal attribute for the life of the instance. Similar to property(), with the addition of caching. Useful for expensive computed properties of instances that are otherwise effectively immutable.","title":"@functools.cached_property(func)"},{"location":"python/functools/#functoolscmp_to_keyfunc","text":"Transform an old-style comparison function to a key function. Used with tools that accept key functions (such as sorted(), min(), max(), heapq.nlargest(), heapq.nsmallest(), itertools.groupby()). This function is primarily used as a transition tool for programs being converted from Python 2 which supported the use of comparison functions. A comparison function is any callable that accept two arguments, compares them, and returns a negative number for less-than, zero for equality, or a positive number for greater-than. A key function is a callable that accepts one argument and returns another value to be used as the sort key. Example: sorted ( iterable , key = cmp_to_key ( locale . strcoll )) # locale-aware sort order","title":"functools.cmp_to_key(func)"},{"location":"python/functools/#functoolstotal_ordering","text":"Given a class defining one or more rich comparison ordering methods, this class decorator supplies the rest. This simplifies the effort involved in specifying all of the possible rich comparison operations: The class must define one of __lt__() , __le__() , __gt__() , or __ge__() . In addition, the class should supply an __eq__() method.","title":"@functools.total_ordering"},{"location":"python/functools/#class-functoolspartialmethodfunc-args-keywords","text":"Needs more investigation","title":"class functools.partialmethod(func, /, *args, **keywords)"},{"location":"python/functools/#functoolssingledispatch","text":"Example: from functools import singledispatch @singledispatch def fun ( arg , verbose = False ): if verbose : print ( \"Let me just say,\" , end = \" \" ) print ( arg ) @fun . register def _ ( arg : int , verbose = False ): if verbose : print ( \"Strength in numbers, eh?\" , end = \" \" ) print ( arg ) @fun . register def _ ( arg : list , verbose = False ): if verbose : print ( \"Enumerate this:\" ) for i , elem in enumerate ( arg ): print ( i , elem )","title":"@functools.singledispatch"},{"location":"python/functools/#class-functoolssingledispatchmethodfunc","text":"Needs more investigation","title":"class functools.singledispatchmethod(func)"},{"location":"python/functools/#functoolsupdate_wrapperwrapper-wrapped-assignedwrapper_assignments-updatedwrapper_updates","text":"Update a wrapper function to look like the wrapped function. The optional arguments are tuples to specify which attributes of the original function are assigned directly to the matching attributes on the wrapper function and which attributes of the wrapper function are updated with the corresponding attributes from the original function. The default values for these arguments are the module level constants WRAPPER_ASSIGNMENTS (which assigns to the wrapper function\u2019s module , name , qualname , annotations and doc , the documentation string) and WRAPPER_UPDATES (which updates the wrapper function\u2019s dict , i.e. the instance dictionary).","title":"functools.update_wrapper(wrapper, wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES)"},{"location":"python/functools/#datamodel","text":"Source: https://docs.python.org/3/reference/datamodel.html","title":"Datamodel"},{"location":"python/pip/","text":"Pip Install packages without internet If you want to install a bunch of dependencies from, say a requirements.txt, you would do: mkdir dependencies pip download -r requirements.txt -d \"./dependencies\" tar cvfz dependencies.tar.gz dependencies And, once you transfer the dependencies.tar.gz to the machine which does not have internet you would do: tar zxvf dependencies.tar.gz cd dependencies pip install * -f ./ --no-index Use pip","title":"Pip"},{"location":"python/pip/#pip","text":"","title":"Pip"},{"location":"python/pip/#install-packages-without-internet","text":"If you want to install a bunch of dependencies from, say a requirements.txt, you would do: mkdir dependencies pip download -r requirements.txt -d \"./dependencies\" tar cvfz dependencies.tar.gz dependencies And, once you transfer the dependencies.tar.gz to the machine which does not have internet you would do: tar zxvf dependencies.tar.gz cd dependencies pip install * -f ./ --no-index","title":"Install packages without internet"},{"location":"python/pip/#use-pip","text":"","title":"Use pip"},{"location":"python/react/","text":"Introducing JSX Embedding Expressions in JSX You can put any valid JavaScript expression inside the curly braces in JSX JSX is an Expression Too After compilation, JSX expressions become regular JavaScript function calls and evaluate to JavaScript objects. This means that you can use JSX inside of if statements and for loops, assign it to variables, accept it as arguments, and return it from functions Specifying Attributes with JSX You may use quotes to specify string literals as attributes or curly braces to embed a JavaScript expression in an attribute. JSX Prevents Injection Attacks By default, React DOM escapes any values embedded in JSX before rendering them. Thus it ensures that you can never inject anything that\u2019s not explicitly written in your application. Everything is converted to a string before being rendered. Rendering Elements Rendering an Element into the DOM const element = <h1> Hello, world </h1> ; ReactDOM.render(element, document.getElementById('root')); Updating the Rendered Element React elements are immutable. Once you create an element, you can\u2019t change its children or attributes, the only way to update the UI is to create a new element, and pass it to ReactDOM.render() , note that this method is implicitly called when using setState method. Components and Props Conceptually, components are like JavaScript functions. They accept arbitrary inputs (called props ) and return React elements describing what should appear on the screen. Function and Class Components function Welcome(props) { return <h1> Hello, {props.name} </h1> ; } class Welcome extends React.Component { render() { return <h1> Hello, {this.props.name} </h1> ; } } Rendering a Component When React sees an element representing a user-defined component, it passes JSX attributes to this component as a single object. We call this object props . function Welcome(props) { return <h1> Hello, {props.name} </h1> ; } const element = <Welcome name= \"Sara\" /> ; ReactDOM.render( element, document.getElementById('root') ); Note: Always start component names with a capital letter. Props are Read-Only All React components must act like pure functions with respect to their props. Of course, application UIs are dynamic and change over time. In the next section, we will introduce a new concept of state State and Lifecycle Class components should always call the base constructor with props. The componentDidMount() method runs after the component output has been rendered to the DOM. If the component is ever removed from the DOM, React calls componentWillUnmount() . You are free to add additional fields to the class manually if you need to store something that doesn\u2019t participate in the data flow Using State Correctly Do Not Modify State Directly The only place where you can assign this.state is the constructor. State Updates May Be Asynchronous Because this.props and this.state may be updated asynchronously, you should not rely on their values for calculating the next state. // Wrong this . setState ( { counter : this . state . counter + this . props . increment , } ); // Correct this . setState (( state , props ) => ( { counter : state . counter + props . increment } )); The Data Flows Down Neither parent nor child components can know if a certain component is stateful or stateless, and they shouldn\u2019t care whether it is defined as a function or a class. Any state is always owned by some specific component, and any data or UI derived from that state can only affect components \u201cbelow\u201d them in the tree, this is commonly called a top-down or unidirectional data flow Handling Events React events are named using camelCase, rather than lowercase. With JSX you pass a function as the event handler, rather than a string. You cannot return false to prevent default behavior in React. You must call preventDefault explicitly. When you define a component using an ES6 class, a common pattern is for an event handler to be a method on the class class Toggle extends React . Component { constructor ( props ) { super ( props ); this . state = { isToggleOn : true }; // This binding is necessary to make ` this ` work in the callback this . handleClick = this . handleClick . bind ( this ); } handleClick () { this . setState ( state => ({ isToggleOn : ! state . isToggleOn })); } render () { return ( < button onClick = { this . handleClick } > { this . state . isToggleOn ? 'ON' : 'OFF' } </ button > ); } } ReactDOM . render ( < Toggle /> , document . getElementById ( 'root' ) ); You have to be careful about the meaning of this in JSX callbacks. In JavaScript, class methods are not bound by default. If you forget to bind this.handleClick and pass it to onClick, this will be undefined when the function is actually called, if you refer to a method without () after it, such as onClick={this.handleClick} , you should bind that method. If calling bind annoys you, there are two ways you can get around this. If you are using the experimental public class fields syntax, you can use class fields to correctly bind callbacks: class LoggingButton extends React . Component { // This syntax ensures `this` is bound within handleClick . // Warning : this is * experimental * syntax . handleClick = () => { console . log ( 'this is:' , this ); } render () { return ( < button onClick = { this . handleClick } > Click me </ button > ); } } This syntax is enabled by default in Create React App. If you aren\u2019t using class fields syntax, you can use an arrow function in the callback: class LoggingButton extends React . Component { handleClick () { console . log ( 'this is:' , this ); } render () { // This syntax ensures `this` is bound within handleClick return ( < button onClick = { ( e ) => this . handleClick ( e ) } > Click me </ button > ); } } The problem with this syntax is that a different callback is created each time the LoggingButton renders. In most cases, this is fine. However, if this callback is passed as a prop to lower components, those components might do an extra re-rendering. We generally recommend binding in the constructor or using the class fields syntax, to avoid this sort of performance problem. Passing Arguments to Event Handlers Inside a loop it is common to want to pass an extra parameter to an event handler. For example, if id is the row ID, either of the following would work: <button onClick= {(e) = > this.deleteRow(id, e)}>Delete Row </button> <button onClick= {this.deleteRow.bind(this, id)} > Delete Row </button> The above two lines are equivalent, and use arrow functions and Function.prototype.bind respectively. In both cases, the e argument representing the React event will be passed as a second argument after the ID. With an arrow function, we have to pass it explicitly, but with bind any further arguments are automatically forwarded. Conditional Rendering Element Variables class LoginControl extends React . Component { constructor ( props ) { super ( props ); this . handleLoginClick = this . handleLoginClick . bind ( this ); this . handleLogoutClick = this . handleLogoutClick . bind ( this ); this . state = { isLoggedIn : false }; } handleLoginClick () { this . setState ({ isLoggedIn : true }); } handleLogoutClick () { this . setState ({ isLoggedIn : false }); } render () { const isLoggedIn = this . state . isLoggedIn ; let button ; if ( isLoggedIn ) { button = < LogoutButton onClick = { this . handleLogoutClick } /> ; } else { button = < LoginButton onClick = { this . handleLoginClick } /> ; } return ( < div > < Greeting isLoggedIn = { isLoggedIn } /> { button } </ div > ); } } ReactDOM . render ( < LoginControl /> , document . getElementById ( 'root' ) ); Inline If with Logical && Operator function Mailbox(props) { const unreadMessages = props.unreadMessages; return ( <div> <h1> Hello! </h1> {unreadMessages.length > 0 && <h2> You have {unreadMessages.length} unread messages. </h2> } </div> ); } const messages = ['React', 'Re: React', 'Re:Re: React']; ReactDOM.render( <Mailbox unreadMessages= {messages} /> , document.getElementById('root') ); Inline If-Else with Conditional Operator render() { const isLoggedIn = this.state.isLoggedIn; return ( <div> The user is <b> {isLoggedIn ? 'currently' : 'not'} </b> logged in. </div> ); } Preventing Component from Rendering To do this return null instead of its render output. Returning null from a component\u2019s render method does not affect the firing of the component\u2019s lifecycle methods. For instance componentDidUpdate will still be called. Lists and Keys Rendering Multiple Components You can build collections of elements and include them in JSX using curly braces {} . const numbers = [1, 2, 3, 4, 5]; const listItems = numbers.map((number) => <li> {number} </li> ); ReactDOM.render( <ul> {listItems} </ul> , document.getElementById('root') ); Keys Keys help React identify which items have changed, are added, or are removed. Keys should be given to the elements inside the array to give the elements a stable identity We don\u2019t recommend using indexes for keys if the order of items may change. This can negatively impact performance and may cause issues with component state Keys serve as a hint to React but they don\u2019t get passed to your components. If you need the same value in your component, pass it explicitly as a prop with a different name: const content = posts . map (( post ) => < Post key = { post . id } id = { post . id } title = { post . title } /> ); Forms Controlled Components class NameForm extends React . Component { constructor ( props ) { super ( props ); this . state = { value : '' }; this . handleChange = this . handleChange . bind ( this ); this . handleSubmit = this . handleSubmit . bind ( this ); } handleChange ( event ) { this . setState ({ value : event . target . value }); } handleSubmit ( event ) { alert ( 'A name was submitted: ' + this . state . value ); event . preventDefault (); } render () { return ( < form onSubmit = { this . handleSubmit } > < label > Name : < input type = \"text\" value = { this . state . value } onChange = { this . handleChange } /> </ label > < input type = \"submit\" value = \"Submit\" /> </ form > ); } } Controlled Input Null Value Specifying the value prop on a controlled component prevents the user from changing the input unless you desire so. If you\u2019ve specified a value but the input is still editable, you may have accidentally set value to undefined or null. The following code demonstrates this. (The input is locked at first but becomes editable after a short delay.) ReactDOM.render(<input value=\"hi\" />, mountNode); setTimeout(function() { ReactDOM.render(<input value={null} />, mountNode); }, 1000); Handling Multiple Inputs // ES6 computed property name syntax to update the state key corresponding to the given input name : this . setState ( { [ name ] : value } ); // It is equivalent to this ES5 code : var partialState = {} ; partialState [ name ] = value ; this . setState ( partialState ); Lifting State Up Often, several components need to reflect the same changing data. We recommend lifting the shared state up to their closest common ancestor. Let\u2019s see how this works in action. Composition vs Inheritance Containment Some components don\u2019t know their children ahead of time. This is especially common for components like Sidebar or Dialog that represent generic boxes . We recommend that such components use the special children prop to pass children elements directly into their output: function FancyBorder(props) { return ( <div className= {'FancyBorder FancyBorder-' + props.color} > {props.children} </div> ); } This lets other components pass arbitrary children to them by nesting the JSX: function WelcomeDialog() { return ( <FancyBorder color= \"blue\" > <h1 className= \"Dialog-title\" > Welcome </h1> <p className= \"Dialog-message\" > Thank you for visiting our spacecraft! </p> </FancyBorder> ); } Anything inside the <FancyBorder> JSX tag gets passed into the FancyBorder component as a children prop. Since FancyBorder renders {props.children} inside a <div> , the passed elements appear in the final output. While this is less common, sometimes you might need multiple holes in a component. In such cases you may come up with your own convention instead of using children: function SplitPane(props) { return ( <div className= \"SplitPane\" > <div className= \"SplitPane-left\" > {props.left} </div> <div className= \"SplitPane-right\" > {props.right} </div> </div> ); } function App() { return ( <SplitPane left= { <Contacts /> } right={ <Chat /> } /> ); } So What About Inheritance? At Facebook, we use React in thousands of components, and we haven\u2019t found any use cases where we would recommend creating component inheritance hierarchies. Props and composition give you all the flexibility you need to customize a component\u2019s look and behavior in an explicit and safe way. Remember that components may accept arbitrary props, including primitive values, React elements, or functions. If you want to reuse non-UI functionality between components, we suggest extracting it into a separate JavaScript module. The components may import it and use that function, object, or a class, without extending it. Thinking in React Step 1: Break The UI Into A Component Hierarchy Step 2: Build A Static Version in React (don\u2019t use state at all) Step 3: Identify The Minimal (but complete) Representation Of UI State In order to define state. Simply ask three questions about each piece of data: Is it passed in from a parent via props? If so, it probably isn\u2019t state. Does it remain unchanged over time? If so, it probably isn\u2019t state. Can you compute it based on any other state or props in your component? If so, it isn\u2019t state. Step 4: Identify Where Your State Should Live Remember: React is all about one-way data flow down the component hierarchy. It may not be immediately clear which component should own what state. This is often the most challenging part for newcomers to understand . Step 5: Add Inverse Data Flow Since components should only update their own state, callbacks should be passed to the children components in order to allow them the change the state","title":"Introducing JSX"},{"location":"python/react/#introducing-jsx","text":"","title":"Introducing JSX"},{"location":"python/react/#embedding-expressions-in-jsx","text":"You can put any valid JavaScript expression inside the curly braces in JSX","title":"Embedding Expressions in JSX"},{"location":"python/react/#jsx-is-an-expression-too","text":"After compilation, JSX expressions become regular JavaScript function calls and evaluate to JavaScript objects. This means that you can use JSX inside of if statements and for loops, assign it to variables, accept it as arguments, and return it from functions","title":"JSX is an Expression Too"},{"location":"python/react/#specifying-attributes-with-jsx","text":"You may use quotes to specify string literals as attributes or curly braces to embed a JavaScript expression in an attribute.","title":"Specifying Attributes with JSX"},{"location":"python/react/#jsx-prevents-injection-attacks","text":"By default, React DOM escapes any values embedded in JSX before rendering them. Thus it ensures that you can never inject anything that\u2019s not explicitly written in your application. Everything is converted to a string before being rendered.","title":"JSX Prevents Injection Attacks"},{"location":"python/react/#rendering-elements","text":"","title":"Rendering Elements"},{"location":"python/react/#rendering-an-element-into-the-dom","text":"const element = <h1> Hello, world </h1> ; ReactDOM.render(element, document.getElementById('root'));","title":"Rendering an Element into the DOM"},{"location":"python/react/#updating-the-rendered-element","text":"React elements are immutable. Once you create an element, you can\u2019t change its children or attributes, the only way to update the UI is to create a new element, and pass it to ReactDOM.render() , note that this method is implicitly called when using setState method.","title":"Updating the Rendered Element"},{"location":"python/react/#components-and-props","text":"Conceptually, components are like JavaScript functions. They accept arbitrary inputs (called props ) and return React elements describing what should appear on the screen.","title":"Components and Props"},{"location":"python/react/#function-and-class-components","text":"function Welcome(props) { return <h1> Hello, {props.name} </h1> ; } class Welcome extends React.Component { render() { return <h1> Hello, {this.props.name} </h1> ; } }","title":"Function and Class Components"},{"location":"python/react/#rendering-a-component","text":"When React sees an element representing a user-defined component, it passes JSX attributes to this component as a single object. We call this object props . function Welcome(props) { return <h1> Hello, {props.name} </h1> ; } const element = <Welcome name= \"Sara\" /> ; ReactDOM.render( element, document.getElementById('root') ); Note: Always start component names with a capital letter.","title":"Rendering a Component"},{"location":"python/react/#props-are-read-only","text":"All React components must act like pure functions with respect to their props. Of course, application UIs are dynamic and change over time. In the next section, we will introduce a new concept of state","title":"Props are Read-Only"},{"location":"python/react/#state-and-lifecycle","text":"Class components should always call the base constructor with props. The componentDidMount() method runs after the component output has been rendered to the DOM. If the component is ever removed from the DOM, React calls componentWillUnmount() . You are free to add additional fields to the class manually if you need to store something that doesn\u2019t participate in the data flow","title":"State and Lifecycle"},{"location":"python/react/#using-state-correctly","text":"","title":"Using State Correctly"},{"location":"python/react/#do-not-modify-state-directly","text":"The only place where you can assign this.state is the constructor.","title":"Do Not Modify State Directly"},{"location":"python/react/#state-updates-may-be-asynchronous","text":"Because this.props and this.state may be updated asynchronously, you should not rely on their values for calculating the next state. // Wrong this . setState ( { counter : this . state . counter + this . props . increment , } ); // Correct this . setState (( state , props ) => ( { counter : state . counter + props . increment } ));","title":"State Updates May Be Asynchronous"},{"location":"python/react/#the-data-flows-down","text":"Neither parent nor child components can know if a certain component is stateful or stateless, and they shouldn\u2019t care whether it is defined as a function or a class. Any state is always owned by some specific component, and any data or UI derived from that state can only affect components \u201cbelow\u201d them in the tree, this is commonly called a top-down or unidirectional data flow","title":"The Data Flows Down"},{"location":"python/react/#handling-events","text":"React events are named using camelCase, rather than lowercase. With JSX you pass a function as the event handler, rather than a string. You cannot return false to prevent default behavior in React. You must call preventDefault explicitly. When you define a component using an ES6 class, a common pattern is for an event handler to be a method on the class class Toggle extends React . Component { constructor ( props ) { super ( props ); this . state = { isToggleOn : true }; // This binding is necessary to make ` this ` work in the callback this . handleClick = this . handleClick . bind ( this ); } handleClick () { this . setState ( state => ({ isToggleOn : ! state . isToggleOn })); } render () { return ( < button onClick = { this . handleClick } > { this . state . isToggleOn ? 'ON' : 'OFF' } </ button > ); } } ReactDOM . render ( < Toggle /> , document . getElementById ( 'root' ) ); You have to be careful about the meaning of this in JSX callbacks. In JavaScript, class methods are not bound by default. If you forget to bind this.handleClick and pass it to onClick, this will be undefined when the function is actually called, if you refer to a method without () after it, such as onClick={this.handleClick} , you should bind that method. If calling bind annoys you, there are two ways you can get around this. If you are using the experimental public class fields syntax, you can use class fields to correctly bind callbacks: class LoggingButton extends React . Component { // This syntax ensures `this` is bound within handleClick . // Warning : this is * experimental * syntax . handleClick = () => { console . log ( 'this is:' , this ); } render () { return ( < button onClick = { this . handleClick } > Click me </ button > ); } } This syntax is enabled by default in Create React App. If you aren\u2019t using class fields syntax, you can use an arrow function in the callback: class LoggingButton extends React . Component { handleClick () { console . log ( 'this is:' , this ); } render () { // This syntax ensures `this` is bound within handleClick return ( < button onClick = { ( e ) => this . handleClick ( e ) } > Click me </ button > ); } } The problem with this syntax is that a different callback is created each time the LoggingButton renders. In most cases, this is fine. However, if this callback is passed as a prop to lower components, those components might do an extra re-rendering. We generally recommend binding in the constructor or using the class fields syntax, to avoid this sort of performance problem.","title":"Handling Events"},{"location":"python/react/#passing-arguments-to-event-handlers","text":"Inside a loop it is common to want to pass an extra parameter to an event handler. For example, if id is the row ID, either of the following would work: <button onClick= {(e) = > this.deleteRow(id, e)}>Delete Row </button> <button onClick= {this.deleteRow.bind(this, id)} > Delete Row </button> The above two lines are equivalent, and use arrow functions and Function.prototype.bind respectively. In both cases, the e argument representing the React event will be passed as a second argument after the ID. With an arrow function, we have to pass it explicitly, but with bind any further arguments are automatically forwarded.","title":"Passing Arguments to Event Handlers"},{"location":"python/react/#conditional-rendering","text":"","title":"Conditional Rendering"},{"location":"python/react/#element-variables","text":"class LoginControl extends React . Component { constructor ( props ) { super ( props ); this . handleLoginClick = this . handleLoginClick . bind ( this ); this . handleLogoutClick = this . handleLogoutClick . bind ( this ); this . state = { isLoggedIn : false }; } handleLoginClick () { this . setState ({ isLoggedIn : true }); } handleLogoutClick () { this . setState ({ isLoggedIn : false }); } render () { const isLoggedIn = this . state . isLoggedIn ; let button ; if ( isLoggedIn ) { button = < LogoutButton onClick = { this . handleLogoutClick } /> ; } else { button = < LoginButton onClick = { this . handleLoginClick } /> ; } return ( < div > < Greeting isLoggedIn = { isLoggedIn } /> { button } </ div > ); } } ReactDOM . render ( < LoginControl /> , document . getElementById ( 'root' ) );","title":"Element Variables"},{"location":"python/react/#inline-if-with-logical-operator","text":"function Mailbox(props) { const unreadMessages = props.unreadMessages; return ( <div> <h1> Hello! </h1> {unreadMessages.length > 0 && <h2> You have {unreadMessages.length} unread messages. </h2> } </div> ); } const messages = ['React', 'Re: React', 'Re:Re: React']; ReactDOM.render( <Mailbox unreadMessages= {messages} /> , document.getElementById('root') );","title":"Inline If with Logical &amp;&amp; Operator"},{"location":"python/react/#inline-if-else-with-conditional-operator","text":"render() { const isLoggedIn = this.state.isLoggedIn; return ( <div> The user is <b> {isLoggedIn ? 'currently' : 'not'} </b> logged in. </div> ); }","title":"Inline If-Else with Conditional Operator"},{"location":"python/react/#preventing-component-from-rendering","text":"To do this return null instead of its render output. Returning null from a component\u2019s render method does not affect the firing of the component\u2019s lifecycle methods. For instance componentDidUpdate will still be called.","title":"Preventing Component from Rendering"},{"location":"python/react/#lists-and-keys","text":"","title":"Lists and Keys"},{"location":"python/react/#rendering-multiple-components","text":"You can build collections of elements and include them in JSX using curly braces {} . const numbers = [1, 2, 3, 4, 5]; const listItems = numbers.map((number) => <li> {number} </li> ); ReactDOM.render( <ul> {listItems} </ul> , document.getElementById('root') );","title":"Rendering Multiple Components"},{"location":"python/react/#keys","text":"Keys help React identify which items have changed, are added, or are removed. Keys should be given to the elements inside the array to give the elements a stable identity We don\u2019t recommend using indexes for keys if the order of items may change. This can negatively impact performance and may cause issues with component state Keys serve as a hint to React but they don\u2019t get passed to your components. If you need the same value in your component, pass it explicitly as a prop with a different name: const content = posts . map (( post ) => < Post key = { post . id } id = { post . id } title = { post . title } /> );","title":"Keys"},{"location":"python/react/#forms","text":"","title":"Forms"},{"location":"python/react/#controlled-components","text":"class NameForm extends React . Component { constructor ( props ) { super ( props ); this . state = { value : '' }; this . handleChange = this . handleChange . bind ( this ); this . handleSubmit = this . handleSubmit . bind ( this ); } handleChange ( event ) { this . setState ({ value : event . target . value }); } handleSubmit ( event ) { alert ( 'A name was submitted: ' + this . state . value ); event . preventDefault (); } render () { return ( < form onSubmit = { this . handleSubmit } > < label > Name : < input type = \"text\" value = { this . state . value } onChange = { this . handleChange } /> </ label > < input type = \"submit\" value = \"Submit\" /> </ form > ); } }","title":"Controlled Components"},{"location":"python/react/#controlled-input-null-value","text":"Specifying the value prop on a controlled component prevents the user from changing the input unless you desire so. If you\u2019ve specified a value but the input is still editable, you may have accidentally set value to undefined or null. The following code demonstrates this. (The input is locked at first but becomes editable after a short delay.) ReactDOM.render(<input value=\"hi\" />, mountNode); setTimeout(function() { ReactDOM.render(<input value={null} />, mountNode); }, 1000);","title":"Controlled Input Null Value"},{"location":"python/react/#handling-multiple-inputs","text":"// ES6 computed property name syntax to update the state key corresponding to the given input name : this . setState ( { [ name ] : value } ); // It is equivalent to this ES5 code : var partialState = {} ; partialState [ name ] = value ; this . setState ( partialState );","title":"Handling Multiple Inputs"},{"location":"python/react/#lifting-state-up","text":"Often, several components need to reflect the same changing data. We recommend lifting the shared state up to their closest common ancestor. Let\u2019s see how this works in action.","title":"Lifting State Up"},{"location":"python/react/#composition-vs-inheritance","text":"","title":"Composition vs Inheritance"},{"location":"python/react/#containment","text":"Some components don\u2019t know their children ahead of time. This is especially common for components like Sidebar or Dialog that represent generic boxes . We recommend that such components use the special children prop to pass children elements directly into their output: function FancyBorder(props) { return ( <div className= {'FancyBorder FancyBorder-' + props.color} > {props.children} </div> ); } This lets other components pass arbitrary children to them by nesting the JSX: function WelcomeDialog() { return ( <FancyBorder color= \"blue\" > <h1 className= \"Dialog-title\" > Welcome </h1> <p className= \"Dialog-message\" > Thank you for visiting our spacecraft! </p> </FancyBorder> ); } Anything inside the <FancyBorder> JSX tag gets passed into the FancyBorder component as a children prop. Since FancyBorder renders {props.children} inside a <div> , the passed elements appear in the final output. While this is less common, sometimes you might need multiple holes in a component. In such cases you may come up with your own convention instead of using children: function SplitPane(props) { return ( <div className= \"SplitPane\" > <div className= \"SplitPane-left\" > {props.left} </div> <div className= \"SplitPane-right\" > {props.right} </div> </div> ); } function App() { return ( <SplitPane left= { <Contacts /> } right={ <Chat /> } /> ); }","title":"Containment"},{"location":"python/react/#so-what-about-inheritance","text":"At Facebook, we use React in thousands of components, and we haven\u2019t found any use cases where we would recommend creating component inheritance hierarchies. Props and composition give you all the flexibility you need to customize a component\u2019s look and behavior in an explicit and safe way. Remember that components may accept arbitrary props, including primitive values, React elements, or functions. If you want to reuse non-UI functionality between components, we suggest extracting it into a separate JavaScript module. The components may import it and use that function, object, or a class, without extending it.","title":"So What About Inheritance?"},{"location":"python/react/#thinking-in-react","text":"","title":"Thinking in React"},{"location":"python/react/#step-1-break-the-ui-into-a-component-hierarchy","text":"","title":"Step 1: Break The UI Into A Component Hierarchy"},{"location":"python/react/#step-2-build-a-static-version-in-react-dont-use-state-at-all","text":"","title":"Step 2: Build A Static Version in React (don\u2019t use state at all)"},{"location":"python/react/#step-3-identify-the-minimal-but-complete-representation-of-ui-state","text":"In order to define state. Simply ask three questions about each piece of data: Is it passed in from a parent via props? If so, it probably isn\u2019t state. Does it remain unchanged over time? If so, it probably isn\u2019t state. Can you compute it based on any other state or props in your component? If so, it isn\u2019t state.","title":"Step 3: Identify The Minimal (but complete) Representation Of UI State"},{"location":"python/react/#step-4-identify-where-your-state-should-live","text":"Remember: React is all about one-way data flow down the component hierarchy. It may not be immediately clear which component should own what state. This is often the most challenging part for newcomers to understand .","title":"Step 4: Identify Where Your State Should Live"},{"location":"python/react/#step-5-add-inverse-data-flow","text":"Since components should only update their own state, callbacks should be passed to the children components in order to allow them the change the state","title":"Step 5: Add Inverse Data Flow"},{"location":"scala/SUMMARY/","text":"Summary Scala Sbt","title":"Summary"},{"location":"scala/SUMMARY/#summary","text":"Scala Sbt","title":"Summary"},{"location":"scala/ammonite/","text":"Ammonite Install sudo sh -c '(echo \"#!/usr/bin/env sh\" && curl -L https://github.com/lihaoyi/Ammonite/releases/download/1.6.4/2.12-1.6.4) > /usr/local/bin/amm && chmod +x /usr/local/bin/amm' && amm Script Files Ammonite defines a format that allows you to load external scripts into the REPL; this can be used to save common functionality so it can be used at a later date. In the simplest case, a script file is simply a sequence of Scala statements, e.g.","title":"Ammonite"},{"location":"scala/ammonite/#ammonite","text":"","title":"Ammonite"},{"location":"scala/ammonite/#install","text":"sudo sh -c '(echo \"#!/usr/bin/env sh\" && curl -L https://github.com/lihaoyi/Ammonite/releases/download/1.6.4/2.12-1.6.4) > /usr/local/bin/amm && chmod +x /usr/local/bin/amm' && amm","title":"Install"},{"location":"scala/ammonite/#script-files","text":"Ammonite defines a format that allows you to load external scripts into the REPL; this can be used to save common functionality so it can be used at a later date. In the simplest case, a script file is simply a sequence of Scala statements, e.g.","title":"Script Files"},{"location":"scala/chapter1/","text":"Scala How to slow down an akka stream ? //send messages over tcp one message per second in order to make sure Tcp connection is closed after sending //a message. Note that is a requirement from collectorApp Source ( messages ) . throttle ( 1 , 1 seconds , 1 , ThrottleMode . Shaping ) . runWith ( tcpSink ) How akka Tcp Work ? All of the Akka I/O APIs are accessed through manager objects. When using an I/O API, the first step is to acquire a reference to the appropriate manager. The code below shows how to acquire a reference to the Tcp manager. import akka . io .{ IO , Tcp } import context . system // implicitly used by IO(Tcp) val manager = IO ( Tcp ) The manager is an actor that handles the underlying low level I/O resources (selectors, channels) and instantiates workers for specific tasks, such as listening to incoming connections.","title":"Scala"},{"location":"scala/chapter1/#scala","text":"","title":"Scala"},{"location":"scala/chapter1/#how-to-slow-down-an-akka-stream","text":"//send messages over tcp one message per second in order to make sure Tcp connection is closed after sending //a message. Note that is a requirement from collectorApp Source ( messages ) . throttle ( 1 , 1 seconds , 1 , ThrottleMode . Shaping ) . runWith ( tcpSink )","title":"How to slow down an akka stream ?"},{"location":"scala/chapter1/#how-akka-tcp-work","text":"All of the Akka I/O APIs are accessed through manager objects. When using an I/O API, the first step is to acquire a reference to the appropriate manager. The code below shows how to acquire a reference to the Tcp manager. import akka . io .{ IO , Tcp } import context . system // implicitly used by IO(Tcp) val manager = IO ( Tcp ) The manager is an actor that handles the underlying low level I/O resources (selectors, channels) and instantiates workers for specific tasks, such as listening to incoming connections.","title":"How akka Tcp Work ?"},{"location":"scala/sbt-native-packager/","text":"sbt-native-packer Instalation add to plugins.sbt : addSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.3.17\") Configuration Archtypes: Java Application Java Server Application Formats: SbtNativePackager + | | +-------+ Universal +--------|-------------|----------------+ | + | | | | | | | | | | | | | + + + + + Docker +-+ Linux +-+ Windows JDKPackager GraalVM native-image | | | | + + Debian RPM Universal Linux Debian Rpm Docker Windows JdkPackager GraallVM Native image packager Create a package We can generate other packages via the following tasks. Note that each packaging format may needs some additional configuration and native tools available. Here\u2019s a complete list of current formats. universal:packageBin - Generates a universal zip file universal:packageZipTarball - Generates a universal tgz file debian:packageBin - Generates a deb docker:publishLocal - Builds a Docker image using the local Docker server universal:packageOsxDmg - Generates a DMG file with the same contents as the universal zip/tgz. windows:packageBin - Generates an MSI Running the application In order to pass application arguments you need to separate the jvm arguments from the application arguments with --. For example","title":"sbt-native-packer"},{"location":"scala/sbt-native-packager/#sbt-native-packer","text":"","title":"sbt-native-packer"},{"location":"scala/sbt-native-packager/#instalation","text":"add to plugins.sbt : addSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.3.17\")","title":"Instalation"},{"location":"scala/sbt-native-packager/#configuration","text":"","title":"Configuration"},{"location":"scala/sbt-native-packager/#archtypes","text":"Java Application Java Server Application","title":"Archtypes:"},{"location":"scala/sbt-native-packager/#formats","text":"SbtNativePackager + | | +-------+ Universal +--------|-------------|----------------+ | + | | | | | | | | | | | | | + + + + + Docker +-+ Linux +-+ Windows JDKPackager GraalVM native-image | | | | + + Debian RPM Universal Linux Debian Rpm Docker Windows JdkPackager GraallVM Native image packager","title":"Formats:"},{"location":"scala/sbt-native-packager/#create-a-package","text":"We can generate other packages via the following tasks. Note that each packaging format may needs some additional configuration and native tools available. Here\u2019s a complete list of current formats. universal:packageBin - Generates a universal zip file universal:packageZipTarball - Generates a universal tgz file debian:packageBin - Generates a deb docker:publishLocal - Builds a Docker image using the local Docker server universal:packageOsxDmg - Generates a DMG file with the same contents as the universal zip/tgz. windows:packageBin - Generates an MSI","title":"Create a package"},{"location":"scala/sbt-native-packager/#running-the-application","text":"In order to pass application arguments you need to separate the jvm arguments from the application arguments with --. For example","title":"Running the application"},{"location":"scala/sbt/","text":"Sbt Install echo \"deb https://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823 sudo apt-get update sudo apt-get install sbt Packaging using [sb-native-package]: in build.sbt : enablePlugins(JavaAppPackaging, DockerPluging in project/plugins.sbt : addSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.3.15\") and then run: sbt docker:publishLocal to push the docker image locally sbt universal:packageBin to create target/universal/yourapp.zip distribution file","title":"Sbt"},{"location":"scala/sbt/#sbt","text":"","title":"Sbt"},{"location":"scala/sbt/#install","text":"echo \"deb https://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823 sudo apt-get update sudo apt-get install sbt","title":"Install"},{"location":"scala/sbt/#packaging","text":"using [sb-native-package]: in build.sbt : enablePlugins(JavaAppPackaging, DockerPluging in project/plugins.sbt : addSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.3.15\") and then run: sbt docker:publishLocal to push the docker image locally sbt universal:packageBin to create target/universal/yourapp.zip distribution file","title":"Packaging"},{"location":"scala/standars/","text":"Logging should be done with java.slf4j.Logger For akka inclue akka-slf4j and add akka.event.Sl4jLogger to loggers configuration logback-classic as a backend for slf4j, use fixLogging to fix conflicts. For logging use the parameterized mehtods instead of the ones that takes only a String, this is an optimisation as the former materialises the string objects even if the messages are not logged when the latter doesn't. akka actors should be named for clarity (this applies to big-mama-sdk and silver-watch-backend), system and actor names should be lower cased and hyphenated don't add uneccessary dependencies, try to use the minimal set of them for multi build project, use the same structure as big-mama-sdk and silver-watch-backend (we have to create a seed project for that) If one of the sub projects need scalapb add retrieveManaged := true PB.targets := Seq(scalapb.gen() -> (sourceManaged in Compile).value) to the build.sbt of the subproject (not the root project), if you use want to compile src/test/protobf files also add the folowing : retrieveManaged := true PB.targets in Test := Seq(scalapb.gen() -> (sourceManaged in Test).value) Project.inConfig(Test)(sbtprotoc.ProtocPlugin.protobufConfigSettings) All project should use scoverage tool as follows: add addSbtPlugin(\"org.scoverage\" % \"sbt-scoverage\" % \"1.5.1\") to your project/plugins.sbt add the folowwing to your build.sbt (or to your commonConfig variable if you are in a multi project build) scala coverageEnabled := true, coverageMinimum := 50, coverageFailOnMinimum := true, coverageHighlighting := true, To run the test with coverate enabled: sbt test coverage To get The coverage report (for every sub project if you are in multi project build): sbt test coverateReport To get an aggregated report for all sub projects sbt coverageAggregate . This should be executed after one of the previous commands Supervision strategy for streams: Streams fails by default, if you want to avoid stream failures you need to provide a supervision strategy to the ActorMaterializer. scala val decider: Supervision.Decider = { case x => logger.error(\"Stream Error\", x) Supervision.Resume } implicit val materializer = ActorMaterializer( ActorMaterializerSettings(system).withSupervisionStrategy(decider)) Note that the supervision strategy is not applied to all the stages/operators, some of them not adhere to the supervision strategy, check the docs for that. sbt assembly: Sbt assembly needs a mergeStrategy to resolve conflicts (same files in diffrent paths), you need to put it in the common settings for multi build project structure scala assemblyMergeStrategy in assembly := { //this ensure netty native libraries are not discarded by the second case case PathList(\"META-INF\", \"native\", xs @ _*) => MergeStrategy.first //discard all other META-INF duplicated files. Be aware !! case PathList(\"META-INF\", xs @ _*) => MergeStrategy.discard //merge configuration files case \"application.conf\" => MergeStrategy.concat case \"reference.conf\" => MergeStrategy.concat //apply the default strategy for ohters case x => val oldStrategy = (assemblyMergeStrategy in assembly).value oldStrategy(x) } If you don't want to assemble the root project (by default the aggregation of all the subprojects) use this configutaitons ``` lazy val rootSettings = Seq( publishArtifact := false, publishArtifact in Test := false ) val root = project .settings(rootSettings: _*) if you want to not publish the root project, add the following to the rootSettings scala skip in publish := true ```","title":"Standars"}]}